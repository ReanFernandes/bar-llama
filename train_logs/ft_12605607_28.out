Running configuration: 
[2025-01-12 12:11:49,203][root][INFO] - Global seed set to 21
[2025-01-12 12:11:49,204][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-12 12:11:49,204][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-12 12:11:49,204][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-12 12:11:49,204][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : json 
[2025-01-12 12:11:58,517][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-12 12:11:58,530][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-12 12:11:58,533][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-12 12:11:58,533][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-12 12:11:58,536][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'json', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-12 12:11:59,015][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:12:01,571][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:12:03,875][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'o_proj', 'gate_proj', 'up_proj', 'k_proj', 'q_proj', 'v_proj', 'down_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-12 12:12:06,297][root][INFO] - Lora adapter added to model
{'loss': 1.1202, 'grad_norm': 0.11350952833890915, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.6197, 'grad_norm': 0.10020405054092407, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.4432, 'grad_norm': 0.10643456876277924, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.415, 'grad_norm': 0.06760606914758682, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.3706, 'grad_norm': 0.07032028585672379, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.6106, 'grad_norm': 0.07886958867311478, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.4705, 'grad_norm': 0.07221824675798416, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.4647, 'grad_norm': 0.07159513980150223, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.3708, 'grad_norm': 0.061529044061899185, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.333, 'grad_norm': 0.070095956325531, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.5075, 'grad_norm': 0.07110513746738434, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.474, 'grad_norm': 0.06583739817142487, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.4245, 'grad_norm': 0.06324270367622375, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.3573, 'grad_norm': 0.07268227636814117, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.2983, 'grad_norm': 0.06413152813911438, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.5385, 'grad_norm': 0.06278745830059052, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.5153, 'grad_norm': 0.07749009877443314, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.4204, 'grad_norm': 0.069736547768116, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.3976, 'grad_norm': 0.0687645673751831, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.2968, 'grad_norm': 0.06778319925069809, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 0.5312, 'grad_norm': 0.06468309462070465, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4341, 'grad_norm': 0.05619072541594505, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4339, 'grad_norm': 0.056555572897195816, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.3944, 'grad_norm': 0.06181267276406288, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.2983, 'grad_norm': 0.05738788843154907, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.5646, 'grad_norm': 0.06657707691192627, 'learning_rate': 0.0002, 'epoch': 0.17}
{'loss': 0.4786, 'grad_norm': 0.0664294883608818, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.4041, 'grad_norm': 0.0792795941233635, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.3689, 'grad_norm': 0.0605890229344368, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.3098, 'grad_norm': 0.06034257262945175, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.5513, 'grad_norm': 0.06787250190973282, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.4313, 'grad_norm': 0.07202503085136414, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.402, 'grad_norm': 0.07680631428956985, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.3645, 'grad_norm': 0.0756383091211319, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.2792, 'grad_norm': 0.05929837375879288, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.556, 'grad_norm': 0.07524406909942627, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.4782, 'grad_norm': 0.0682070329785347, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.3713, 'grad_norm': 0.05585740879178047, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.3512, 'grad_norm': 0.07180295139551163, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.311, 'grad_norm': 0.05542653053998947, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.5703, 'grad_norm': 0.07339619845151901, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.4514, 'grad_norm': 0.061438627541065216, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.406, 'grad_norm': 0.08025437593460083, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.3868, 'grad_norm': 0.0674700140953064, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.2731, 'grad_norm': 0.06065024062991142, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.5229, 'grad_norm': 0.06080061197280884, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.439, 'grad_norm': 0.06533466279506683, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.3973, 'grad_norm': 0.082338847219944, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3541, 'grad_norm': 0.06812610477209091, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3266, 'grad_norm': 0.08545584976673126, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.5673, 'grad_norm': 0.07132989168167114, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.4746, 'grad_norm': 0.08577080816030502, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.4508, 'grad_norm': 0.07831118255853653, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3814, 'grad_norm': 0.1041371077299118, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3223, 'grad_norm': 0.06052543595433235, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 0.5594, 'grad_norm': 0.08072090148925781, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4657, 'grad_norm': 0.07555289566516876, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4312, 'grad_norm': 0.07687695324420929, 'learning_rate': 0.0002, 'epoch': 0.38}
{'loss': 0.3647, 'grad_norm': 0.0720105692744255, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.2814, 'grad_norm': 0.07869253307580948, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.5007, 'grad_norm': 0.08402436226606369, 'learning_rate': 0.0002, 'epoch': 0.4}
{'loss': 0.4956, 'grad_norm': 0.08209780603647232, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.4283, 'grad_norm': 0.08741389214992523, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.3918, 'grad_norm': 0.07695527374744415, 'learning_rate': 0.0002, 'epoch': 0.42}
{'loss': 0.3241, 'grad_norm': 0.060353074222803116, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.5202, 'grad_norm': 0.07387455552816391, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.4163, 'grad_norm': 0.08520246297121048, 'learning_rate': 0.0002, 'epoch': 0.44}
{'loss': 0.3764, 'grad_norm': 0.0657026618719101, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3514, 'grad_norm': 0.07003160566091537, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3075, 'grad_norm': 0.06627664715051651, 'learning_rate': 0.0002, 'epoch': 0.46}
{'loss': 0.5964, 'grad_norm': 0.08104494214057922, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.4308, 'grad_norm': 0.07680390030145645, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.4024, 'grad_norm': 0.08901292085647583, 'learning_rate': 0.0002, 'epoch': 0.48}
{'loss': 0.3497, 'grad_norm': 0.07900276780128479, 'learning_rate': 0.0002, 'epoch': 0.49}
