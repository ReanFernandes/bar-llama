Running configuration: 
[2025-01-12 12:03:35,766][root][INFO] - Global seed set to 21
[2025-01-12 12:03:35,766][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-12 12:03:35,766][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-12 12:03:35,766][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-12 12:03:35,767][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : json 
[2025-01-12 12:03:46,585][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-12 12:03:46,598][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-12 12:03:46,601][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-12 12:03:46,601][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-12 12:03:46,604][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'json', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-12 12:03:47,103][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:03:49,522][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:03:51,817][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'o_proj', 'up_proj', 'v_proj', 'down_proj', 'q_proj', 'gate_proj', 'k_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-12 12:03:54,240][root][INFO] - Lora adapter added to model
{'loss': 1.1202, 'grad_norm': 0.11350950598716736, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.6197, 'grad_norm': 0.10020347684621811, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.4432, 'grad_norm': 0.10643409192562103, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.415, 'grad_norm': 0.06761091947555542, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.3706, 'grad_norm': 0.07033339142799377, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.6106, 'grad_norm': 0.0788307934999466, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.4705, 'grad_norm': 0.07240822166204453, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.4647, 'grad_norm': 0.07075347006320953, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.3707, 'grad_norm': 0.061778437346220016, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.3329, 'grad_norm': 0.07020304352045059, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.5077, 'grad_norm': 0.07165980339050293, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.4737, 'grad_norm': 0.06656844913959503, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.4245, 'grad_norm': 0.062275320291519165, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.3579, 'grad_norm': 0.07503680139780045, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.2982, 'grad_norm': 0.06410421431064606, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.5385, 'grad_norm': 0.06235894560813904, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.5151, 'grad_norm': 0.07706979662179947, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.4202, 'grad_norm': 0.06981194764375687, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.3979, 'grad_norm': 0.06902821362018585, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.2967, 'grad_norm': 0.0677146315574646, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 0.5311, 'grad_norm': 0.06559596955776215, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4338, 'grad_norm': 0.0559307336807251, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4339, 'grad_norm': 0.057212695479393005, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.3944, 'grad_norm': 0.06257401406764984, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.2986, 'grad_norm': 0.05722833424806595, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.5649, 'grad_norm': 0.06654917448759079, 'learning_rate': 0.0002, 'epoch': 0.17}
{'loss': 0.4785, 'grad_norm': 0.06606615334749222, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.4042, 'grad_norm': 0.07921447604894638, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.3691, 'grad_norm': 0.060613539069890976, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.3095, 'grad_norm': 0.05786649137735367, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.5513, 'grad_norm': 0.06757186353206635, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.4309, 'grad_norm': 0.0715385228395462, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.4024, 'grad_norm': 0.07562364637851715, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.3642, 'grad_norm': 0.07621253281831741, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.2786, 'grad_norm': 0.05870669335126877, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.5562, 'grad_norm': 0.07515154033899307, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.4783, 'grad_norm': 0.09216255694627762, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.3718, 'grad_norm': 0.05672602728009224, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.3514, 'grad_norm': 0.07209017872810364, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.3109, 'grad_norm': 0.056021735072135925, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.5702, 'grad_norm': 0.0738503560423851, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.4514, 'grad_norm': 0.062038443982601166, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.4064, 'grad_norm': 0.08206003159284592, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.3863, 'grad_norm': 0.0666499137878418, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.2729, 'grad_norm': 0.06060031056404114, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.523, 'grad_norm': 0.06040225550532341, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.4392, 'grad_norm': 0.0649498775601387, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.3974, 'grad_norm': 0.08254123479127884, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3536, 'grad_norm': 0.06894868612289429, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.327, 'grad_norm': 0.08180612325668335, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.5673, 'grad_norm': 0.07009663432836533, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.4742, 'grad_norm': 0.08520766347646713, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.4506, 'grad_norm': 0.0785643681883812, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.381, 'grad_norm': 0.10669826716184616, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3217, 'grad_norm': 0.060547202825546265, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 0.5595, 'grad_norm': 0.08100838959217072, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4658, 'grad_norm': 0.07520923018455505, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4314, 'grad_norm': 0.07758428901433945, 'learning_rate': 0.0002, 'epoch': 0.38}
{'loss': 0.3646, 'grad_norm': 0.0718759298324585, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.2816, 'grad_norm': 0.07877294719219208, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.5003, 'grad_norm': 0.08238013833761215, 'learning_rate': 0.0002, 'epoch': 0.4}
{'loss': 0.4954, 'grad_norm': 0.08262019604444504, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.4281, 'grad_norm': 0.08683572709560394, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.3916, 'grad_norm': 0.07611794024705887, 'learning_rate': 0.0002, 'epoch': 0.42}
{'loss': 0.3243, 'grad_norm': 0.0601239949464798, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.5196, 'grad_norm': 0.0732429027557373, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.4162, 'grad_norm': 0.08529277890920639, 'learning_rate': 0.0002, 'epoch': 0.44}
{'loss': 0.3765, 'grad_norm': 0.06500968337059021, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3509, 'grad_norm': 0.06955619901418686, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3075, 'grad_norm': 0.06568700075149536, 'learning_rate': 0.0002, 'epoch': 0.46}
{'loss': 0.5966, 'grad_norm': 0.08114273846149445, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.431, 'grad_norm': 0.07676399499177933, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.4028, 'grad_norm': 0.0876782163977623, 'learning_rate': 0.0002, 'epoch': 0.48}
{'loss': 0.349, 'grad_norm': 0.07492859661579132, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.3238, 'grad_norm': 0.06354126334190369, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.5083, 'grad_norm': 0.06629057973623276, 'learning_rate': 0.0002, 'epoch': 0.5}
{'loss': 0.4557, 'grad_norm': 0.0858965590596199, 'learning_rate': 0.0002, 'epoch': 0.51}
{'loss': 0.3883, 'grad_norm': 0.07808404415845871, 'learning_rate': 0.0002, 'epoch': 0.51}
{'loss': 0.3192, 'grad_norm': 0.07078813761472702, 'learning_rate': 0.0002, 'epoch': 0.52}
{'loss': 0.2755, 'grad_norm': 0.0611296147108078, 'learning_rate': 0.0002, 'epoch': 0.53}
{'loss': 0.5283, 'grad_norm': 0.07797831296920776, 'learning_rate': 0.0002, 'epoch': 0.53}
{'loss': 0.4354, 'grad_norm': 0.08980017155408859, 'learning_rate': 0.0002, 'epoch': 0.54}
{'loss': 0.4169, 'grad_norm': 0.08640606701374054, 'learning_rate': 0.0002, 'epoch': 0.54}
{'loss': 0.3822, 'grad_norm': 0.07665176689624786, 'learning_rate': 0.0002, 'epoch': 0.55}
{'loss': 0.2715, 'grad_norm': 0.06463534384965897, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.5358, 'grad_norm': 0.09964800626039505, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.451, 'grad_norm': 0.08074382692575455, 'learning_rate': 0.0002, 'epoch': 0.57}
{'loss': 0.4137, 'grad_norm': 0.07612697780132294, 'learning_rate': 0.0002, 'epoch': 0.58}
{'loss': 0.3661, 'grad_norm': 0.06137937679886818, 'learning_rate': 0.0002, 'epoch': 0.58}
{'loss': 0.2929, 'grad_norm': 0.06988438218832016, 'learning_rate': 0.0002, 'epoch': 0.59}
{'loss': 0.5586, 'grad_norm': 0.07944293320178986, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.4891, 'grad_norm': 0.077693410217762, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.4201, 'grad_norm': 0.07629228383302689, 'learning_rate': 0.0002, 'epoch': 0.61}
{'loss': 0.346, 'grad_norm': 0.06805918365716934, 'learning_rate': 0.0002, 'epoch': 0.62}
{'loss': 0.2857, 'grad_norm': 0.06906306743621826, 'learning_rate': 0.0002, 'epoch': 0.62}
{'loss': 0.5276, 'grad_norm': 0.07170134782791138, 'learning_rate': 0.0002, 'epoch': 0.63}
{'loss': 0.4325, 'grad_norm': 0.07084345072507858, 'learning_rate': 0.0002, 'epoch': 0.64}
{'loss': 0.3926, 'grad_norm': 0.07353720813989639, 'learning_rate': 0.0002, 'epoch': 0.64}
