Running configuration: 
[2025-01-12 12:15:23,677][root][INFO] - Global seed set to 21
[2025-01-12 12:15:23,677][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-12 12:15:23,677][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-12 12:15:23,678][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-12 12:15:23,678][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : json 
[2025-01-12 12:15:32,507][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-12 12:15:32,520][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-12 12:15:32,523][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-12 12:15:32,523][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-12 12:15:32,526][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'json', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-12 12:15:32,980][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:15:35,419][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:15:37,699][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'q_proj', 'gate_proj', 'up_proj', 'down_proj', 'o_proj', 'k_proj', 'v_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-12 12:15:40,125][root][INFO] - Lora adapter added to model
{'loss': 1.1202, 'grad_norm': 0.11350948363542557, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.6197, 'grad_norm': 0.10020334273576736, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.4432, 'grad_norm': 0.10643436759710312, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.415, 'grad_norm': 0.06761075556278229, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.3706, 'grad_norm': 0.0703340545296669, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.6106, 'grad_norm': 0.07880805432796478, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.4705, 'grad_norm': 0.07253444939851761, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.4647, 'grad_norm': 0.07153394818305969, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.3708, 'grad_norm': 0.061991382390260696, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.3331, 'grad_norm': 0.06970208138227463, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.5078, 'grad_norm': 0.0716904029250145, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.4736, 'grad_norm': 0.06647200137376785, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.4247, 'grad_norm': 0.06303205341100693, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.3577, 'grad_norm': 0.0745246559381485, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.2983, 'grad_norm': 0.06354723125696182, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.5386, 'grad_norm': 0.06227050721645355, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.5151, 'grad_norm': 0.07412555813789368, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.42, 'grad_norm': 0.06895628571510315, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.3978, 'grad_norm': 0.06952414661645889, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.2968, 'grad_norm': 0.06886905431747437, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 0.5314, 'grad_norm': 0.06622044742107391, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4337, 'grad_norm': 0.055958885699510574, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4338, 'grad_norm': 0.05797838047146797, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.3946, 'grad_norm': 0.062470898032188416, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.2983, 'grad_norm': 0.056724850088357925, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.5649, 'grad_norm': 0.06633388251066208, 'learning_rate': 0.0002, 'epoch': 0.17}
{'loss': 0.4783, 'grad_norm': 0.06652434170246124, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.4039, 'grad_norm': 0.07892175763845444, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.3691, 'grad_norm': 0.06052014231681824, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.3096, 'grad_norm': 0.05942720174789429, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.5512, 'grad_norm': 0.06818480789661407, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.4315, 'grad_norm': 0.07248841971158981, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.4022, 'grad_norm': 0.07675434648990631, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.3643, 'grad_norm': 0.0742393359541893, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.279, 'grad_norm': 0.05897846817970276, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.556, 'grad_norm': 0.07584211975336075, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.4783, 'grad_norm': 0.06905344873666763, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.3715, 'grad_norm': 0.05623370036482811, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.3512, 'grad_norm': 0.07342053204774857, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.3107, 'grad_norm': 0.055989012122154236, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.5702, 'grad_norm': 0.07473105937242508, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.4515, 'grad_norm': 0.06304039806127548, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.4065, 'grad_norm': 0.08097871392965317, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.3864, 'grad_norm': 0.06766986101865768, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.2731, 'grad_norm': 0.06087619438767433, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.5233, 'grad_norm': 0.06055660918354988, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.4392, 'grad_norm': 0.06565108150243759, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.3976, 'grad_norm': 0.08216164261102676, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3542, 'grad_norm': 0.06813818216323853, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3268, 'grad_norm': 0.07790025323629379, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.5671, 'grad_norm': 0.07049990445375443, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.4746, 'grad_norm': 0.08457861095666885, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.4506, 'grad_norm': 0.07814262807369232, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.381, 'grad_norm': 0.10511776059865952, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.322, 'grad_norm': 0.060834161937236786, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 0.5593, 'grad_norm': 0.08138904720544815, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4659, 'grad_norm': 0.07339296489953995, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4311, 'grad_norm': 0.07784820348024368, 'learning_rate': 0.0002, 'epoch': 0.38}
{'loss': 0.3644, 'grad_norm': 0.07194937020540237, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.2816, 'grad_norm': 0.07934761792421341, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.5004, 'grad_norm': 0.08345795422792435, 'learning_rate': 0.0002, 'epoch': 0.4}
{'loss': 0.4957, 'grad_norm': 0.08156975358724594, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.4283, 'grad_norm': 0.08651746064424515, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.3918, 'grad_norm': 0.074775330722332, 'learning_rate': 0.0002, 'epoch': 0.42}
