Running configuration: 
[2025-01-12 12:18:44,966][root][INFO] - Global seed set to 21
[2025-01-12 12:18:44,966][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-12 12:18:44,966][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-12 12:18:44,966][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-12 12:18:44,966][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : json 
[2025-01-12 12:18:53,643][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-12 12:18:53,656][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-12 12:18:53,659][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-12 12:18:53,659][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-12 12:18:53,662][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'json', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-12 12:18:54,156][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:18:56,474][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:18:58,729][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'o_proj', 'gate_proj', 'k_proj', 'v_proj', 'up_proj', 'down_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-12 12:19:01,141][root][INFO] - Lora adapter added to model
{'loss': 1.1202, 'grad_norm': 0.11350956559181213, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.6197, 'grad_norm': 0.10020338743925095, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.4432, 'grad_norm': 0.10643403977155685, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.415, 'grad_norm': 0.06761175394058228, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.3706, 'grad_norm': 0.07033663988113403, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.6106, 'grad_norm': 0.078842893242836, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.4705, 'grad_norm': 0.07243439555168152, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.4647, 'grad_norm': 0.070729099214077, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.3707, 'grad_norm': 0.06186414882540703, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.333, 'grad_norm': 0.06982772797346115, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.5076, 'grad_norm': 0.07154785096645355, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.4736, 'grad_norm': 0.06657201051712036, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.4242, 'grad_norm': 0.06238117069005966, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.3579, 'grad_norm': 0.07478540390729904, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.2984, 'grad_norm': 0.06430964171886444, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.5386, 'grad_norm': 0.06251449137926102, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.5153, 'grad_norm': 0.07719795405864716, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.4202, 'grad_norm': 0.06964410096406937, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.3978, 'grad_norm': 0.06856595724821091, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.2967, 'grad_norm': 0.06754186749458313, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 0.531, 'grad_norm': 0.06639096885919571, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4339, 'grad_norm': 0.056011103093624115, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4337, 'grad_norm': 0.056098103523254395, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.3942, 'grad_norm': 0.06200899928808212, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.2985, 'grad_norm': 0.05671809986233711, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.5648, 'grad_norm': 0.06661868840456009, 'learning_rate': 0.0002, 'epoch': 0.17}
{'loss': 0.4786, 'grad_norm': 0.06599555909633636, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.4041, 'grad_norm': 0.07998810708522797, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.3689, 'grad_norm': 0.06004730239510536, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.3095, 'grad_norm': 0.05940185859799385, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.5507, 'grad_norm': 0.06825269013643265, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.4318, 'grad_norm': 0.07304925471544266, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.4019, 'grad_norm': 0.08077889680862427, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.3646, 'grad_norm': 0.07443252950906754, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.2788, 'grad_norm': 0.05928421765565872, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.5562, 'grad_norm': 0.0759066566824913, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.4785, 'grad_norm': 0.06937889009714127, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.3716, 'grad_norm': 0.0559266023337841, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.3512, 'grad_norm': 0.07117631286382675, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.311, 'grad_norm': 0.055962447077035904, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.5696, 'grad_norm': 0.0730811282992363, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.4518, 'grad_norm': 0.061978522688150406, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.4058, 'grad_norm': 0.08136481791734695, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.386, 'grad_norm': 0.06656253337860107, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.2724, 'grad_norm': 0.0609024241566658, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.523, 'grad_norm': 0.06113148853182793, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.4391, 'grad_norm': 0.06496802717447281, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.3973, 'grad_norm': 0.08139663189649582, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3536, 'grad_norm': 0.06778010725975037, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3271, 'grad_norm': 0.08164434880018234, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.5671, 'grad_norm': 0.07171840220689774, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.4746, 'grad_norm': 0.08569923043251038, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.4509, 'grad_norm': 0.08013361692428589, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.381, 'grad_norm': 0.10423626005649567, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.322, 'grad_norm': 0.05965640768408775, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 0.5593, 'grad_norm': 0.08045705407857895, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4657, 'grad_norm': 0.07342346012592316, 'learning_rate': 0.0002, 'epoch': 0.37}
