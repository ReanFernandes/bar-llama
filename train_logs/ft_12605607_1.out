Running configuration: 
[2025-01-11 21:24:42,130][root][INFO] - Global seed set to 21
[2025-01-11 21:24:42,130][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-11 21:24:42,130][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-11 21:24:42,130][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-11 21:24:42,130][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : json 
[2025-01-11 21:24:50,601][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-11 21:24:50,615][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-11 21:24:50,618][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-11 21:24:50,618][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-11 21:24:50,621][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'json', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-11 21:24:51,108][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-11 21:24:53,508][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-11 21:24:55,751][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'gate_proj', 'down_proj', 'k_proj', 'v_proj', 'q_proj', 'up_proj', 'o_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-11 21:24:58,130][root][INFO] - Lora adapter added to model
{'loss': 1.1202, 'grad_norm': 0.11350949108600616, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.6197, 'grad_norm': 0.10020343214273453, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.4432, 'grad_norm': 0.106434665620327, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.415, 'grad_norm': 0.06760675460100174, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.3706, 'grad_norm': 0.07031232118606567, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.6106, 'grad_norm': 0.07873176038265228, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.4705, 'grad_norm': 0.07217215746641159, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.4647, 'grad_norm': 0.07037264853715897, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.3709, 'grad_norm': 0.06187886744737625, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.3328, 'grad_norm': 0.07030707597732544, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.5077, 'grad_norm': 0.07254288345575333, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.4738, 'grad_norm': 0.06657897680997849, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.4244, 'grad_norm': 0.06263209134340286, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.3575, 'grad_norm': 0.07357046008110046, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.2983, 'grad_norm': 0.06459955871105194, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.5386, 'grad_norm': 0.0633276030421257, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.5153, 'grad_norm': 0.0759698897600174, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.4205, 'grad_norm': 0.06911087781190872, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.3979, 'grad_norm': 0.06950992345809937, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.2969, 'grad_norm': 0.06859486550092697, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 0.5312, 'grad_norm': 0.06515855342149734, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4338, 'grad_norm': 0.05563786253333092, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4338, 'grad_norm': 0.05723073333501816, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.3948, 'grad_norm': 0.06277695298194885, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.2987, 'grad_norm': 0.05663685500621796, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.5647, 'grad_norm': 0.06616692990064621, 'learning_rate': 0.0002, 'epoch': 0.17}
{'loss': 0.4786, 'grad_norm': 0.06693298369646072, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.4039, 'grad_norm': 0.07808434218168259, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.3691, 'grad_norm': 0.06069018319249153, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.3096, 'grad_norm': 0.059421271085739136, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.5512, 'grad_norm': 0.0698784664273262, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.4315, 'grad_norm': 0.07182393223047256, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.4023, 'grad_norm': 0.07638567686080933, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.3645, 'grad_norm': 0.07527361810207367, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.279, 'grad_norm': 0.059024106711149216, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.5567, 'grad_norm': 0.07563198357820511, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.4787, 'grad_norm': 0.06906436383724213, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.3717, 'grad_norm': 0.05605846643447876, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.3515, 'grad_norm': 0.07243036478757858, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.3108, 'grad_norm': 0.05580361187458038, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.5704, 'grad_norm': 0.0748794749379158, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.4512, 'grad_norm': 0.06209848076105118, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.4069, 'grad_norm': 0.08074331283569336, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.3869, 'grad_norm': 0.06630681455135345, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.2734, 'grad_norm': 0.061377767473459244, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.5232, 'grad_norm': 0.06029374152421951, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.4394, 'grad_norm': 0.06525780260562897, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.3976, 'grad_norm': 0.08251427859067917, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3538, 'grad_norm': 0.06912517547607422, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3271, 'grad_norm': 0.08068734407424927, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.5672, 'grad_norm': 0.07000602036714554, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.4746, 'grad_norm': 0.08586537837982178, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.4511, 'grad_norm': 0.07842887192964554, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3809, 'grad_norm': 0.10208985954523087, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3221, 'grad_norm': 0.08812418580055237, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 0.56, 'grad_norm': 0.08032510429620743, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4657, 'grad_norm': 0.07387390732765198, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4316, 'grad_norm': 0.07821615040302277, 'learning_rate': 0.0002, 'epoch': 0.38}
{'loss': 0.3649, 'grad_norm': 0.07285413891077042, 'learning_rate': 0.0002, 'epoch': 0.39}
