Running configuration: 
[2025-01-12 12:03:50,749][root][INFO] - Global seed set to 21
[2025-01-12 12:03:50,749][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-12 12:03:50,749][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-12 12:03:50,749][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-12 12:03:50,749][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : json 
[2025-01-12 12:03:59,569][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-12 12:03:59,582][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-12 12:03:59,584][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-12 12:03:59,585][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-12 12:03:59,587][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'json', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-12 12:04:00,028][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:04:02,961][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:04:05,246][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'up_proj', 'o_proj', 'down_proj', 'gate_proj', 'v_proj', 'k_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-12 12:04:07,625][root][INFO] - Lora adapter added to model
{'loss': 1.1202, 'grad_norm': 0.11350957304239273, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.6197, 'grad_norm': 0.10020339488983154, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.4432, 'grad_norm': 0.10643438249826431, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.415, 'grad_norm': 0.06760834902524948, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.3706, 'grad_norm': 0.07032699882984161, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.6106, 'grad_norm': 0.07883547246456146, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.4705, 'grad_norm': 0.07200973480939865, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.4647, 'grad_norm': 0.07067655771970749, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.3708, 'grad_norm': 0.06179169565439224, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.3328, 'grad_norm': 0.07042616605758667, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.5076, 'grad_norm': 0.0710788443684578, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.474, 'grad_norm': 0.06622476875782013, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.4243, 'grad_norm': 0.06292401999235153, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.3572, 'grad_norm': 0.07294753938913345, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.2983, 'grad_norm': 0.06457025557756424, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.5389, 'grad_norm': 0.0632113516330719, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.5154, 'grad_norm': 0.07715503126382828, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.4206, 'grad_norm': 0.06873579323291779, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.3977, 'grad_norm': 0.06923467665910721, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.2969, 'grad_norm': 0.06821741908788681, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 0.5311, 'grad_norm': 0.06573357433080673, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.434, 'grad_norm': 0.05612478777766228, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4338, 'grad_norm': 0.056731853634119034, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.3945, 'grad_norm': 0.06233365461230278, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.2985, 'grad_norm': 0.056603267788887024, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.5645, 'grad_norm': 0.06690173596143723, 'learning_rate': 0.0002, 'epoch': 0.17}
{'loss': 0.4785, 'grad_norm': 0.06607062369585037, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.404, 'grad_norm': 0.07947622239589691, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.369, 'grad_norm': 0.06040734052658081, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.3096, 'grad_norm': 0.0589829757809639, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.5514, 'grad_norm': 0.0671699270606041, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.4312, 'grad_norm': 0.07131802290678024, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.4024, 'grad_norm': 0.07622560113668442, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.3645, 'grad_norm': 0.07629912346601486, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.279, 'grad_norm': 0.05875755846500397, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.5566, 'grad_norm': 0.07542398571968079, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.4783, 'grad_norm': 0.06837976723909378, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.3718, 'grad_norm': 0.05593577399849892, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.3515, 'grad_norm': 0.07196065783500671, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.3108, 'grad_norm': 0.055431660264730453, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.57, 'grad_norm': 0.07285106927156448, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.4513, 'grad_norm': 0.06189507618546486, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.4065, 'grad_norm': 0.08012323081493378, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.3863, 'grad_norm': 0.06663207709789276, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.273, 'grad_norm': 0.06120486557483673, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.523, 'grad_norm': 0.060285527259111404, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.439, 'grad_norm': 0.06507083773612976, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.3975, 'grad_norm': 0.08220971375703812, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3544, 'grad_norm': 0.06856314092874527, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3269, 'grad_norm': 0.08128567785024643, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.5672, 'grad_norm': 0.07037030160427094, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.4749, 'grad_norm': 0.08448988944292068, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.4507, 'grad_norm': 0.07879748195409775, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.381, 'grad_norm': 0.1047789603471756, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3222, 'grad_norm': 0.059642642736434937, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 0.5596, 'grad_norm': 0.08136962354183197, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4654, 'grad_norm': 0.07428281754255295, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4313, 'grad_norm': 0.07769563794136047, 'learning_rate': 0.0002, 'epoch': 0.38}
{'loss': 0.3646, 'grad_norm': 0.0718403309583664, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.2814, 'grad_norm': 0.07885834574699402, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.5004, 'grad_norm': 0.08235030621290207, 'learning_rate': 0.0002, 'epoch': 0.4}
{'loss': 0.4953, 'grad_norm': 0.0824558436870575, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.4281, 'grad_norm': 0.08494792878627777, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.3917, 'grad_norm': 0.07488419115543365, 'learning_rate': 0.0002, 'epoch': 0.42}
{'loss': 0.3245, 'grad_norm': 0.060802798718214035, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.52, 'grad_norm': 0.07385934889316559, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.4165, 'grad_norm': 0.08503077924251556, 'learning_rate': 0.0002, 'epoch': 0.44}
{'loss': 0.3767, 'grad_norm': 0.06551099568605423, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3511, 'grad_norm': 0.06985651701688766, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3078, 'grad_norm': 0.0660005584359169, 'learning_rate': 0.0002, 'epoch': 0.46}
{'loss': 0.5965, 'grad_norm': 0.08131152391433716, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.4313, 'grad_norm': 0.07650928944349289, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.4025, 'grad_norm': 0.08820905536413193, 'learning_rate': 0.0002, 'epoch': 0.48}
{'loss': 0.3489, 'grad_norm': 0.07409965991973877, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.3238, 'grad_norm': 0.06419285386800766, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.5085, 'grad_norm': 0.06626804918050766, 'learning_rate': 0.0002, 'epoch': 0.5}
{'loss': 0.456, 'grad_norm': 0.08634334802627563, 'learning_rate': 0.0002, 'epoch': 0.51}
{'loss': 0.3877, 'grad_norm': 0.07792700827121735, 'learning_rate': 0.0002, 'epoch': 0.51}
{'loss': 0.3197, 'grad_norm': 0.07010533660650253, 'learning_rate': 0.0002, 'epoch': 0.52}
{'loss': 0.2758, 'grad_norm': 0.06200006976723671, 'learning_rate': 0.0002, 'epoch': 0.53}
{'loss': 0.5285, 'grad_norm': 0.07800169289112091, 'learning_rate': 0.0002, 'epoch': 0.53}
{'loss': 0.4354, 'grad_norm': 0.0903724655508995, 'learning_rate': 0.0002, 'epoch': 0.54}
{'loss': 0.4171, 'grad_norm': 0.08771881461143494, 'learning_rate': 0.0002, 'epoch': 0.54}
{'loss': 0.3824, 'grad_norm': 0.07748671621084213, 'learning_rate': 0.0002, 'epoch': 0.55}
{'loss': 0.2716, 'grad_norm': 0.06484313309192657, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.535, 'grad_norm': 0.09997418522834778, 'learning_rate': 0.0002, 'epoch': 0.56}
{'loss': 0.4518, 'grad_norm': 0.080507293343544, 'learning_rate': 0.0002, 'epoch': 0.57}
{'loss': 0.4138, 'grad_norm': 0.07726677507162094, 'learning_rate': 0.0002, 'epoch': 0.58}
{'loss': 0.3662, 'grad_norm': 0.06203822046518326, 'learning_rate': 0.0002, 'epoch': 0.58}
{'loss': 0.2929, 'grad_norm': 0.07032963633537292, 'learning_rate': 0.0002, 'epoch': 0.59}
{'loss': 0.559, 'grad_norm': 0.08331375569105148, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.4895, 'grad_norm': 0.0795142650604248, 'learning_rate': 0.0002, 'epoch': 0.6}
{'loss': 0.42, 'grad_norm': 0.07641928642988205, 'learning_rate': 0.0002, 'epoch': 0.61}
{'loss': 0.3459, 'grad_norm': 0.06898043304681778, 'learning_rate': 0.0002, 'epoch': 0.62}
{'loss': 0.2854, 'grad_norm': 0.06826243549585342, 'learning_rate': 0.0002, 'epoch': 0.62}
{'loss': 0.5276, 'grad_norm': 0.07135549932718277, 'learning_rate': 0.0002, 'epoch': 0.63}
