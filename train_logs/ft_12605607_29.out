Running configuration: 
[2025-01-12 12:11:53,009][root][INFO] - Global seed set to 21
[2025-01-12 12:11:53,009][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-12 12:11:53,009][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-12 12:11:53,009][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-12 12:11:53,009][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : json 
[2025-01-12 12:12:01,779][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-12 12:12:01,792][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-12 12:12:01,795][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-12 12:12:01,795][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-12 12:12:01,797][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'json', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-12 12:12:02,223][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:12:04,845][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:12:07,095][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'gate_proj', 'q_proj', 'k_proj', 'down_proj', 'v_proj', 'o_proj', 'up_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-12 12:12:09,491][root][INFO] - Lora adapter added to model
{'loss': 1.1202, 'grad_norm': 0.11350946873426437, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.6197, 'grad_norm': 0.10020337998867035, 'learning_rate': 0.0002, 'epoch': 0.01}
{'loss': 0.4432, 'grad_norm': 0.10643436759710312, 'learning_rate': 0.0002, 'epoch': 0.02}
{'loss': 0.415, 'grad_norm': 0.06760850548744202, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.3706, 'grad_norm': 0.07032493501901627, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.6106, 'grad_norm': 0.07879547774791718, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 0.4705, 'grad_norm': 0.07207304984331131, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.4647, 'grad_norm': 0.07083415985107422, 'learning_rate': 0.0002, 'epoch': 0.05}
{'loss': 0.3708, 'grad_norm': 0.06147566810250282, 'learning_rate': 0.0002, 'epoch': 0.06}
{'loss': 0.3328, 'grad_norm': 0.07130195200443268, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.5079, 'grad_norm': 0.07298648357391357, 'learning_rate': 0.0002, 'epoch': 0.07}
{'loss': 0.4738, 'grad_norm': 0.06655581295490265, 'learning_rate': 0.0002, 'epoch': 0.08}
{'loss': 0.4245, 'grad_norm': 0.062884122133255, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.3574, 'grad_norm': 0.07378055155277252, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 0.2982, 'grad_norm': 0.0641392394900322, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 0.5388, 'grad_norm': 0.06250398606061935, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.5156, 'grad_norm': 0.07515615224838257, 'learning_rate': 0.0002, 'epoch': 0.11}
{'loss': 0.4206, 'grad_norm': 0.06904425472021103, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.3978, 'grad_norm': 0.06917393952608109, 'learning_rate': 0.0002, 'epoch': 0.12}
{'loss': 0.2969, 'grad_norm': 0.0694180577993393, 'learning_rate': 0.0002, 'epoch': 0.13}
{'loss': 0.5313, 'grad_norm': 0.06587792187929153, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.434, 'grad_norm': 0.05665324255824089, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 0.4337, 'grad_norm': 0.0565292164683342, 'learning_rate': 0.0002, 'epoch': 0.15}
{'loss': 0.3947, 'grad_norm': 0.06367556750774384, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.2985, 'grad_norm': 0.05653036758303642, 'learning_rate': 0.0002, 'epoch': 0.16}
{'loss': 0.5645, 'grad_norm': 0.066282257437706, 'learning_rate': 0.0002, 'epoch': 0.17}
{'loss': 0.4782, 'grad_norm': 0.06728669255971909, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.4041, 'grad_norm': 0.07906338572502136, 'learning_rate': 0.0002, 'epoch': 0.18}
{'loss': 0.3691, 'grad_norm': 0.06045468524098396, 'learning_rate': 0.0002, 'epoch': 0.19}
{'loss': 0.3096, 'grad_norm': 0.05986471846699715, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.551, 'grad_norm': 0.06849099695682526, 'learning_rate': 0.0002, 'epoch': 0.2}
{'loss': 0.4313, 'grad_norm': 0.07155878841876984, 'learning_rate': 0.0002, 'epoch': 0.21}
{'loss': 0.4023, 'grad_norm': 0.08302545547485352, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.3646, 'grad_norm': 0.07634921371936798, 'learning_rate': 0.0002, 'epoch': 0.22}
{'loss': 0.2796, 'grad_norm': 0.0587167851626873, 'learning_rate': 0.0002, 'epoch': 0.23}
{'loss': 0.5565, 'grad_norm': 0.07543720304965973, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.4785, 'grad_norm': 0.06880656629800797, 'learning_rate': 0.0002, 'epoch': 0.24}
{'loss': 0.3717, 'grad_norm': 0.056401412934064865, 'learning_rate': 0.0002, 'epoch': 0.25}
{'loss': 0.3515, 'grad_norm': 0.0727822557091713, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.3107, 'grad_norm': 0.055686548352241516, 'learning_rate': 0.0002, 'epoch': 0.26}
{'loss': 0.5701, 'grad_norm': 0.07314151525497437, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 0.4516, 'grad_norm': 0.06266367435455322, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.4063, 'grad_norm': 0.08092419058084488, 'learning_rate': 0.0002, 'epoch': 0.28}
{'loss': 0.3866, 'grad_norm': 0.06624839454889297, 'learning_rate': 0.0002, 'epoch': 0.29}
{'loss': 0.273, 'grad_norm': 0.06137770414352417, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.523, 'grad_norm': 0.060770273208618164, 'learning_rate': 0.0002, 'epoch': 0.3}
{'loss': 0.4394, 'grad_norm': 0.06518203765153885, 'learning_rate': 0.0002, 'epoch': 0.31}
{'loss': 0.3975, 'grad_norm': 0.08225137740373611, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3539, 'grad_norm': 0.06860393285751343, 'learning_rate': 0.0002, 'epoch': 0.32}
{'loss': 0.3267, 'grad_norm': 0.08546145260334015, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.5673, 'grad_norm': 0.07050636410713196, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 0.4747, 'grad_norm': 0.08430501073598862, 'learning_rate': 0.0002, 'epoch': 0.34}
{'loss': 0.4508, 'grad_norm': 0.07946693897247314, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3809, 'grad_norm': 0.105096235871315, 'learning_rate': 0.0002, 'epoch': 0.35}
{'loss': 0.3226, 'grad_norm': 0.0609516017138958, 'learning_rate': 0.0002, 'epoch': 0.36}
{'loss': 0.5593, 'grad_norm': 0.08007961511611938, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4655, 'grad_norm': 0.07511046528816223, 'learning_rate': 0.0002, 'epoch': 0.37}
{'loss': 0.4309, 'grad_norm': 0.07780508697032928, 'learning_rate': 0.0002, 'epoch': 0.38}
{'loss': 0.3647, 'grad_norm': 0.072154700756073, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.2815, 'grad_norm': 0.07886430621147156, 'learning_rate': 0.0002, 'epoch': 0.39}
{'loss': 0.5007, 'grad_norm': 0.08193358778953552, 'learning_rate': 0.0002, 'epoch': 0.4}
{'loss': 0.4952, 'grad_norm': 0.08264979720115662, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.4285, 'grad_norm': 0.08424658328294754, 'learning_rate': 0.0002, 'epoch': 0.41}
{'loss': 0.392, 'grad_norm': 0.0766969695687294, 'learning_rate': 0.0002, 'epoch': 0.42}
{'loss': 0.324, 'grad_norm': 0.0607336163520813, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.5199, 'grad_norm': 0.07343382388353348, 'learning_rate': 0.0002, 'epoch': 0.43}
{'loss': 0.4162, 'grad_norm': 0.0853901132941246, 'learning_rate': 0.0002, 'epoch': 0.44}
{'loss': 0.3762, 'grad_norm': 0.06418195366859436, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3512, 'grad_norm': 0.07021138072013855, 'learning_rate': 0.0002, 'epoch': 0.45}
{'loss': 0.3072, 'grad_norm': 0.06635034829378128, 'learning_rate': 0.0002, 'epoch': 0.46}
{'loss': 0.597, 'grad_norm': 0.0818684995174408, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.4309, 'grad_norm': 0.07739074528217316, 'learning_rate': 0.0002, 'epoch': 0.47}
{'loss': 0.4027, 'grad_norm': 0.08779650926589966, 'learning_rate': 0.0002, 'epoch': 0.48}
{'loss': 0.3491, 'grad_norm': 0.07493960857391357, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.3242, 'grad_norm': 0.06404196470975876, 'learning_rate': 0.0002, 'epoch': 0.49}
{'loss': 0.5084, 'grad_norm': 0.06538261473178864, 'learning_rate': 0.0002, 'epoch': 0.5}
