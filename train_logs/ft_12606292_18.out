Running configuration: seeds=seed_21 dataset=all_domains_all_samples  train=markdown_answer_first_zero_shot_structured
[2025-01-12 12:48:13,402][root][INFO] - Global seed set to 21
[2025-01-12 12:48:13,402][root][WARNING] - No quantisation will be done on loaded model, THIS IS A LORA FINE-TUNING RUN
[2025-01-12 12:48:13,402][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : False 
 Perform Validation on epoch end : False
[2025-01-12 12:48:13,402][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-01-12 12:48:13,402][root][INFO] - Current training config is : 
 Prompt type : zero_shot 
 Response type : answer_first 
 Explanation type : structured 
 Response Format : markdown 
[2025-01-12 12:48:22,426][root][INFO] - Loading dataset from /work/dlclarge1/fernandr-thesis_workspace/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-01-12 12:48:22,437][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-01-12 12:48:22,440][root][INFO] - Indexing complete, sampling of questions set to True
[2025-01-12 12:48:22,440][root][INFO] - Questions selected, dataset contains 1523 questions with 217.57142857142858 questions per domain
[2025-01-12 12:48:22,442][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'zero_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'answer_first', 'response_format': 'markdown', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/fernandr/.cache/huggingface/token
Login successful
[2025-01-12 12:48:22,895][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:48:25,456][root][INFO] - Model meta-llama/Llama-2-7b-hf loaded successfully
[2025-01-12 12:48:27,697][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=128, target_modules={'k_proj', 'down_proj', 'up_proj', 'v_proj', 'o_proj', 'gate_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.5, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
[2025-01-12 12:48:30,070][root][INFO] - Lora adapter added to model
