train: True
dataset_label: "none_fact_first_few_shot_structured"
dataset_path:  ${hydra:runtime.cwd}/dataset/tokenizable_dataset/train/${train.dataset_label}

wandb:
  model_id: ${model.model_label}_${train.dataset_label}
  project_name: thesis_sft
  run_name: fine_tuning_${train.wandb.model_id}
lora_config:
  r: 8
  lora_alpha: 16
  bias: "none"
  lora_dropout: 0.5 # higher drop out because more params than tokens
  task_type: "CAUSAL_LM"
  target_modules: [
    "q_proj",
    "up_proj",
    "o_proj",
    "k_proj",
    "down_proj",
    "gate_proj",
    "v_proj"]

training_args:
  output_dir: ${hydra:runtime.cwd}/../../train_results
  num_train_epochs: 5
  per_device_train_batch_size: 1
  gradient_checkpointing: True
  gradient_accumulation_steps: 1
  optim: "paged_adamw_32bit"
  save_steps: 25
  logging_steps: 10
  learning_rate: 2e-4
  weight_decay: 0.01 #l2 reg
  fp16: False
  bf16: False
  max_grad_norm: 0.3
  max_steps: -1
  group_by_length: True
  lr_scheduler_type: "constant"
  report_to: "wandb"

new_model_name: ${model.model_label}_${train.dataset_label}
model_save_directory: ${hydra:runtime.cwd}/../../saved_models/${train.new_model_name}