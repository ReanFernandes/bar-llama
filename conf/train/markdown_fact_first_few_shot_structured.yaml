train_config_label : markdown_fact_first_few_shot_structured

prompt: 
  prompt_type: few_shot
  example_path: ${hydra:runtime.cwd}/prompt/examples/structured_example.json # this gets ignored if prompt_type is set to zero_shot
  explanation_type: structured
  response_type: fact_first
  response_format: markdown
  store_prompt: True # true if compiling trainset, false if using for eval ( so that there is no need to store the prompt as a batch)
  system_prompt: ${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt # this thing is completed in the prompt handler to put together the actual system prompt file name
  include_system_prompt: True # Set to true if we want the instruction to be there in the system prompt and False otherwise

wandb:
  model_id: ${model.model_label}_${train.train_config_label}
  project_name: bar_llama
  run_name: fine_tune_${train.train_config_label}
  tags: [${model.model_label}, ${train.train_config_label}]

lora_config:
  r: 8
  lora_alpha: 16
  bias: "none"
  lora_dropout: 0.5 # higher drop out because more params than tokens
  task_type: "CAUSAL_LM"
  target_modules: [
    "q_proj",
    "up_proj",
    "o_proj",
    "k_proj",
    "down_proj",
    "gate_proj",
    "v_proj"]

training_args:
  output_dir: ${hydra:runtime.cwd}/train_checkpoints/${train.train_config_label}
  num_train_epochs: 5
  per_device_train_batch_size: 1
  gradient_checkpointing: True
  gradient_accumulation_steps: 1
  optim: "paged_adamw_32bit"
  save_steps: 500
  logging_steps: 10
  learning_rate: 2e-4
  weight_decay: 0.01 #l2 reg
  fp16: False
  bf16: False
  max_grad_norm: 0.3
  max_steps: -1
  group_by_length: True
  lr_scheduler_type: "constant"
  report_to: "wandb"

model_adapter_name: ${model.model_label}_${train.train_config_label}
lora_adapter_path: ${hydra:runtime.cwd}/sft_adapters/${train.model_adapter_name}