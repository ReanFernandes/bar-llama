train_config_label : json_fact_first_few_shot_unstructured

prompt: 
  prompt_type: few_shot
  example_path: ${hydra:runtime.cwd}/prompt/examples/unstructured_example.json # this gets ignored if prompt_type is set to zero_shot
  explanation_type: unstructured
  response_type: fact_first
  response_format: json
  store_prompt: True # true if compiling trainset, false if using for eval ( so that there is no need to store the prompt as a batch)
  system_prompt: ${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt # this thing is completed in the prompt handler to put together the actual system prompt file name
  include_system_prompt: True # Set to true if we want the instruction to be there in the system prompt and False otherwise

wandb:
  model_id: ${model.model_label}_${train.train_config_label}
  project_name: bar_llama
  run_name: fine_tune_${train.train_config_label}
  # tags: [${model.model_label}, ${train.train_config_label}]

lora_config:
  r: 64  # Increased for more capacity
  lora_alpha: 32  # Increased to maintain alpha/r ratio
  bias: "none"
  lora_dropout: 0.05  # Reduced for small dataset
  task_type: "CAUSAL_LM"
  target_modules: [
    "q_proj",
    "up_proj",
    "o_proj",
    "k_proj",
    "down_proj",
    "gate_proj",
    "v_proj"]

training_args:
  output_dir: ${hydra:runtime.cwd}/train_checkpoints/${train.train_config_label}
  num_train_epochs: 10  # Increased for better learning on small dataset
  per_device_train_batch_size: 1  # Kept for memory constraints
  gradient_checkpointing: True
  gradient_accumulation_steps: 8  # Increased for stability
  optim: "paged_adamw_32bit"
  save_steps: 500
  logging_steps: 10
  learning_rate: 1e-4  # Reduced for stability
  weight_decay: 0.01
  fp16: False
  bf16: False
  max_grad_norm: 0.3
  max_steps: -1
  group_by_length: True
  lr_scheduler_type: "cosine_with_restarts"  # Better exploration
  warmup_ratio: 0.1  # Added warmup
  num_cycles: 3  # Number of cosine cycles
  report_to: "wandb"

model_adapter_name: ${model.model_label}_${train.train_config_label}
lora_adapter_path: ${hydra:runtime.cwd}/sft_adapters # this is the raw path, the final sys path will be resolved while the code runs