# this must be set in the slurm bash, depending on which combination must be checked
training_status: <training_status> # "trained" or  "untrained"
quantisation_status: <quantisation_status> # "quantised_model" or "full_model"

#hardcoded for this config file. Not to be changed
train_config_label: <response_format>_<response_type>_<prompt_type>_<explanation_type>
model_label: ${model.model_label}
eval_model_name: ${eval.model_label}_${eval.train_config_label}
mode: "eval"
pipeline_available: None # again must be true or false, trained models cant run pipeline, only untrained models can
lora_adapter_path: ${hydra:runtime.cwd}/sft_adapters/${eval.eval_model_name}
output_directory: ${hydra:runtime.cwd}/model_outputs/raw_responses

prompt: 
  system_prompt: ${hydra:runtime.cwd}/prompt/system_prompt/system # this thing is completed in the prompt handler to put together the actual system prompt file name
  prompt_type: <prompt_type>  # "few_shot" or "zero_shot"
  example_path: ${hydra:runtime.cwd}/prompt/examples/<explanation_type>_example.json
  explanation_type: <explanation_type> # "structured" or "unstructured"
  response_type: <response_type> # "answer_first" or "fact_first"
  response_format: <response_format> # "json" or "markdown" or "number_list"
  store_prompt: False # by default, since we dont care about storing the question and stuff, this is only set to true when we make the training dataset
  include_system_prompt: True # this is set to true if our prompt has an instructtion, and false otherwise