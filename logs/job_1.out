Starting job 1 at Mo 10. Feb 00:03:44 CET 2025
Configuration: dataset=all_domains_all_samples prompt=markdown_fact_first_few_shot_structured train=markdown_fact_first_few_shot_structured ++train.training_args.per_device_train_batch_size=8 ++train.training_args.gradient_accumulation_steps=2
[2025-02-10 00:04:10,235][root][INFO] - Global seed set to 314
[2025-02-10 00:04:10,237][root][WARNING] - Quantisation will be done on loaded model, THIS IS A QLORA FINE-TUNING RUN
[2025-02-10 00:04:10,237][root][WARNING] - Following flags are set for the run : 
 Use Quantisation before fine-tuning : True 
 Perform Validation on epoch end : False
[2025-02-10 00:04:10,237][root][INFO] - Running Fine-tuning on the all_domains_all_samples dataset
[2025-02-10 00:04:10,237][root][INFO] - Current training config is : 
 Prompt type : few_shot 
 Response type : fact_first 
 Explanation type : structured 
 Response Format : markdown 
[2025-02-10 00:04:10,237][root][INFO] - Loading dataset from /home/fr/fr_fr/fr_rf1031/bar-llama/dataset/seed_dataset/distilled_dataset.json
[2025-02-10 00:04:10,274][root][INFO] - Dataset loaded successfully, indexing questions...
[2025-02-10 00:04:10,765][root][INFO] - Indexing complete, sampling of questions set to True
[2025-02-10 00:04:10,766][root][INFO] - Questions selected, dataset contains 1512 questions with 216.0 questions per domain
[2025-02-10 00:04:10,798][root][INFO] - Prompt Handler initialized with configuration: {'prompt_type': 'few_shot', 'example_path': '${hydra:runtime.cwd}/prompt/examples/structured_example.json', 'explanation_type': 'structured', 'response_type': 'fact_first', 'response_format': 'markdown', 'store_prompt': True, 'system_prompt': '${hydra:runtime.cwd}/prompt/system_prompt/system_${train.prompt.response_format}_${train.prompt.response_type}_${train.prompt.explanation_type}.txt', 'include_system_prompt': True}
[2025-02-10 00:04:12,077][root][INFO] - Tokenizer meta-llama/Llama-2-7b-hf loaded successfully
[2025-02-10 00:04:41,956][root][INFO] - Quantised model meta-llama/Llama-2-7b-hf loaded successfully
[2025-02-10 00:05:15,818][root][INFO] - Model prepared for kbit training
[2025-02-10 00:05:15,818][root][INFO] - Lora config loaded, following are the details : 
 LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules={'q_proj', 'gate_proj', 'k_proj', 'o_proj', 'up_proj', 'down_proj', 'v_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))
[2025-02-10 00:05:17,518][root][INFO] - Lora adapter added to model
[2025-02-10 00:05:18,939][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
