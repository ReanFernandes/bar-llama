Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.92s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.46s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<00:00, 508.36 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<00:00, 481.20 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_185956-tvdhj071
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_answer_first_few_shot_unstructured_seed_314_llama2
wandb: â­ï¸ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: ðŸš€ View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/tvdhj071
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:27<1:07:57, 27.37s/it]  1%|â–         | 2/150 [00:52<1:04:32, 26.17s/it]  2%|â–         | 3/150 [01:17<1:02:01, 25.32s/it]                                                   2%|â–         | 3/150 [01:17<1:02:01, 25.32s/it]  3%|â–Ž         | 4/150 [01:42<1:01:43, 25.37s/it]  3%|â–Ž         | 5/150 [02:06<1:00:27, 25.02s/it]  4%|â–         | 6/150 [02:32<1:00:28, 25.19s/it]                                                   4%|â–         | 6/150 [02:32<1:00:28, 25.19s/it]  5%|â–         | 7/150 [02:56<59:30, 24.97s/it]    5%|â–Œ         | 8/150 [03:22<59:35, 25.18s/it]  6%|â–Œ         | 9/150 [03:47<58:42, 24.98s/it]                                                 6%|â–Œ         | 9/150 [03:47<58:42, 24.98s/it]  7%|â–‹         | 10/150 [04:12<58:46, 25.19s/it]  7%|â–‹         | 11/150 [04:37<57:56, 25.01s/it]  8%|â–Š         | 12/150 [05:03<58:01, 25.23s/it]                                                  8%|â–Š         | 12/150 [05:03<58:01, 25.23s/it]  9%|â–Š         | 13/150 [05:27<57:09, 25.04s/it]  9%|â–‰         | 14/150 [05:53<57:12, 25.24s/it] 10%|â–ˆ         | 15/150 [06:19<57:05, 25.38s/it]                                                 10%|â–ˆ         | 15/150 [06:19<57:05, 25.38s/it] 11%|â–ˆ         | 16/150 [06:43<55:58, 25.06s/it] 11%|â–ˆâ–        | 17/150 [07:07<55:13, 24.92s/it] 12%|â–ˆâ–        | 18/150 [07:33<55:17, 25.13s/it]                                                 12%|â–ˆâ–        | 18/150 [07:33<55:17, 25.13s/it] 13%|â–ˆâ–Ž        | 19/150 [07:58<54:29, 24.96s/it] 13%|â–ˆâ–Ž        | 20/150 [08:23<54:30, 25.15s/it] 14%|â–ˆâ–        | 21/150 [08:48<53:41, 24.97s/it]                                                 14%|â–ˆâ–        | 21/150 [08:48<53:41, 24.97s/it] 15%|â–ˆâ–        | 22/150 [09:13<53:40, 25.16s/it] 15%|â–ˆâ–Œ        | 23/150 [09:38<52:50, 24.96s/it] 16%|â–ˆâ–Œ        | 24/150 [10:03<52:49, 25.15s/it]                                                 16%|â–ˆâ–Œ        | 24/150 [10:03<52:49, 25.15s/it] 17%|â–ˆâ–‹        | 25/150 [10:28<51:59, 24.96s/it] 17%|â–ˆâ–‹        | 26/150 [10:54<51:59, 25.16s/it] 18%|â–ˆâ–Š        | 27/150 [11:18<51:10, 24.97s/it]                                                 18%|â–ˆâ–Š        | 27/150 [11:18<51:10, 24.97s/it] 19%|â–ˆâ–Š        | 28/150 [11:44<51:10, 25.17s/it] 19%|â–ˆâ–‰        | 29/150 [12:08<50:21, 24.97s/it] 20%|â–ˆâ–ˆ        | 30/150 [12:33<49:42, 24.85s/it]                                                 20%|â–ˆâ–ˆ        | 30/150 [12:33<49:42, 24.85s/it] 21%|â–ˆâ–ˆ        | 31/150 [12:57<49:05, 24.75s/it] 21%|â–ˆâ–ˆâ–       | 32/150 [13:23<49:11, 25.01s/it] 22%|â–ˆâ–ˆâ–       | 33/150 [13:48<48:29, 24.87s/it]                                                 22%|â–ˆâ–ˆâ–       | 33/150 [13:48<48:29, 24.87s/it] 23%|â–ˆâ–ˆâ–Ž       | 34/150 [14:13<48:29, 25.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 35/150 [14:38<47:44, 24.91s/it] 24%|â–ˆâ–ˆâ–       | 36/150 [15:03<47:44, 25.13s/it]                                                 24%|â–ˆâ–ˆâ–       | 36/150 [15:03<47:44, 25.13s/it] 25%|â–ˆâ–ˆâ–       | 37/150 [15:28<46:58, 24.95s/it] 25%|â–ˆâ–ˆâ–Œ       | 38/150 [15:52<46:22, 24.84s/it] 26%|â–ˆâ–ˆâ–Œ       | 39/150 [16:17<45:46, 24.75s/it]                                                 26%|â–ˆâ–ˆâ–Œ       | 39/150 [16:17<45:46, 24.75s/it] 27%|â–ˆâ–ˆâ–‹       | 40/150 [16:43<45:50, 25.00s/it] 27%|â–ˆâ–ˆâ–‹       | 41/150 [17:07<45:09, 24.86s/it] 28%|â–ˆâ–ˆâ–Š       | 42/150 [17:33<45:09, 25.09s/it]                                                 28%|â–ˆâ–ˆâ–Š       | 42/150 [17:33<45:09, 25.09s/it] 29%|â–ˆâ–ˆâ–Š       | 43/150 [17:57<44:26, 24.92s/it] 29%|â–ˆâ–ˆâ–‰       | 44/150 [18:23<44:23, 25.12s/it] 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [18:47<43:38, 24.94s/it]                                                 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [18:47<43:38, 24.94s/it] 31%|â–ˆâ–ˆâ–ˆ       | 46/150 [19:13<43:35, 25.15s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 47/150 [19:37<42:50, 24.96s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [20:03<42:45, 25.15s/it]                                                 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [20:03<42:45, 25.15s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 49/150 [20:28<42:00, 24.96s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 50/150 [20:53<41:53, 25.14s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [21:18<41:10, 24.95s/it]                                                 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [21:18<41:10, 24.95s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 52/150 [21:43<41:03, 25.14s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/150 [22:08<40:20, 24.96s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [22:33<40:14, 25.16s/it]                                                 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [22:33<40:14, 25.16s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 55/150 [22:58<39:31, 24.96s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/150 [23:23<39:24, 25.15s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [23:48<38:41, 24.96s/it]                                                 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [23:48<38:41, 24.96s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 58/150 [24:14<38:34, 25.16s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/150 [24:38<37:52, 24.97s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [25:04<37:44, 25.16s/it]                                                 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [25:04<37:44, 25.16s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/150 [25:28<37:02, 24.97s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 62/150 [25:54<36:54, 25.16s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [26:18<36:12, 24.97s/it]                                                 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [26:18<36:12, 24.97s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 64/150 [26:44<36:03, 25.16s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 65/150 [27:09<35:22, 24.97s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [27:33<34:48, 24.87s/it]                                                 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [27:33<34:48, 24.87s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/150 [27:58<34:15, 24.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/150 [28:23<34:11, 25.02s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [28:48<33:34, 24.87s/it]                                                 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [28:48<33:34, 24.87s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/150 [29:13<33:27, 25.09s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/150 [29:38<32:48, 24.92s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [30:04<32:39, 25.12s/it]                                                 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [30:04<32:39, 25.12s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/150 [30:28<32:00, 24.94s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/150 [30:53<31:27, 24.83s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [31:17<30:54, 24.73s/it]                                                 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [31:17<30:54, 24.73s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/150 [31:42<30:28, 24.70s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/150 [32:06<29:59, 24.65s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [32:32<29:55, 24.94s/it]                                                 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [32:32<29:55, 24.94s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 79/150 [32:56<29:22, 24.82s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 80/150 [33:22<29:13, 25.06s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [33:47<28:37, 24.90s/it]                                                 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [33:47<28:37, 24.90s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/150 [34:12<28:27, 25.11s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 83/150 [34:37<27:50, 24.93s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [35:02<27:38, 25.14s/it]                                                 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [35:02<27:38, 25.14s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/150 [35:27<27:01, 24.95s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/150 [35:52<26:48, 25.13s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [36:17<26:11, 24.94s/it]                                                 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [36:17<26:11, 24.94s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/150 [36:42<25:59, 25.15s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/150 [37:07<25:22, 24.96s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [37:33<25:08, 25.14s/it]                                                 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [37:33<25:08, 25.14s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/150 [37:58<24:52, 25.29s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/150 [38:23<24:15, 25.10s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [38:47<23:40, 24.93s/it]                                                 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [38:47<23:40, 24.93s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 94/150 [39:13<23:27, 25.13s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 95/150 [39:38<22:51, 24.94s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [40:03<22:36, 25.13s/it]                                                 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [40:03<22:36, 25.13s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/150 [40:28<22:01, 24.94s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 98/150 [40:53<21:47, 25.14s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [41:18<21:12, 24.95s/it]                                                 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [41:18<21:12, 24.95s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 100/150 [41:43<20:57, 25.15s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/150 [42:08<20:22, 24.96s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [42:33<20:06, 25.14s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [42:33<20:06, 25.14s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/150 [42:58<19:33, 24.96s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/150 [43:24<19:17, 25.16s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [43:48<18:43, 24.97s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [43:48<18:43, 24.97s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/150 [44:14<18:26, 25.15s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107/150 [44:38<17:53, 24.97s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [45:04<17:36, 25.16s/it]                                                  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [45:04<17:36, 25.16s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 109/150 [45:28<17:03, 24.96s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 110/150 [45:54<16:46, 25.15s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [46:18<16:13, 24.97s/it]                                                  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [46:18<16:13, 24.97s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/150 [46:44<15:55, 25.16s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 113/150 [47:09<15:23, 24.96s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [47:34<15:05, 25.16s/it]                                                  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [47:34<15:05, 25.16s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 115/150 [47:59<14:33, 24.97s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/150 [48:24<14:15, 25.16s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [48:49<13:43, 24.97s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [48:49<13:43, 24.97s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/150 [49:14<13:25, 25.16s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/150 [49:39<12:53, 24.97s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [50:04<12:25, 24.86s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [50:04<12:25, 24.86s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/150 [50:28<11:58, 24.76s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122/150 [50:54<11:40, 25.01s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [51:18<11:11, 24.86s/it]                                                  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [51:18<11:11, 24.86s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 124/150 [51:44<10:52, 25.08s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 125/150 [52:08<10:22, 24.91s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [52:34<10:02, 25.12s/it]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [52:34<10:02, 25.12s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/150 [52:58<09:33, 24.95s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 128/150 [53:24<09:13, 25.16s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [53:49<08:45, 25.01s/it]                                                  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [53:49<08:45, 25.01s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 130/150 [54:14<08:24, 25.22s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/150 [54:39<07:55, 25.05s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [55:04<07:28, 24.94s/it]                                                  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [55:04<07:28, 24.94s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/150 [55:28<07:02, 24.85s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 134/150 [55:54<06:42, 25.13s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [56:19<06:14, 25.00s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [56:19<06:14, 25.00s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/150 [56:45<05:53, 25.24s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137/150 [57:09<05:26, 25.08s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [57:35<05:03, 25.31s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [57:35<05:03, 25.31s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 139/150 [58:00<04:36, 25.14s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 140/150 [58:26<04:13, 25.33s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [58:51<03:46, 25.16s/it]                                                  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [58:51<03:46, 25.16s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/150 [59:16<03:22, 25.37s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 143/150 [59:41<02:56, 25.18s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [1:00:07<02:32, 25.37s/it]                                                    96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [1:00:07<02:32, 25.37s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 145/150 [1:00:32<02:05, 25.18s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/150 [1:00:58<01:41, 25.38s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [1:01:23<01:16, 25.52s/it]                                                    98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [1:01:23<01:16, 25.52s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/150 [1:01:48<00:50, 25.33s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 149/150 [1:02:13<00:25, 25.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:02:39<00:00, 25.36s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:02:39<00:00, 25.36s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:02:39<00:00, 25.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:02:39<00:00, 25.06s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   train/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     train/grad_norm â–‚â–‚â–ƒâ–ƒâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ˆâ–ƒâ–„â–„â–„â–„â–†â–…â–„â–„â–„â–ƒâ–ƒ
wandb: train/learning_rate â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–
wandb:          train/loss â–ˆâ–ˆâ–ˆâ–‡â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               total_flos 4.358216791223501e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.20366
wandb:      train/learning_rate 0
wandb:               train/loss 0.072
wandb:               train_loss 0.46629
wandb:            train_runtime 3763.3546
wandb: train_samples_per_second 0.279
wandb:   train_steps_per_second 0.04
wandb: 
wandb: ðŸš€ View run fine_tune_json_answer_first_few_shot_unstructured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/tvdhj071
wandb: â­ï¸ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_185956-tvdhj071/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.42s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.05s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  7.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.15s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.85s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.25s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.69s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  7.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.41s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.79s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.92s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.84s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.38s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.84s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
