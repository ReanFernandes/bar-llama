Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.46s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 508.36 examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 481.20 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_185956-tvdhj071
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_answer_first_few_shot_unstructured_seed_314_llama2
wandb: ⭐️ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: 🚀 View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/tvdhj071
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:27<1:07:57, 27.37s/it]  1%|▏         | 2/150 [00:52<1:04:32, 26.17s/it]  2%|▏         | 3/150 [01:17<1:02:01, 25.32s/it]                                                   2%|▏         | 3/150 [01:17<1:02:01, 25.32s/it]  3%|▎         | 4/150 [01:42<1:01:43, 25.37s/it]  3%|▎         | 5/150 [02:06<1:00:27, 25.02s/it]  4%|▍         | 6/150 [02:32<1:00:28, 25.19s/it]                                                   4%|▍         | 6/150 [02:32<1:00:28, 25.19s/it]  5%|▍         | 7/150 [02:56<59:30, 24.97s/it]    5%|▌         | 8/150 [03:22<59:35, 25.18s/it]  6%|▌         | 9/150 [03:47<58:42, 24.98s/it]                                                 6%|▌         | 9/150 [03:47<58:42, 24.98s/it]  7%|▋         | 10/150 [04:12<58:46, 25.19s/it]  7%|▋         | 11/150 [04:37<57:56, 25.01s/it]  8%|▊         | 12/150 [05:03<58:01, 25.23s/it]                                                  8%|▊         | 12/150 [05:03<58:01, 25.23s/it]  9%|▊         | 13/150 [05:27<57:09, 25.04s/it]  9%|▉         | 14/150 [05:53<57:12, 25.24s/it] 10%|█         | 15/150 [06:19<57:05, 25.38s/it]                                                 10%|█         | 15/150 [06:19<57:05, 25.38s/it] 11%|█         | 16/150 [06:43<55:58, 25.06s/it] 11%|█▏        | 17/150 [07:07<55:13, 24.92s/it] 12%|█▏        | 18/150 [07:33<55:17, 25.13s/it]                                                 12%|█▏        | 18/150 [07:33<55:17, 25.13s/it] 13%|█▎        | 19/150 [07:58<54:29, 24.96s/it] 13%|█▎        | 20/150 [08:23<54:30, 25.15s/it] 14%|█▍        | 21/150 [08:48<53:41, 24.97s/it]                                                 14%|█▍        | 21/150 [08:48<53:41, 24.97s/it] 15%|█▍        | 22/150 [09:13<53:40, 25.16s/it] 15%|█▌        | 23/150 [09:38<52:50, 24.96s/it] 16%|█▌        | 24/150 [10:03<52:49, 25.15s/it]                                                 16%|█▌        | 24/150 [10:03<52:49, 25.15s/it] 17%|█▋        | 25/150 [10:28<51:59, 24.96s/it] 17%|█▋        | 26/150 [10:54<51:59, 25.16s/it] 18%|█▊        | 27/150 [11:18<51:10, 24.97s/it]                                                 18%|█▊        | 27/150 [11:18<51:10, 24.97s/it] 19%|█▊        | 28/150 [11:44<51:10, 25.17s/it] 19%|█▉        | 29/150 [12:08<50:21, 24.97s/it] 20%|██        | 30/150 [12:33<49:42, 24.85s/it]                                                 20%|██        | 30/150 [12:33<49:42, 24.85s/it] 21%|██        | 31/150 [12:57<49:05, 24.75s/it] 21%|██▏       | 32/150 [13:23<49:11, 25.01s/it] 22%|██▏       | 33/150 [13:48<48:29, 24.87s/it]                                                 22%|██▏       | 33/150 [13:48<48:29, 24.87s/it] 23%|██▎       | 34/150 [14:13<48:29, 25.08s/it] 23%|██▎       | 35/150 [14:38<47:44, 24.91s/it] 24%|██▍       | 36/150 [15:03<47:44, 25.13s/it]                                                 24%|██▍       | 36/150 [15:03<47:44, 25.13s/it] 25%|██▍       | 37/150 [15:28<46:58, 24.95s/it] 25%|██▌       | 38/150 [15:52<46:22, 24.84s/it] 26%|██▌       | 39/150 [16:17<45:46, 24.75s/it]                                                 26%|██▌       | 39/150 [16:17<45:46, 24.75s/it] 27%|██▋       | 40/150 [16:43<45:50, 25.00s/it] 27%|██▋       | 41/150 [17:07<45:09, 24.86s/it] 28%|██▊       | 42/150 [17:33<45:09, 25.09s/it]                                                 28%|██▊       | 42/150 [17:33<45:09, 25.09s/it] 29%|██▊       | 43/150 [17:57<44:26, 24.92s/it] 29%|██▉       | 44/150 [18:23<44:23, 25.12s/it] 30%|███       | 45/150 [18:47<43:38, 24.94s/it]                                                 30%|███       | 45/150 [18:47<43:38, 24.94s/it] 31%|███       | 46/150 [19:13<43:35, 25.15s/it] 31%|███▏      | 47/150 [19:37<42:50, 24.96s/it] 32%|███▏      | 48/150 [20:03<42:45, 25.15s/it]                                                 32%|███▏      | 48/150 [20:03<42:45, 25.15s/it] 33%|███▎      | 49/150 [20:28<42:00, 24.96s/it] 33%|███▎      | 50/150 [20:53<41:53, 25.14s/it] 34%|███▍      | 51/150 [21:18<41:10, 24.95s/it]                                                 34%|███▍      | 51/150 [21:18<41:10, 24.95s/it] 35%|███▍      | 52/150 [21:43<41:03, 25.14s/it] 35%|███▌      | 53/150 [22:08<40:20, 24.96s/it] 36%|███▌      | 54/150 [22:33<40:14, 25.16s/it]                                                 36%|███▌      | 54/150 [22:33<40:14, 25.16s/it] 37%|███▋      | 55/150 [22:58<39:31, 24.96s/it] 37%|███▋      | 56/150 [23:23<39:24, 25.15s/it] 38%|███▊      | 57/150 [23:48<38:41, 24.96s/it]                                                 38%|███▊      | 57/150 [23:48<38:41, 24.96s/it] 39%|███▊      | 58/150 [24:14<38:34, 25.16s/it] 39%|███▉      | 59/150 [24:38<37:52, 24.97s/it] 40%|████      | 60/150 [25:04<37:44, 25.16s/it]                                                 40%|████      | 60/150 [25:04<37:44, 25.16s/it] 41%|████      | 61/150 [25:28<37:02, 24.97s/it] 41%|████▏     | 62/150 [25:54<36:54, 25.16s/it] 42%|████▏     | 63/150 [26:18<36:12, 24.97s/it]                                                 42%|████▏     | 63/150 [26:18<36:12, 24.97s/it] 43%|████▎     | 64/150 [26:44<36:03, 25.16s/it] 43%|████▎     | 65/150 [27:09<35:22, 24.97s/it] 44%|████▍     | 66/150 [27:33<34:48, 24.87s/it]                                                 44%|████▍     | 66/150 [27:33<34:48, 24.87s/it] 45%|████▍     | 67/150 [27:58<34:15, 24.77s/it] 45%|████▌     | 68/150 [28:23<34:11, 25.02s/it] 46%|████▌     | 69/150 [28:48<33:34, 24.87s/it]                                                 46%|████▌     | 69/150 [28:48<33:34, 24.87s/it] 47%|████▋     | 70/150 [29:13<33:27, 25.09s/it] 47%|████▋     | 71/150 [29:38<32:48, 24.92s/it] 48%|████▊     | 72/150 [30:04<32:39, 25.12s/it]                                                 48%|████▊     | 72/150 [30:04<32:39, 25.12s/it] 49%|████▊     | 73/150 [30:28<32:00, 24.94s/it] 49%|████▉     | 74/150 [30:53<31:27, 24.83s/it] 50%|█████     | 75/150 [31:17<30:54, 24.73s/it]                                                 50%|█████     | 75/150 [31:17<30:54, 24.73s/it] 51%|█████     | 76/150 [31:42<30:28, 24.70s/it] 51%|█████▏    | 77/150 [32:06<29:59, 24.65s/it] 52%|█████▏    | 78/150 [32:32<29:55, 24.94s/it]                                                 52%|█████▏    | 78/150 [32:32<29:55, 24.94s/it] 53%|█████▎    | 79/150 [32:56<29:22, 24.82s/it] 53%|█████▎    | 80/150 [33:22<29:13, 25.06s/it] 54%|█████▍    | 81/150 [33:47<28:37, 24.90s/it]                                                 54%|█████▍    | 81/150 [33:47<28:37, 24.90s/it] 55%|█████▍    | 82/150 [34:12<28:27, 25.11s/it] 55%|█████▌    | 83/150 [34:37<27:50, 24.93s/it] 56%|█████▌    | 84/150 [35:02<27:38, 25.14s/it]                                                 56%|█████▌    | 84/150 [35:02<27:38, 25.14s/it] 57%|█████▋    | 85/150 [35:27<27:01, 24.95s/it] 57%|█████▋    | 86/150 [35:52<26:48, 25.13s/it] 58%|█████▊    | 87/150 [36:17<26:11, 24.94s/it]                                                 58%|█████▊    | 87/150 [36:17<26:11, 24.94s/it] 59%|█████▊    | 88/150 [36:42<25:59, 25.15s/it] 59%|█████▉    | 89/150 [37:07<25:22, 24.96s/it] 60%|██████    | 90/150 [37:33<25:08, 25.14s/it]                                                 60%|██████    | 90/150 [37:33<25:08, 25.14s/it] 61%|██████    | 91/150 [37:58<24:52, 25.29s/it] 61%|██████▏   | 92/150 [38:23<24:15, 25.10s/it] 62%|██████▏   | 93/150 [38:47<23:40, 24.93s/it]                                                 62%|██████▏   | 93/150 [38:47<23:40, 24.93s/it] 63%|██████▎   | 94/150 [39:13<23:27, 25.13s/it] 63%|██████▎   | 95/150 [39:38<22:51, 24.94s/it] 64%|██████▍   | 96/150 [40:03<22:36, 25.13s/it]                                                 64%|██████▍   | 96/150 [40:03<22:36, 25.13s/it] 65%|██████▍   | 97/150 [40:28<22:01, 24.94s/it] 65%|██████▌   | 98/150 [40:53<21:47, 25.14s/it] 66%|██████▌   | 99/150 [41:18<21:12, 24.95s/it]                                                 66%|██████▌   | 99/150 [41:18<21:12, 24.95s/it] 67%|██████▋   | 100/150 [41:43<20:57, 25.15s/it] 67%|██████▋   | 101/150 [42:08<20:22, 24.96s/it] 68%|██████▊   | 102/150 [42:33<20:06, 25.14s/it]                                                  68%|██████▊   | 102/150 [42:33<20:06, 25.14s/it] 69%|██████▊   | 103/150 [42:58<19:33, 24.96s/it] 69%|██████▉   | 104/150 [43:24<19:17, 25.16s/it] 70%|███████   | 105/150 [43:48<18:43, 24.97s/it]                                                  70%|███████   | 105/150 [43:48<18:43, 24.97s/it] 71%|███████   | 106/150 [44:14<18:26, 25.15s/it] 71%|███████▏  | 107/150 [44:38<17:53, 24.97s/it] 72%|███████▏  | 108/150 [45:04<17:36, 25.16s/it]                                                  72%|███████▏  | 108/150 [45:04<17:36, 25.16s/it] 73%|███████▎  | 109/150 [45:28<17:03, 24.96s/it] 73%|███████▎  | 110/150 [45:54<16:46, 25.15s/it] 74%|███████▍  | 111/150 [46:18<16:13, 24.97s/it]                                                  74%|███████▍  | 111/150 [46:18<16:13, 24.97s/it] 75%|███████▍  | 112/150 [46:44<15:55, 25.16s/it] 75%|███████▌  | 113/150 [47:09<15:23, 24.96s/it] 76%|███████▌  | 114/150 [47:34<15:05, 25.16s/it]                                                  76%|███████▌  | 114/150 [47:34<15:05, 25.16s/it] 77%|███████▋  | 115/150 [47:59<14:33, 24.97s/it] 77%|███████▋  | 116/150 [48:24<14:15, 25.16s/it] 78%|███████▊  | 117/150 [48:49<13:43, 24.97s/it]                                                  78%|███████▊  | 117/150 [48:49<13:43, 24.97s/it] 79%|███████▊  | 118/150 [49:14<13:25, 25.16s/it] 79%|███████▉  | 119/150 [49:39<12:53, 24.97s/it] 80%|████████  | 120/150 [50:04<12:25, 24.86s/it]                                                  80%|████████  | 120/150 [50:04<12:25, 24.86s/it] 81%|████████  | 121/150 [50:28<11:58, 24.76s/it] 81%|████████▏ | 122/150 [50:54<11:40, 25.01s/it] 82%|████████▏ | 123/150 [51:18<11:11, 24.86s/it]                                                  82%|████████▏ | 123/150 [51:18<11:11, 24.86s/it] 83%|████████▎ | 124/150 [51:44<10:52, 25.08s/it] 83%|████████▎ | 125/150 [52:08<10:22, 24.91s/it] 84%|████████▍ | 126/150 [52:34<10:02, 25.12s/it]                                                  84%|████████▍ | 126/150 [52:34<10:02, 25.12s/it] 85%|████████▍ | 127/150 [52:58<09:33, 24.95s/it] 85%|████████▌ | 128/150 [53:24<09:13, 25.16s/it] 86%|████████▌ | 129/150 [53:49<08:45, 25.01s/it]                                                  86%|████████▌ | 129/150 [53:49<08:45, 25.01s/it] 87%|████████▋ | 130/150 [54:14<08:24, 25.22s/it] 87%|████████▋ | 131/150 [54:39<07:55, 25.05s/it] 88%|████████▊ | 132/150 [55:04<07:28, 24.94s/it]                                                  88%|████████▊ | 132/150 [55:04<07:28, 24.94s/it] 89%|████████▊ | 133/150 [55:28<07:02, 24.85s/it] 89%|████████▉ | 134/150 [55:54<06:42, 25.13s/it] 90%|█████████ | 135/150 [56:19<06:14, 25.00s/it]                                                  90%|█████████ | 135/150 [56:19<06:14, 25.00s/it] 91%|█████████ | 136/150 [56:45<05:53, 25.24s/it] 91%|█████████▏| 137/150 [57:09<05:26, 25.08s/it] 92%|█████████▏| 138/150 [57:35<05:03, 25.31s/it]                                                  92%|█████████▏| 138/150 [57:35<05:03, 25.31s/it] 93%|█████████▎| 139/150 [58:00<04:36, 25.14s/it] 93%|█████████▎| 140/150 [58:26<04:13, 25.33s/it] 94%|█████████▍| 141/150 [58:51<03:46, 25.16s/it]                                                  94%|█████████▍| 141/150 [58:51<03:46, 25.16s/it] 95%|█████████▍| 142/150 [59:16<03:22, 25.37s/it] 95%|█████████▌| 143/150 [59:41<02:56, 25.18s/it] 96%|█████████▌| 144/150 [1:00:07<02:32, 25.37s/it]                                                    96%|█████████▌| 144/150 [1:00:07<02:32, 25.37s/it] 97%|█████████▋| 145/150 [1:00:32<02:05, 25.18s/it] 97%|█████████▋| 146/150 [1:00:58<01:41, 25.38s/it] 98%|█████████▊| 147/150 [1:01:23<01:16, 25.52s/it]                                                    98%|█████████▊| 147/150 [1:01:23<01:16, 25.52s/it] 99%|█████████▊| 148/150 [1:01:48<00:50, 25.33s/it] 99%|█████████▉| 149/150 [1:02:13<00:25, 25.16s/it]100%|██████████| 150/150 [1:02:39<00:00, 25.36s/it]                                                   100%|██████████| 150/150 [1:02:39<00:00, 25.36s/it]                                                   100%|██████████| 150/150 [1:02:39<00:00, 25.36s/it]100%|██████████| 150/150 [1:02:39<00:00, 25.06s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:     train/grad_norm ▂▂▃▃▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃█▃▄▄▄▄▆▅▄▄▄▃▃
wandb: train/learning_rate ▂▄▅▇███▇▇▇▆▅▅▄▄▃▂▂▂▁▁▁███▇▇▇▆▆▅▄▄▃▃▂▂▂▁▁
wandb:          train/loss ███▇▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 4.358216791223501e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.20366
wandb:      train/learning_rate 0
wandb:               train/loss 0.072
wandb:               train_loss 0.46629
wandb:            train_runtime 3763.3546
wandb: train_samples_per_second 0.279
wandb:   train_steps_per_second 0.04
wandb: 
wandb: 🚀 View run fine_tune_json_answer_first_few_shot_unstructured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/tvdhj071
wandb: ⭐️ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_185956-tvdhj071/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.05s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.15s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.85s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.25s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.69s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.41s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.79s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.92s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.84s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.84s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
