Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.52s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<00:00, 1236.00 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_190402-6dhnkje3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_answer_first_zero_shot_unstructured_seed_314_llama2
wandb: â­ï¸ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: ðŸš€ View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/6dhnkje3
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:25<1:03:15, 25.47s/it]  1%|â–         | 2/150 [00:39<46:01, 18.66s/it]    2%|â–         | 3/150 [01:03<51:35, 21.06s/it]                                                 2%|â–         | 3/150 [01:03<51:35, 21.06s/it]  3%|â–Ž         | 4/150 [01:16<43:18, 17.80s/it]  3%|â–Ž         | 5/150 [01:33<42:17, 17.50s/it]  4%|â–         | 6/150 [01:45<38:00, 15.84s/it]                                                 4%|â–         | 6/150 [01:45<38:00, 15.84s/it]  5%|â–         | 7/150 [02:07<42:28, 17.82s/it]  5%|â–Œ         | 8/150 [02:22<39:53, 16.86s/it]  6%|â–Œ         | 9/150 [02:41<41:09, 17.51s/it]                                                 6%|â–Œ         | 9/150 [02:41<41:09, 17.51s/it]  7%|â–‹         | 10/150 [02:54<37:56, 16.26s/it]  7%|â–‹         | 11/150 [03:19<44:00, 19.00s/it]  8%|â–Š         | 12/150 [03:35<41:27, 18.02s/it]                                                  8%|â–Š         | 12/150 [03:35<41:27, 18.02s/it]  9%|â–Š         | 13/150 [03:57<43:54, 19.23s/it]  9%|â–‰         | 14/150 [04:13<41:19, 18.23s/it] 10%|â–ˆ         | 15/150 [04:29<39:38, 17.62s/it]                                                 10%|â–ˆ         | 15/150 [04:29<39:38, 17.62s/it] 11%|â–ˆ         | 16/150 [04:41<35:21, 15.83s/it] 11%|â–ˆâ–        | 17/150 [04:59<36:42, 16.56s/it] 12%|â–ˆâ–        | 18/150 [05:12<33:57, 15.43s/it]                                                 12%|â–ˆâ–        | 18/150 [05:12<33:57, 15.43s/it] 13%|â–ˆâ–Ž        | 19/150 [05:34<37:40, 17.26s/it] 13%|â–ˆâ–Ž        | 20/150 [05:47<34:33, 15.95s/it] 14%|â–ˆâ–        | 21/150 [06:12<40:15, 18.72s/it]                                                 14%|â–ˆâ–        | 21/150 [06:12<40:15, 18.72s/it] 15%|â–ˆâ–        | 22/150 [06:25<36:37, 17.17s/it] 15%|â–ˆâ–Œ        | 23/150 [06:47<39:23, 18.61s/it] 16%|â–ˆâ–Œ        | 24/150 [07:00<35:21, 16.84s/it]                                                 16%|â–ˆâ–Œ        | 24/150 [07:00<35:21, 16.84s/it] 17%|â–ˆâ–‹        | 25/150 [07:22<38:20, 18.40s/it] 17%|â–ˆâ–‹        | 26/150 [07:37<35:50, 17.34s/it] 18%|â–ˆâ–Š        | 27/150 [08:01<39:42, 19.37s/it]                                                 18%|â–ˆâ–Š        | 27/150 [08:01<39:42, 19.37s/it] 19%|â–ˆâ–Š        | 28/150 [08:15<36:23, 17.90s/it] 19%|â–ˆâ–‰        | 29/150 [08:34<36:20, 18.02s/it] 20%|â–ˆâ–ˆ        | 30/150 [08:46<32:34, 16.29s/it]                                                 20%|â–ˆâ–ˆ        | 30/150 [08:46<32:34, 16.29s/it] 21%|â–ˆâ–ˆ        | 31/150 [09:11<37:37, 18.97s/it] 21%|â–ˆâ–ˆâ–       | 32/150 [09:24<33:44, 17.16s/it] 22%|â–ˆâ–ˆâ–       | 33/150 [09:41<33:20, 17.09s/it]                                                 22%|â–ˆâ–ˆâ–       | 33/150 [09:41<33:20, 17.09s/it] 23%|â–ˆâ–ˆâ–Ž       | 34/150 [09:54<30:37, 15.84s/it] 23%|â–ˆâ–ˆâ–Ž       | 35/150 [10:16<33:42, 17.59s/it] 24%|â–ˆâ–ˆâ–       | 36/150 [10:33<33:19, 17.54s/it]                                                 24%|â–ˆâ–ˆâ–       | 36/150 [10:33<33:19, 17.54s/it] 25%|â–ˆâ–ˆâ–       | 37/150 [10:57<36:45, 19.51s/it] 25%|â–ˆâ–ˆâ–Œ       | 38/150 [11:10<32:23, 17.35s/it] 26%|â–ˆâ–ˆâ–Œ       | 39/150 [11:29<33:03, 17.87s/it]                                                 26%|â–ˆâ–ˆâ–Œ       | 39/150 [11:29<33:03, 17.87s/it] 27%|â–ˆâ–ˆâ–‹       | 40/150 [11:42<30:24, 16.58s/it] 27%|â–ˆâ–ˆâ–‹       | 41/150 [12:07<34:51, 19.19s/it] 28%|â–ˆâ–ˆâ–Š       | 42/150 [12:22<32:12, 17.89s/it]                                                 28%|â–ˆâ–ˆâ–Š       | 42/150 [12:22<32:12, 17.89s/it] 29%|â–ˆâ–ˆâ–Š       | 43/150 [12:44<34:08, 19.15s/it] 29%|â–ˆâ–ˆâ–‰       | 44/150 [12:57<30:30, 17.27s/it] 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [13:19<32:31, 18.58s/it]                                                 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [13:19<32:31, 18.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 46/150 [13:34<30:17, 17.47s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 47/150 [13:58<33:24, 19.46s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [14:11<30:04, 17.69s/it]                                                 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [14:11<30:04, 17.69s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 49/150 [14:30<30:05, 17.88s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 50/150 [14:43<27:18, 16.39s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [15:08<31:25, 19.05s/it]                                                 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [15:08<31:25, 19.05s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 52/150 [15:21<28:05, 17.20s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/150 [15:43<30:09, 18.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [15:57<27:49, 17.39s/it]                                                 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [15:57<27:49, 17.39s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 55/150 [16:21<30:43, 19.40s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/150 [16:34<27:19, 17.44s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [16:56<28:51, 18.62s/it]                                                 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [16:56<28:51, 18.62s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 58/150 [17:11<26:49, 17.50s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/150 [17:33<28:35, 18.85s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [17:47<26:20, 17.56s/it]                                                 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [17:47<26:20, 17.56s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/150 [18:12<29:30, 19.89s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 62/150 [18:27<27:03, 18.45s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [18:52<29:12, 20.15s/it]                                                 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [18:52<29:12, 20.15s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 64/150 [19:05<25:45, 17.97s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 65/150 [19:27<27:10, 19.18s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [19:39<23:58, 17.13s/it]                                                 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [19:39<23:58, 17.13s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/150 [19:58<24:30, 17.71s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/150 [20:12<22:41, 16.61s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [20:34<24:37, 18.24s/it]                                                 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [20:34<24:37, 18.24s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/150 [20:49<22:50, 17.13s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/150 [21:14<25:46, 19.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [21:27<22:49, 17.56s/it]                                                 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [21:27<22:49, 17.56s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/150 [21:45<22:53, 17.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/150 [21:58<20:33, 16.24s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [22:20<22:27, 17.97s/it]                                                 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [22:20<22:27, 17.97s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/150 [22:32<20:04, 16.28s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/150 [22:56<22:41, 18.65s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [23:11<20:54, 17.42s/it]                                                 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [23:11<20:54, 17.42s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 79/150 [23:33<22:15, 18.81s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 80/150 [23:48<20:34, 17.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [24:13<22:56, 19.95s/it]                                                 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [24:13<22:56, 19.95s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/150 [24:28<20:50, 18.40s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 83/150 [24:52<22:27, 20.12s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [25:06<19:58, 18.16s/it]                                                 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [25:06<19:58, 18.16s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/150 [25:24<19:45, 18.23s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/150 [25:37<17:40, 16.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [25:58<18:54, 18.01s/it]                                                 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [25:58<18:54, 18.01s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/150 [26:13<17:32, 16.98s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/150 [26:35<18:47, 18.49s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [26:47<16:47, 16.80s/it]                                                 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [26:47<16:47, 16.80s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/150 [27:13<19:00, 19.33s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/150 [27:25<16:39, 17.24s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [27:49<18:21, 19.33s/it]                                                 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [27:49<18:21, 19.33s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 94/150 [28:06<17:22, 18.62s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 95/150 [28:28<17:48, 19.43s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [28:40<15:40, 17.41s/it]                                                 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [28:40<15:40, 17.41s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/150 [29:00<15:58, 18.09s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 98/150 [29:13<14:20, 16.55s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [29:31<14:33, 17.12s/it]                                                 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [29:31<14:33, 17.12s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 100/150 [29:45<13:29, 16.19s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/150 [30:11<15:26, 18.91s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [30:23<13:40, 17.10s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [30:23<13:40, 17.10s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/150 [30:45<14:23, 18.37s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/150 [30:59<13:04, 17.06s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [31:17<13:07, 17.49s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [31:17<13:07, 17.49s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/150 [31:31<11:57, 16.32s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107/150 [31:53<12:55, 18.04s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [32:07<11:47, 16.84s/it]                                                  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [32:07<11:47, 16.84s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 109/150 [32:31<13:00, 19.03s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 110/150 [32:46<11:53, 17.83s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [33:11<13:02, 20.07s/it]                                                  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [33:11<13:02, 20.07s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/150 [33:26<11:39, 18.41s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 113/150 [33:48<11:56, 19.36s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [34:00<10:27, 17.43s/it]                                                  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [34:00<10:27, 17.43s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 115/150 [34:25<11:20, 19.45s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/150 [34:39<10:10, 17.95s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [34:57<09:56, 18.07s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [34:57<09:56, 18.07s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/150 [35:13<09:16, 17.40s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/150 [35:35<09:42, 18.78s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [35:48<08:25, 16.84s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [35:48<08:25, 16.84s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/150 [36:13<09:21, 19.38s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122/150 [36:26<08:07, 17.43s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [36:50<08:44, 19.44s/it]                                                  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [36:50<08:44, 19.44s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 124/150 [37:04<07:43, 17.81s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 125/150 [37:26<07:57, 19.08s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [37:40<07:04, 17.69s/it]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [37:40<07:04, 17.69s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/150 [38:02<07:13, 18.86s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 128/150 [38:15<06:15, 17.08s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [38:32<06:01, 17.19s/it]                                                  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [38:32<06:01, 17.19s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 130/150 [38:46<05:24, 16.24s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/150 [39:12<06:00, 18.95s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [39:24<05:06, 17.03s/it]                                                  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [39:24<05:06, 17.03s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/150 [39:46<05:14, 18.51s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 134/150 [39:59<04:29, 16.82s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [40:23<04:45, 19.01s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [40:23<04:45, 19.01s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/150 [40:39<04:12, 18.06s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137/150 [41:01<04:08, 19.14s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [41:17<03:38, 18.20s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [41:17<03:38, 18.20s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 139/150 [41:38<03:30, 19.13s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 140/150 [41:52<02:54, 17.45s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [42:17<02:58, 19.83s/it]                                                  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [42:17<02:58, 19.83s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/150 [42:34<02:31, 18.92s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 143/150 [42:56<02:18, 19.85s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [43:11<01:50, 18.44s/it]                                                  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [43:11<01:50, 18.44s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 145/150 [43:29<01:32, 18.45s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/150 [43:43<01:07, 16.99s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [44:04<00:54, 18.27s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [44:04<00:54, 18.27s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/150 [44:17<00:32, 16.50s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 149/150 [44:36<00:17, 17.46s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:50<00:00, 16.44s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:50<00:00, 16.44s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:50<00:00, 16.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:50<00:00, 17.94s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     train/grad_norm â–â–â–â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–ƒâ–„â–„â–„â–ˆâ–…â–„â–„â–„â–…â–…â–„â–„â–ƒâ–„â–‚â–‚
wandb: train/learning_rate â–‚â–„â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:          train/loss â–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               total_flos 3.140598034263245e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.30113
wandb:      train/learning_rate 0
wandb:               train/loss 0.085
wandb:               train_loss 0.72081
wandb:            train_runtime 2691.9857
wandb: train_samples_per_second 0.39
wandb:   train_steps_per_second 0.056
wandb: 
wandb: ðŸš€ View run fine_tune_json_answer_first_zero_shot_unstructured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/6dhnkje3
wandb: â­ï¸ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_190402-6dhnkje3/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.97s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  7.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.36s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.50s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.11s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.95s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  7.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.28s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.83s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
