Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.52s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 1236.00 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_190402-6dhnkje3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_answer_first_zero_shot_unstructured_seed_314_llama2
wandb: ⭐️ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: 🚀 View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/6dhnkje3
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:25<1:03:15, 25.47s/it]  1%|▏         | 2/150 [00:39<46:01, 18.66s/it]    2%|▏         | 3/150 [01:03<51:35, 21.06s/it]                                                 2%|▏         | 3/150 [01:03<51:35, 21.06s/it]  3%|▎         | 4/150 [01:16<43:18, 17.80s/it]  3%|▎         | 5/150 [01:33<42:17, 17.50s/it]  4%|▍         | 6/150 [01:45<38:00, 15.84s/it]                                                 4%|▍         | 6/150 [01:45<38:00, 15.84s/it]  5%|▍         | 7/150 [02:07<42:28, 17.82s/it]  5%|▌         | 8/150 [02:22<39:53, 16.86s/it]  6%|▌         | 9/150 [02:41<41:09, 17.51s/it]                                                 6%|▌         | 9/150 [02:41<41:09, 17.51s/it]  7%|▋         | 10/150 [02:54<37:56, 16.26s/it]  7%|▋         | 11/150 [03:19<44:00, 19.00s/it]  8%|▊         | 12/150 [03:35<41:27, 18.02s/it]                                                  8%|▊         | 12/150 [03:35<41:27, 18.02s/it]  9%|▊         | 13/150 [03:57<43:54, 19.23s/it]  9%|▉         | 14/150 [04:13<41:19, 18.23s/it] 10%|█         | 15/150 [04:29<39:38, 17.62s/it]                                                 10%|█         | 15/150 [04:29<39:38, 17.62s/it] 11%|█         | 16/150 [04:41<35:21, 15.83s/it] 11%|█▏        | 17/150 [04:59<36:42, 16.56s/it] 12%|█▏        | 18/150 [05:12<33:57, 15.43s/it]                                                 12%|█▏        | 18/150 [05:12<33:57, 15.43s/it] 13%|█▎        | 19/150 [05:34<37:40, 17.26s/it] 13%|█▎        | 20/150 [05:47<34:33, 15.95s/it] 14%|█▍        | 21/150 [06:12<40:15, 18.72s/it]                                                 14%|█▍        | 21/150 [06:12<40:15, 18.72s/it] 15%|█▍        | 22/150 [06:25<36:37, 17.17s/it] 15%|█▌        | 23/150 [06:47<39:23, 18.61s/it] 16%|█▌        | 24/150 [07:00<35:21, 16.84s/it]                                                 16%|█▌        | 24/150 [07:00<35:21, 16.84s/it] 17%|█▋        | 25/150 [07:22<38:20, 18.40s/it] 17%|█▋        | 26/150 [07:37<35:50, 17.34s/it] 18%|█▊        | 27/150 [08:01<39:42, 19.37s/it]                                                 18%|█▊        | 27/150 [08:01<39:42, 19.37s/it] 19%|█▊        | 28/150 [08:15<36:23, 17.90s/it] 19%|█▉        | 29/150 [08:34<36:20, 18.02s/it] 20%|██        | 30/150 [08:46<32:34, 16.29s/it]                                                 20%|██        | 30/150 [08:46<32:34, 16.29s/it] 21%|██        | 31/150 [09:11<37:37, 18.97s/it] 21%|██▏       | 32/150 [09:24<33:44, 17.16s/it] 22%|██▏       | 33/150 [09:41<33:20, 17.09s/it]                                                 22%|██▏       | 33/150 [09:41<33:20, 17.09s/it] 23%|██▎       | 34/150 [09:54<30:37, 15.84s/it] 23%|██▎       | 35/150 [10:16<33:42, 17.59s/it] 24%|██▍       | 36/150 [10:33<33:19, 17.54s/it]                                                 24%|██▍       | 36/150 [10:33<33:19, 17.54s/it] 25%|██▍       | 37/150 [10:57<36:45, 19.51s/it] 25%|██▌       | 38/150 [11:10<32:23, 17.35s/it] 26%|██▌       | 39/150 [11:29<33:03, 17.87s/it]                                                 26%|██▌       | 39/150 [11:29<33:03, 17.87s/it] 27%|██▋       | 40/150 [11:42<30:24, 16.58s/it] 27%|██▋       | 41/150 [12:07<34:51, 19.19s/it] 28%|██▊       | 42/150 [12:22<32:12, 17.89s/it]                                                 28%|██▊       | 42/150 [12:22<32:12, 17.89s/it] 29%|██▊       | 43/150 [12:44<34:08, 19.15s/it] 29%|██▉       | 44/150 [12:57<30:30, 17.27s/it] 30%|███       | 45/150 [13:19<32:31, 18.58s/it]                                                 30%|███       | 45/150 [13:19<32:31, 18.58s/it] 31%|███       | 46/150 [13:34<30:17, 17.47s/it] 31%|███▏      | 47/150 [13:58<33:24, 19.46s/it] 32%|███▏      | 48/150 [14:11<30:04, 17.69s/it]                                                 32%|███▏      | 48/150 [14:11<30:04, 17.69s/it] 33%|███▎      | 49/150 [14:30<30:05, 17.88s/it] 33%|███▎      | 50/150 [14:43<27:18, 16.39s/it] 34%|███▍      | 51/150 [15:08<31:25, 19.05s/it]                                                 34%|███▍      | 51/150 [15:08<31:25, 19.05s/it] 35%|███▍      | 52/150 [15:21<28:05, 17.20s/it] 35%|███▌      | 53/150 [15:43<30:09, 18.65s/it] 36%|███▌      | 54/150 [15:57<27:49, 17.39s/it]                                                 36%|███▌      | 54/150 [15:57<27:49, 17.39s/it] 37%|███▋      | 55/150 [16:21<30:43, 19.40s/it] 37%|███▋      | 56/150 [16:34<27:19, 17.44s/it] 38%|███▊      | 57/150 [16:56<28:51, 18.62s/it]                                                 38%|███▊      | 57/150 [16:56<28:51, 18.62s/it] 39%|███▊      | 58/150 [17:11<26:49, 17.50s/it] 39%|███▉      | 59/150 [17:33<28:35, 18.85s/it] 40%|████      | 60/150 [17:47<26:20, 17.56s/it]                                                 40%|████      | 60/150 [17:47<26:20, 17.56s/it] 41%|████      | 61/150 [18:12<29:30, 19.89s/it] 41%|████▏     | 62/150 [18:27<27:03, 18.45s/it] 42%|████▏     | 63/150 [18:52<29:12, 20.15s/it]                                                 42%|████▏     | 63/150 [18:52<29:12, 20.15s/it] 43%|████▎     | 64/150 [19:05<25:45, 17.97s/it] 43%|████▎     | 65/150 [19:27<27:10, 19.18s/it] 44%|████▍     | 66/150 [19:39<23:58, 17.13s/it]                                                 44%|████▍     | 66/150 [19:39<23:58, 17.13s/it] 45%|████▍     | 67/150 [19:58<24:30, 17.71s/it] 45%|████▌     | 68/150 [20:12<22:41, 16.61s/it] 46%|████▌     | 69/150 [20:34<24:37, 18.24s/it]                                                 46%|████▌     | 69/150 [20:34<24:37, 18.24s/it] 47%|████▋     | 70/150 [20:49<22:50, 17.13s/it] 47%|████▋     | 71/150 [21:14<25:46, 19.57s/it] 48%|████▊     | 72/150 [21:27<22:49, 17.56s/it]                                                 48%|████▊     | 72/150 [21:27<22:49, 17.56s/it] 49%|████▊     | 73/150 [21:45<22:53, 17.83s/it] 49%|████▉     | 74/150 [21:58<20:33, 16.24s/it] 50%|█████     | 75/150 [22:20<22:27, 17.97s/it]                                                 50%|█████     | 75/150 [22:20<22:27, 17.97s/it] 51%|█████     | 76/150 [22:32<20:04, 16.28s/it] 51%|█████▏    | 77/150 [22:56<22:41, 18.65s/it] 52%|█████▏    | 78/150 [23:11<20:54, 17.42s/it]                                                 52%|█████▏    | 78/150 [23:11<20:54, 17.42s/it] 53%|█████▎    | 79/150 [23:33<22:15, 18.81s/it] 53%|█████▎    | 80/150 [23:48<20:34, 17.64s/it] 54%|█████▍    | 81/150 [24:13<22:56, 19.95s/it]                                                 54%|█████▍    | 81/150 [24:13<22:56, 19.95s/it] 55%|█████▍    | 82/150 [24:28<20:50, 18.40s/it] 55%|█████▌    | 83/150 [24:52<22:27, 20.12s/it] 56%|█████▌    | 84/150 [25:06<19:58, 18.16s/it]                                                 56%|█████▌    | 84/150 [25:06<19:58, 18.16s/it] 57%|█████▋    | 85/150 [25:24<19:45, 18.23s/it] 57%|█████▋    | 86/150 [25:37<17:40, 16.58s/it] 58%|█████▊    | 87/150 [25:58<18:54, 18.01s/it]                                                 58%|█████▊    | 87/150 [25:58<18:54, 18.01s/it] 59%|█████▊    | 88/150 [26:13<17:32, 16.98s/it] 59%|█████▉    | 89/150 [26:35<18:47, 18.49s/it] 60%|██████    | 90/150 [26:47<16:47, 16.80s/it]                                                 60%|██████    | 90/150 [26:47<16:47, 16.80s/it] 61%|██████    | 91/150 [27:13<19:00, 19.33s/it] 61%|██████▏   | 92/150 [27:25<16:39, 17.24s/it] 62%|██████▏   | 93/150 [27:49<18:21, 19.33s/it]                                                 62%|██████▏   | 93/150 [27:49<18:21, 19.33s/it] 63%|██████▎   | 94/150 [28:06<17:22, 18.62s/it] 63%|██████▎   | 95/150 [28:28<17:48, 19.43s/it] 64%|██████▍   | 96/150 [28:40<15:40, 17.41s/it]                                                 64%|██████▍   | 96/150 [28:40<15:40, 17.41s/it] 65%|██████▍   | 97/150 [29:00<15:58, 18.09s/it] 65%|██████▌   | 98/150 [29:13<14:20, 16.55s/it] 66%|██████▌   | 99/150 [29:31<14:33, 17.12s/it]                                                 66%|██████▌   | 99/150 [29:31<14:33, 17.12s/it] 67%|██████▋   | 100/150 [29:45<13:29, 16.19s/it] 67%|██████▋   | 101/150 [30:11<15:26, 18.91s/it] 68%|██████▊   | 102/150 [30:23<13:40, 17.10s/it]                                                  68%|██████▊   | 102/150 [30:23<13:40, 17.10s/it] 69%|██████▊   | 103/150 [30:45<14:23, 18.37s/it] 69%|██████▉   | 104/150 [30:59<13:04, 17.06s/it] 70%|███████   | 105/150 [31:17<13:07, 17.49s/it]                                                  70%|███████   | 105/150 [31:17<13:07, 17.49s/it] 71%|███████   | 106/150 [31:31<11:57, 16.32s/it] 71%|███████▏  | 107/150 [31:53<12:55, 18.04s/it] 72%|███████▏  | 108/150 [32:07<11:47, 16.84s/it]                                                  72%|███████▏  | 108/150 [32:07<11:47, 16.84s/it] 73%|███████▎  | 109/150 [32:31<13:00, 19.03s/it] 73%|███████▎  | 110/150 [32:46<11:53, 17.83s/it] 74%|███████▍  | 111/150 [33:11<13:02, 20.07s/it]                                                  74%|███████▍  | 111/150 [33:11<13:02, 20.07s/it] 75%|███████▍  | 112/150 [33:26<11:39, 18.41s/it] 75%|███████▌  | 113/150 [33:48<11:56, 19.36s/it] 76%|███████▌  | 114/150 [34:00<10:27, 17.43s/it]                                                  76%|███████▌  | 114/150 [34:00<10:27, 17.43s/it] 77%|███████▋  | 115/150 [34:25<11:20, 19.45s/it] 77%|███████▋  | 116/150 [34:39<10:10, 17.95s/it] 78%|███████▊  | 117/150 [34:57<09:56, 18.07s/it]                                                  78%|███████▊  | 117/150 [34:57<09:56, 18.07s/it] 79%|███████▊  | 118/150 [35:13<09:16, 17.40s/it] 79%|███████▉  | 119/150 [35:35<09:42, 18.78s/it] 80%|████████  | 120/150 [35:48<08:25, 16.84s/it]                                                  80%|████████  | 120/150 [35:48<08:25, 16.84s/it] 81%|████████  | 121/150 [36:13<09:21, 19.38s/it] 81%|████████▏ | 122/150 [36:26<08:07, 17.43s/it] 82%|████████▏ | 123/150 [36:50<08:44, 19.44s/it]                                                  82%|████████▏ | 123/150 [36:50<08:44, 19.44s/it] 83%|████████▎ | 124/150 [37:04<07:43, 17.81s/it] 83%|████████▎ | 125/150 [37:26<07:57, 19.08s/it] 84%|████████▍ | 126/150 [37:40<07:04, 17.69s/it]                                                  84%|████████▍ | 126/150 [37:40<07:04, 17.69s/it] 85%|████████▍ | 127/150 [38:02<07:13, 18.86s/it] 85%|████████▌ | 128/150 [38:15<06:15, 17.08s/it] 86%|████████▌ | 129/150 [38:32<06:01, 17.19s/it]                                                  86%|████████▌ | 129/150 [38:32<06:01, 17.19s/it] 87%|████████▋ | 130/150 [38:46<05:24, 16.24s/it] 87%|████████▋ | 131/150 [39:12<06:00, 18.95s/it] 88%|████████▊ | 132/150 [39:24<05:06, 17.03s/it]                                                  88%|████████▊ | 132/150 [39:24<05:06, 17.03s/it] 89%|████████▊ | 133/150 [39:46<05:14, 18.51s/it] 89%|████████▉ | 134/150 [39:59<04:29, 16.82s/it] 90%|█████████ | 135/150 [40:23<04:45, 19.01s/it]                                                  90%|█████████ | 135/150 [40:23<04:45, 19.01s/it] 91%|█████████ | 136/150 [40:39<04:12, 18.06s/it] 91%|█████████▏| 137/150 [41:01<04:08, 19.14s/it] 92%|█████████▏| 138/150 [41:17<03:38, 18.20s/it]                                                  92%|█████████▏| 138/150 [41:17<03:38, 18.20s/it] 93%|█████████▎| 139/150 [41:38<03:30, 19.13s/it] 93%|█████████▎| 140/150 [41:52<02:54, 17.45s/it] 94%|█████████▍| 141/150 [42:17<02:58, 19.83s/it]                                                  94%|█████████▍| 141/150 [42:17<02:58, 19.83s/it] 95%|█████████▍| 142/150 [42:34<02:31, 18.92s/it] 95%|█████████▌| 143/150 [42:56<02:18, 19.85s/it] 96%|█████████▌| 144/150 [43:11<01:50, 18.44s/it]                                                  96%|█████████▌| 144/150 [43:11<01:50, 18.44s/it] 97%|█████████▋| 145/150 [43:29<01:32, 18.45s/it] 97%|█████████▋| 146/150 [43:43<01:07, 16.99s/it] 98%|█████████▊| 147/150 [44:04<00:54, 18.27s/it]                                                  98%|█████████▊| 147/150 [44:04<00:54, 18.27s/it] 99%|█████████▊| 148/150 [44:17<00:32, 16.50s/it] 99%|█████████▉| 149/150 [44:36<00:17, 17.46s/it]100%|██████████| 150/150 [44:50<00:00, 16.44s/it]                                                 100%|██████████| 150/150 [44:50<00:00, 16.44s/it]                                                 100%|██████████| 150/150 [44:50<00:00, 16.44s/it]100%|██████████| 150/150 [44:50<00:00, 17.94s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:     train/grad_norm ▁▁▁▂▂▁▂▂▁▁▁▁▂▁▂▂▂▂▂▂▂▂▄▃▄▄▄█▅▄▄▄▅▅▄▄▃▄▂▂
wandb: train/learning_rate ▂▄▇███▇▇▇▆▅▅▄▄▃▂▂▂▁▁▁████▇▇▆▆▅▄▄▃▃▃▂▂▁▁▁
wandb:          train/loss █▇▇▆▆▅▅▅▅▅▅▄▄▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▂▃▂▂▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 3.140598034263245e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.30113
wandb:      train/learning_rate 0
wandb:               train/loss 0.085
wandb:               train_loss 0.72081
wandb:            train_runtime 2691.9857
wandb: train_samples_per_second 0.39
wandb:   train_steps_per_second 0.056
wandb: 
wandb: 🚀 View run fine_tune_json_answer_first_zero_shot_unstructured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/6dhnkje3
wandb: ⭐️ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_190402-6dhnkje3/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.97s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.36s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.50s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.11s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.95s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.28s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.83s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
