Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.11s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<00:00, 524.29 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<00:00, 512.13 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_101725-gexb3qnz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_answer_first_few_shot_structured_seed_314_llama2
wandb: â­ï¸ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: ðŸš€ View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/gexb3qnz
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:24<1:00:31, 24.37s/it]  1%|â–         | 2/150 [00:48<59:31, 24.13s/it]    2%|â–         | 3/150 [01:12<59:03, 24.10s/it]                                                 2%|â–         | 3/150 [01:12<59:03, 24.10s/it]  3%|â–Ž         | 4/150 [01:36<58:36, 24.08s/it]  3%|â–Ž         | 5/150 [02:00<58:12, 24.09s/it]  4%|â–         | 6/150 [02:24<57:52, 24.11s/it]                                                 4%|â–         | 6/150 [02:24<57:52, 24.11s/it]  5%|â–         | 7/150 [02:48<57:32, 24.14s/it]  5%|â–Œ         | 8/150 [03:13<57:11, 24.16s/it]  6%|â–Œ         | 9/150 [03:37<56:51, 24.19s/it]                                                 6%|â–Œ         | 9/150 [03:37<56:51, 24.19s/it]  7%|â–‹         | 10/150 [04:01<56:30, 24.22s/it]  7%|â–‹         | 11/150 [04:25<56:09, 24.24s/it]  8%|â–Š         | 12/150 [04:50<55:48, 24.26s/it]                                                  8%|â–Š         | 12/150 [04:50<55:48, 24.26s/it]  9%|â–Š         | 13/150 [05:14<55:28, 24.29s/it]  9%|â–‰         | 14/150 [05:38<55:06, 24.31s/it] 10%|â–ˆ         | 15/150 [06:03<54:43, 24.32s/it]                                                 10%|â–ˆ         | 15/150 [06:03<54:43, 24.32s/it] 11%|â–ˆ         | 16/150 [06:27<54:19, 24.33s/it] 11%|â–ˆâ–        | 17/150 [06:52<53:56, 24.33s/it] 12%|â–ˆâ–        | 18/150 [07:16<53:32, 24.33s/it]                                                 12%|â–ˆâ–        | 18/150 [07:16<53:32, 24.33s/it] 13%|â–ˆâ–Ž        | 19/150 [07:40<53:07, 24.33s/it] 13%|â–ˆâ–Ž        | 20/150 [08:04<52:42, 24.33s/it] 14%|â–ˆâ–        | 21/150 [08:29<52:16, 24.31s/it]                                                 14%|â–ˆâ–        | 21/150 [08:29<52:16, 24.31s/it] 15%|â–ˆâ–        | 22/150 [08:53<51:50, 24.30s/it] 15%|â–ˆâ–Œ        | 23/150 [09:17<51:25, 24.30s/it] 16%|â–ˆâ–Œ        | 24/150 [09:42<51:02, 24.30s/it]                                                 16%|â–ˆâ–Œ        | 24/150 [09:42<51:02, 24.30s/it] 17%|â–ˆâ–‹        | 25/150 [10:06<50:41, 24.33s/it] 17%|â–ˆâ–‹        | 26/150 [10:30<50:20, 24.36s/it] 18%|â–ˆâ–Š        | 27/150 [10:55<49:59, 24.39s/it]                                                 18%|â–ˆâ–Š        | 27/150 [10:55<49:59, 24.39s/it] 19%|â–ˆâ–Š        | 28/150 [11:19<49:37, 24.41s/it] 19%|â–ˆâ–‰        | 29/150 [11:44<49:15, 24.43s/it] 20%|â–ˆâ–ˆ        | 30/150 [12:08<48:54, 24.45s/it]                                                 20%|â–ˆâ–ˆ        | 30/150 [12:08<48:54, 24.45s/it] 21%|â–ˆâ–ˆ        | 31/150 [12:33<48:32, 24.48s/it] 21%|â–ˆâ–ˆâ–       | 32/150 [12:57<48:09, 24.49s/it] 22%|â–ˆâ–ˆâ–       | 33/150 [13:22<47:46, 24.50s/it]                                                 22%|â–ˆâ–ˆâ–       | 33/150 [13:22<47:46, 24.50s/it] 23%|â–ˆâ–ˆâ–Ž       | 34/150 [13:46<47:22, 24.50s/it] 23%|â–ˆâ–ˆâ–Ž       | 35/150 [14:11<46:58, 24.51s/it] 24%|â–ˆâ–ˆâ–       | 36/150 [14:35<46:33, 24.51s/it]                                                 24%|â–ˆâ–ˆâ–       | 36/150 [14:35<46:33, 24.51s/it] 25%|â–ˆâ–ˆâ–       | 37/150 [15:00<46:08, 24.50s/it] 25%|â–ˆâ–ˆâ–Œ       | 38/150 [15:24<45:43, 24.50s/it] 26%|â–ˆâ–ˆâ–Œ       | 39/150 [15:49<45:17, 24.48s/it]                                                 26%|â–ˆâ–ˆâ–Œ       | 39/150 [15:49<45:17, 24.48s/it] 27%|â–ˆâ–ˆâ–‹       | 40/150 [16:13<44:52, 24.48s/it] 27%|â–ˆâ–ˆâ–‹       | 41/150 [16:38<44:28, 24.48s/it] 28%|â–ˆâ–ˆâ–Š       | 42/150 [17:02<44:02, 24.47s/it]                                                 28%|â–ˆâ–ˆâ–Š       | 42/150 [17:02<44:02, 24.47s/it] 29%|â–ˆâ–ˆâ–Š       | 43/150 [17:27<43:37, 24.46s/it] 29%|â–ˆâ–ˆâ–‰       | 44/150 [17:51<43:12, 24.46s/it] 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [18:16<42:47, 24.45s/it]                                                 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [18:16<42:47, 24.45s/it] 31%|â–ˆâ–ˆâ–ˆ       | 46/150 [18:40<42:23, 24.45s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 47/150 [19:05<41:58, 24.45s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [19:29<41:33, 24.45s/it]                                                 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [19:29<41:33, 24.45s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 49/150 [19:53<41:09, 24.45s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 50/150 [20:18<40:44, 24.45s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [20:42<40:20, 24.45s/it]                                                 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [20:42<40:20, 24.45s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 52/150 [21:07<39:56, 24.45s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/150 [21:31<39:31, 24.45s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [21:56<39:07, 24.45s/it]                                                 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [21:56<39:07, 24.45s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 55/150 [22:20<38:42, 24.45s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/150 [22:45<38:18, 24.45s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [23:09<37:53, 24.45s/it]                                                 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [23:09<37:53, 24.45s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 58/150 [23:33<37:29, 24.45s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/150 [23:58<37:05, 24.45s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [24:22<36:39, 24.44s/it]                                                 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [24:22<36:39, 24.44s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/150 [24:47<36:14, 24.44s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 62/150 [25:11<35:50, 24.43s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [25:36<35:26, 24.44s/it]                                                 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [25:36<35:26, 24.44s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 64/150 [26:00<35:02, 24.44s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 65/150 [26:25<34:38, 24.45s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [26:49<34:14, 24.46s/it]                                                 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [26:49<34:14, 24.46s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/150 [27:14<33:50, 24.47s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/150 [27:38<33:27, 24.48s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [28:03<33:03, 24.48s/it]                                                 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [28:03<33:03, 24.48s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/150 [28:27<32:39, 24.49s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/150 [28:52<32:15, 24.50s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [29:16<31:50, 24.49s/it]                                                 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [29:16<31:50, 24.49s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/150 [29:41<31:26, 24.49s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/150 [30:05<31:00, 24.49s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [30:29<30:35, 24.48s/it]                                                 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [30:29<30:35, 24.48s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/150 [30:54<30:10, 24.47s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/150 [31:18<29:46, 24.47s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [31:43<29:21, 24.47s/it]                                                 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [31:43<29:21, 24.47s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 79/150 [32:07<28:57, 24.48s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 80/150 [32:32<28:33, 24.47s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [32:56<28:09, 24.48s/it]                                                 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [32:56<28:09, 24.48s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/150 [33:21<27:45, 24.49s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 83/150 [33:45<27:20, 24.48s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [34:10<26:55, 24.48s/it]                                                 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [34:10<26:55, 24.48s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/150 [34:34<26:31, 24.48s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/150 [34:59<26:06, 24.48s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [35:23<25:42, 24.48s/it]                                                 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [35:23<25:42, 24.48s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/150 [35:48<25:17, 24.48s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/150 [36:12<24:53, 24.48s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [36:37<24:27, 24.46s/it]                                                 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [36:37<24:27, 24.46s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/150 [37:01<24:01, 24.44s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/150 [37:25<23:36, 24.42s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [37:50<23:10, 24.40s/it]                                                 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [37:50<23:10, 24.40s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 94/150 [38:14<22:45, 24.38s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 95/150 [38:38<22:20, 24.37s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [39:03<21:55, 24.36s/it]                                                 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [39:03<21:55, 24.36s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/150 [39:27<21:30, 24.35s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 98/150 [39:51<21:06, 24.35s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [40:16<20:42, 24.36s/it]                                                 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [40:16<20:42, 24.36s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 100/150 [40:40<20:18, 24.37s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/150 [41:05<19:55, 24.39s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [41:29<19:31, 24.41s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [41:29<19:31, 24.41s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/150 [41:54<19:08, 24.44s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/150 [42:18<18:44, 24.45s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [42:43<18:20, 24.46s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [42:43<18:20, 24.46s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/150 [43:07<17:57, 24.48s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107/150 [43:32<17:33, 24.49s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [43:56<17:09, 24.50s/it]                                                  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [43:56<17:09, 24.50s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 109/150 [44:21<16:44, 24.50s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 110/150 [44:45<16:20, 24.50s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [45:10<15:55, 24.51s/it]                                                  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [45:10<15:55, 24.51s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/150 [45:34<15:30, 24.50s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 113/150 [45:59<15:05, 24.48s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [46:23<14:41, 24.48s/it]                                                  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [46:23<14:41, 24.48s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 115/150 [46:47<14:16, 24.47s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/150 [47:12<13:51, 24.47s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [47:36<13:27, 24.46s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [47:36<13:27, 24.46s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/150 [48:01<13:02, 24.46s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/150 [48:25<12:38, 24.46s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [48:50<12:13, 24.45s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [48:50<12:13, 24.45s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/150 [49:14<11:49, 24.45s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122/150 [49:39<11:24, 24.45s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [50:03<11:00, 24.45s/it]                                                  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [50:03<11:00, 24.45s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 124/150 [50:28<10:35, 24.45s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 125/150 [50:52<10:11, 24.45s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [51:16<09:46, 24.45s/it]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [51:16<09:46, 24.45s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/150 [51:41<09:22, 24.45s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 128/150 [52:05<08:58, 24.46s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [52:30<08:33, 24.46s/it]                                                  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [52:30<08:33, 24.46s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 130/150 [52:54<08:09, 24.45s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/150 [53:19<07:44, 24.46s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [53:43<07:20, 24.45s/it]                                                  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [53:43<07:20, 24.45s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/150 [54:08<06:55, 24.45s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 134/150 [54:32<06:30, 24.43s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [54:56<06:06, 24.42s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [54:56<06:06, 24.42s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/150 [55:21<05:41, 24.41s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137/150 [55:45<05:17, 24.42s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [56:10<04:53, 24.42s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [56:10<04:53, 24.42s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 139/150 [56:34<04:28, 24.43s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 140/150 [56:59<04:04, 24.44s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [57:23<03:40, 24.45s/it]                                                  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [57:23<03:40, 24.45s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/150 [57:47<03:15, 24.46s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 143/150 [58:12<02:51, 24.47s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [58:36<02:26, 24.47s/it]                                                  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [58:36<02:26, 24.47s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 145/150 [59:01<02:02, 24.48s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/150 [59:25<01:37, 24.48s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [59:50<01:13, 24.49s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [59:50<01:13, 24.49s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/150 [1:00:14<00:48, 24.49s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 149/150 [1:00:39<00:24, 24.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:01:03<00:00, 24.49s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:01:03<00:00, 24.49s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:01:03<00:00, 24.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [1:01:03<00:00, 24.43s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     train/grad_norm â–ƒâ–„â–„â–…â–†â–†â–ˆâ–ƒâ–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–…â–†â–‡â–†â–…â–ˆâ–ˆâ–†â–…â–‡â–…â–„â–„
wandb: train/learning_rate â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:          train/loss â–ˆâ–ˆâ–ˆâ–‡â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               total_flos 4.36569190170624e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.15953
wandb:      train/learning_rate 0
wandb:               train/loss 0.042
wandb:               train_loss 0.36432
wandb:            train_runtime 3665.8056
wandb: train_samples_per_second 0.286
wandb:   train_steps_per_second 0.041
wandb: 
wandb: ðŸš€ View run fine_tune_json_answer_first_few_shot_structured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/gexb3qnz
wandb: â­ï¸ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_101725-gexb3qnz/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.96s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.76s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.16s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.37s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  9.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  9.85s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.86s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.01s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.73s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.72s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.34s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  5.58s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.14s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.28s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
