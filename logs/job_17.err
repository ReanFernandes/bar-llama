Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.11s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 524.29 examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 512.13 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_101725-gexb3qnz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_answer_first_few_shot_structured_seed_314_llama2
wandb: ⭐️ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: 🚀 View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/gexb3qnz
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:24<1:00:31, 24.37s/it]  1%|▏         | 2/150 [00:48<59:31, 24.13s/it]    2%|▏         | 3/150 [01:12<59:03, 24.10s/it]                                                 2%|▏         | 3/150 [01:12<59:03, 24.10s/it]  3%|▎         | 4/150 [01:36<58:36, 24.08s/it]  3%|▎         | 5/150 [02:00<58:12, 24.09s/it]  4%|▍         | 6/150 [02:24<57:52, 24.11s/it]                                                 4%|▍         | 6/150 [02:24<57:52, 24.11s/it]  5%|▍         | 7/150 [02:48<57:32, 24.14s/it]  5%|▌         | 8/150 [03:13<57:11, 24.16s/it]  6%|▌         | 9/150 [03:37<56:51, 24.19s/it]                                                 6%|▌         | 9/150 [03:37<56:51, 24.19s/it]  7%|▋         | 10/150 [04:01<56:30, 24.22s/it]  7%|▋         | 11/150 [04:25<56:09, 24.24s/it]  8%|▊         | 12/150 [04:50<55:48, 24.26s/it]                                                  8%|▊         | 12/150 [04:50<55:48, 24.26s/it]  9%|▊         | 13/150 [05:14<55:28, 24.29s/it]  9%|▉         | 14/150 [05:38<55:06, 24.31s/it] 10%|█         | 15/150 [06:03<54:43, 24.32s/it]                                                 10%|█         | 15/150 [06:03<54:43, 24.32s/it] 11%|█         | 16/150 [06:27<54:19, 24.33s/it] 11%|█▏        | 17/150 [06:52<53:56, 24.33s/it] 12%|█▏        | 18/150 [07:16<53:32, 24.33s/it]                                                 12%|█▏        | 18/150 [07:16<53:32, 24.33s/it] 13%|█▎        | 19/150 [07:40<53:07, 24.33s/it] 13%|█▎        | 20/150 [08:04<52:42, 24.33s/it] 14%|█▍        | 21/150 [08:29<52:16, 24.31s/it]                                                 14%|█▍        | 21/150 [08:29<52:16, 24.31s/it] 15%|█▍        | 22/150 [08:53<51:50, 24.30s/it] 15%|█▌        | 23/150 [09:17<51:25, 24.30s/it] 16%|█▌        | 24/150 [09:42<51:02, 24.30s/it]                                                 16%|█▌        | 24/150 [09:42<51:02, 24.30s/it] 17%|█▋        | 25/150 [10:06<50:41, 24.33s/it] 17%|█▋        | 26/150 [10:30<50:20, 24.36s/it] 18%|█▊        | 27/150 [10:55<49:59, 24.39s/it]                                                 18%|█▊        | 27/150 [10:55<49:59, 24.39s/it] 19%|█▊        | 28/150 [11:19<49:37, 24.41s/it] 19%|█▉        | 29/150 [11:44<49:15, 24.43s/it] 20%|██        | 30/150 [12:08<48:54, 24.45s/it]                                                 20%|██        | 30/150 [12:08<48:54, 24.45s/it] 21%|██        | 31/150 [12:33<48:32, 24.48s/it] 21%|██▏       | 32/150 [12:57<48:09, 24.49s/it] 22%|██▏       | 33/150 [13:22<47:46, 24.50s/it]                                                 22%|██▏       | 33/150 [13:22<47:46, 24.50s/it] 23%|██▎       | 34/150 [13:46<47:22, 24.50s/it] 23%|██▎       | 35/150 [14:11<46:58, 24.51s/it] 24%|██▍       | 36/150 [14:35<46:33, 24.51s/it]                                                 24%|██▍       | 36/150 [14:35<46:33, 24.51s/it] 25%|██▍       | 37/150 [15:00<46:08, 24.50s/it] 25%|██▌       | 38/150 [15:24<45:43, 24.50s/it] 26%|██▌       | 39/150 [15:49<45:17, 24.48s/it]                                                 26%|██▌       | 39/150 [15:49<45:17, 24.48s/it] 27%|██▋       | 40/150 [16:13<44:52, 24.48s/it] 27%|██▋       | 41/150 [16:38<44:28, 24.48s/it] 28%|██▊       | 42/150 [17:02<44:02, 24.47s/it]                                                 28%|██▊       | 42/150 [17:02<44:02, 24.47s/it] 29%|██▊       | 43/150 [17:27<43:37, 24.46s/it] 29%|██▉       | 44/150 [17:51<43:12, 24.46s/it] 30%|███       | 45/150 [18:16<42:47, 24.45s/it]                                                 30%|███       | 45/150 [18:16<42:47, 24.45s/it] 31%|███       | 46/150 [18:40<42:23, 24.45s/it] 31%|███▏      | 47/150 [19:05<41:58, 24.45s/it] 32%|███▏      | 48/150 [19:29<41:33, 24.45s/it]                                                 32%|███▏      | 48/150 [19:29<41:33, 24.45s/it] 33%|███▎      | 49/150 [19:53<41:09, 24.45s/it] 33%|███▎      | 50/150 [20:18<40:44, 24.45s/it] 34%|███▍      | 51/150 [20:42<40:20, 24.45s/it]                                                 34%|███▍      | 51/150 [20:42<40:20, 24.45s/it] 35%|███▍      | 52/150 [21:07<39:56, 24.45s/it] 35%|███▌      | 53/150 [21:31<39:31, 24.45s/it] 36%|███▌      | 54/150 [21:56<39:07, 24.45s/it]                                                 36%|███▌      | 54/150 [21:56<39:07, 24.45s/it] 37%|███▋      | 55/150 [22:20<38:42, 24.45s/it] 37%|███▋      | 56/150 [22:45<38:18, 24.45s/it] 38%|███▊      | 57/150 [23:09<37:53, 24.45s/it]                                                 38%|███▊      | 57/150 [23:09<37:53, 24.45s/it] 39%|███▊      | 58/150 [23:33<37:29, 24.45s/it] 39%|███▉      | 59/150 [23:58<37:05, 24.45s/it] 40%|████      | 60/150 [24:22<36:39, 24.44s/it]                                                 40%|████      | 60/150 [24:22<36:39, 24.44s/it] 41%|████      | 61/150 [24:47<36:14, 24.44s/it] 41%|████▏     | 62/150 [25:11<35:50, 24.43s/it] 42%|████▏     | 63/150 [25:36<35:26, 24.44s/it]                                                 42%|████▏     | 63/150 [25:36<35:26, 24.44s/it] 43%|████▎     | 64/150 [26:00<35:02, 24.44s/it] 43%|████▎     | 65/150 [26:25<34:38, 24.45s/it] 44%|████▍     | 66/150 [26:49<34:14, 24.46s/it]                                                 44%|████▍     | 66/150 [26:49<34:14, 24.46s/it] 45%|████▍     | 67/150 [27:14<33:50, 24.47s/it] 45%|████▌     | 68/150 [27:38<33:27, 24.48s/it] 46%|████▌     | 69/150 [28:03<33:03, 24.48s/it]                                                 46%|████▌     | 69/150 [28:03<33:03, 24.48s/it] 47%|████▋     | 70/150 [28:27<32:39, 24.49s/it] 47%|████▋     | 71/150 [28:52<32:15, 24.50s/it] 48%|████▊     | 72/150 [29:16<31:50, 24.49s/it]                                                 48%|████▊     | 72/150 [29:16<31:50, 24.49s/it] 49%|████▊     | 73/150 [29:41<31:26, 24.49s/it] 49%|████▉     | 74/150 [30:05<31:00, 24.49s/it] 50%|█████     | 75/150 [30:29<30:35, 24.48s/it]                                                 50%|█████     | 75/150 [30:29<30:35, 24.48s/it] 51%|█████     | 76/150 [30:54<30:10, 24.47s/it] 51%|█████▏    | 77/150 [31:18<29:46, 24.47s/it] 52%|█████▏    | 78/150 [31:43<29:21, 24.47s/it]                                                 52%|█████▏    | 78/150 [31:43<29:21, 24.47s/it] 53%|█████▎    | 79/150 [32:07<28:57, 24.48s/it] 53%|█████▎    | 80/150 [32:32<28:33, 24.47s/it] 54%|█████▍    | 81/150 [32:56<28:09, 24.48s/it]                                                 54%|█████▍    | 81/150 [32:56<28:09, 24.48s/it] 55%|█████▍    | 82/150 [33:21<27:45, 24.49s/it] 55%|█████▌    | 83/150 [33:45<27:20, 24.48s/it] 56%|█████▌    | 84/150 [34:10<26:55, 24.48s/it]                                                 56%|█████▌    | 84/150 [34:10<26:55, 24.48s/it] 57%|█████▋    | 85/150 [34:34<26:31, 24.48s/it] 57%|█████▋    | 86/150 [34:59<26:06, 24.48s/it] 58%|█████▊    | 87/150 [35:23<25:42, 24.48s/it]                                                 58%|█████▊    | 87/150 [35:23<25:42, 24.48s/it] 59%|█████▊    | 88/150 [35:48<25:17, 24.48s/it] 59%|█████▉    | 89/150 [36:12<24:53, 24.48s/it] 60%|██████    | 90/150 [36:37<24:27, 24.46s/it]                                                 60%|██████    | 90/150 [36:37<24:27, 24.46s/it] 61%|██████    | 91/150 [37:01<24:01, 24.44s/it] 61%|██████▏   | 92/150 [37:25<23:36, 24.42s/it] 62%|██████▏   | 93/150 [37:50<23:10, 24.40s/it]                                                 62%|██████▏   | 93/150 [37:50<23:10, 24.40s/it] 63%|██████▎   | 94/150 [38:14<22:45, 24.38s/it] 63%|██████▎   | 95/150 [38:38<22:20, 24.37s/it] 64%|██████▍   | 96/150 [39:03<21:55, 24.36s/it]                                                 64%|██████▍   | 96/150 [39:03<21:55, 24.36s/it] 65%|██████▍   | 97/150 [39:27<21:30, 24.35s/it] 65%|██████▌   | 98/150 [39:51<21:06, 24.35s/it] 66%|██████▌   | 99/150 [40:16<20:42, 24.36s/it]                                                 66%|██████▌   | 99/150 [40:16<20:42, 24.36s/it] 67%|██████▋   | 100/150 [40:40<20:18, 24.37s/it] 67%|██████▋   | 101/150 [41:05<19:55, 24.39s/it] 68%|██████▊   | 102/150 [41:29<19:31, 24.41s/it]                                                  68%|██████▊   | 102/150 [41:29<19:31, 24.41s/it] 69%|██████▊   | 103/150 [41:54<19:08, 24.44s/it] 69%|██████▉   | 104/150 [42:18<18:44, 24.45s/it] 70%|███████   | 105/150 [42:43<18:20, 24.46s/it]                                                  70%|███████   | 105/150 [42:43<18:20, 24.46s/it] 71%|███████   | 106/150 [43:07<17:57, 24.48s/it] 71%|███████▏  | 107/150 [43:32<17:33, 24.49s/it] 72%|███████▏  | 108/150 [43:56<17:09, 24.50s/it]                                                  72%|███████▏  | 108/150 [43:56<17:09, 24.50s/it] 73%|███████▎  | 109/150 [44:21<16:44, 24.50s/it] 73%|███████▎  | 110/150 [44:45<16:20, 24.50s/it] 74%|███████▍  | 111/150 [45:10<15:55, 24.51s/it]                                                  74%|███████▍  | 111/150 [45:10<15:55, 24.51s/it] 75%|███████▍  | 112/150 [45:34<15:30, 24.50s/it] 75%|███████▌  | 113/150 [45:59<15:05, 24.48s/it] 76%|███████▌  | 114/150 [46:23<14:41, 24.48s/it]                                                  76%|███████▌  | 114/150 [46:23<14:41, 24.48s/it] 77%|███████▋  | 115/150 [46:47<14:16, 24.47s/it] 77%|███████▋  | 116/150 [47:12<13:51, 24.47s/it] 78%|███████▊  | 117/150 [47:36<13:27, 24.46s/it]                                                  78%|███████▊  | 117/150 [47:36<13:27, 24.46s/it] 79%|███████▊  | 118/150 [48:01<13:02, 24.46s/it] 79%|███████▉  | 119/150 [48:25<12:38, 24.46s/it] 80%|████████  | 120/150 [48:50<12:13, 24.45s/it]                                                  80%|████████  | 120/150 [48:50<12:13, 24.45s/it] 81%|████████  | 121/150 [49:14<11:49, 24.45s/it] 81%|████████▏ | 122/150 [49:39<11:24, 24.45s/it] 82%|████████▏ | 123/150 [50:03<11:00, 24.45s/it]                                                  82%|████████▏ | 123/150 [50:03<11:00, 24.45s/it] 83%|████████▎ | 124/150 [50:28<10:35, 24.45s/it] 83%|████████▎ | 125/150 [50:52<10:11, 24.45s/it] 84%|████████▍ | 126/150 [51:16<09:46, 24.45s/it]                                                  84%|████████▍ | 126/150 [51:16<09:46, 24.45s/it] 85%|████████▍ | 127/150 [51:41<09:22, 24.45s/it] 85%|████████▌ | 128/150 [52:05<08:58, 24.46s/it] 86%|████████▌ | 129/150 [52:30<08:33, 24.46s/it]                                                  86%|████████▌ | 129/150 [52:30<08:33, 24.46s/it] 87%|████████▋ | 130/150 [52:54<08:09, 24.45s/it] 87%|████████▋ | 131/150 [53:19<07:44, 24.46s/it] 88%|████████▊ | 132/150 [53:43<07:20, 24.45s/it]                                                  88%|████████▊ | 132/150 [53:43<07:20, 24.45s/it] 89%|████████▊ | 133/150 [54:08<06:55, 24.45s/it] 89%|████████▉ | 134/150 [54:32<06:30, 24.43s/it] 90%|█████████ | 135/150 [54:56<06:06, 24.42s/it]                                                  90%|█████████ | 135/150 [54:56<06:06, 24.42s/it] 91%|█████████ | 136/150 [55:21<05:41, 24.41s/it] 91%|█████████▏| 137/150 [55:45<05:17, 24.42s/it] 92%|█████████▏| 138/150 [56:10<04:53, 24.42s/it]                                                  92%|█████████▏| 138/150 [56:10<04:53, 24.42s/it] 93%|█████████▎| 139/150 [56:34<04:28, 24.43s/it] 93%|█████████▎| 140/150 [56:59<04:04, 24.44s/it] 94%|█████████▍| 141/150 [57:23<03:40, 24.45s/it]                                                  94%|█████████▍| 141/150 [57:23<03:40, 24.45s/it] 95%|█████████▍| 142/150 [57:47<03:15, 24.46s/it] 95%|█████████▌| 143/150 [58:12<02:51, 24.47s/it] 96%|█████████▌| 144/150 [58:36<02:26, 24.47s/it]                                                  96%|█████████▌| 144/150 [58:36<02:26, 24.47s/it] 97%|█████████▋| 145/150 [59:01<02:02, 24.48s/it] 97%|█████████▋| 146/150 [59:25<01:37, 24.48s/it] 98%|█████████▊| 147/150 [59:50<01:13, 24.49s/it]                                                  98%|█████████▊| 147/150 [59:50<01:13, 24.49s/it] 99%|█████████▊| 148/150 [1:00:14<00:48, 24.49s/it] 99%|█████████▉| 149/150 [1:00:39<00:24, 24.49s/it]100%|██████████| 150/150 [1:01:03<00:00, 24.49s/it]                                                   100%|██████████| 150/150 [1:01:03<00:00, 24.49s/it]                                                   100%|██████████| 150/150 [1:01:03<00:00, 24.49s/it]100%|██████████| 150/150 [1:01:03<00:00, 24.43s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:     train/grad_norm ▃▄▄▅▆▆█▃▂▁▂▁▁▂▁▁▁▁▁▁▂▂▂▂▂▃▄▅▆▇▆▅██▆▅▇▅▄▄
wandb: train/learning_rate ▂▄▅▇███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁███▇▇▆▆▅▄▄▃▃▂▂▁▁▁
wandb:          train/loss ███▇▆▄▃▃▃▃▃▃▃▂▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 4.36569190170624e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.15953
wandb:      train/learning_rate 0
wandb:               train/loss 0.042
wandb:               train_loss 0.36432
wandb:            train_runtime 3665.8056
wandb: train_samples_per_second 0.286
wandb:   train_steps_per_second 0.041
wandb: 
wandb: 🚀 View run fine_tune_json_answer_first_few_shot_structured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/gexb3qnz
wandb: ⭐️ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_101725-gexb3qnz/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.96s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.76s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.16s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.37s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.85s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.86s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.01s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.73s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.72s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.34s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.28s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
