Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.59s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 664.42 examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 647.32 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_190355-io52wv1g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_fact_first_few_shot_unstructured_seed_314_llama2
wandb: ⭐️ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: 🚀 View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/io52wv1g
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:24<1:00:52, 24.52s/it]  1%|▏         | 2/150 [00:49<1:01:32, 24.95s/it]  2%|▏         | 3/150 [01:14<1:00:19, 24.62s/it]                                                   2%|▏         | 3/150 [01:14<1:00:19, 24.62s/it]  3%|▎         | 4/150 [01:39<1:00:35, 24.90s/it]  3%|▎         | 5/150 [02:03<59:38, 24.68s/it]    4%|▍         | 6/150 [02:28<59:47, 24.91s/it]                                                 4%|▍         | 6/150 [02:28<59:47, 24.91s/it]  5%|▍         | 7/150 [02:53<58:53, 24.71s/it]  5%|▌         | 8/150 [03:18<59:01, 24.94s/it]  6%|▌         | 9/150 [03:43<58:09, 24.75s/it]                                                 6%|▌         | 9/150 [03:43<58:09, 24.75s/it]  7%|▋         | 10/150 [04:08<58:12, 24.95s/it]  7%|▋         | 11/150 [04:32<57:22, 24.77s/it]  8%|▊         | 12/150 [04:58<57:26, 24.97s/it]                                                  8%|▊         | 12/150 [04:58<57:26, 24.97s/it]  9%|▊         | 13/150 [05:22<56:36, 24.79s/it]  9%|▉         | 14/150 [05:48<56:39, 25.00s/it] 10%|█         | 15/150 [06:13<56:35, 25.15s/it]                                                 10%|█         | 15/150 [06:13<56:35, 25.15s/it] 11%|█         | 16/150 [06:37<55:29, 24.85s/it] 11%|█▏        | 17/150 [07:02<54:46, 24.71s/it] 12%|█▏        | 18/150 [07:27<54:50, 24.93s/it]                                                 12%|█▏        | 18/150 [07:27<54:50, 24.93s/it] 13%|█▎        | 19/150 [07:51<54:04, 24.77s/it] 13%|█▎        | 20/150 [08:17<54:08, 24.99s/it] 14%|█▍        | 21/150 [08:41<53:22, 24.82s/it]                                                 14%|█▍        | 21/150 [08:41<53:22, 24.82s/it] 15%|█▍        | 22/150 [09:07<53:23, 25.02s/it] 15%|█▌        | 23/150 [09:31<52:35, 24.84s/it] 16%|█▌        | 24/150 [09:57<52:34, 25.04s/it]                                                 16%|█▌        | 24/150 [09:57<52:34, 25.04s/it] 17%|█▋        | 25/150 [10:21<51:47, 24.86s/it] 17%|█▋        | 26/150 [10:47<51:46, 25.06s/it] 18%|█▊        | 27/150 [11:11<50:58, 24.87s/it]                                                 18%|█▊        | 27/150 [11:11<50:58, 24.87s/it] 19%|█▊        | 28/150 [11:37<50:58, 25.07s/it] 19%|█▉        | 29/150 [12:01<50:09, 24.88s/it] 20%|██        | 30/150 [12:26<49:31, 24.76s/it]                                                 20%|██        | 30/150 [12:26<49:31, 24.76s/it] 21%|██        | 31/150 [12:50<48:55, 24.67s/it] 21%|██▏       | 32/150 [13:16<49:01, 24.93s/it] 22%|██▏       | 33/150 [13:40<48:19, 24.78s/it]                                                 22%|██▏       | 33/150 [13:40<48:19, 24.78s/it] 23%|██▎       | 34/150 [14:06<48:18, 24.99s/it] 23%|██▎       | 35/150 [14:30<47:34, 24.82s/it] 24%|██▍       | 36/150 [14:56<47:35, 25.05s/it]                                                 24%|██▍       | 36/150 [14:56<47:35, 25.05s/it] 25%|██▍       | 37/150 [15:20<46:49, 24.86s/it] 25%|██▌       | 38/150 [15:44<46:13, 24.76s/it] 26%|██▌       | 39/150 [16:09<45:37, 24.67s/it]                                                 26%|██▌       | 39/150 [16:09<45:37, 24.67s/it] 27%|██▋       | 40/150 [16:34<45:42, 24.93s/it] 27%|██▋       | 41/150 [16:59<45:00, 24.77s/it] 28%|██▊       | 42/150 [17:24<44:59, 25.00s/it]                                                 28%|██▊       | 42/150 [17:24<44:59, 25.00s/it] 29%|██▊       | 43/150 [17:49<44:17, 24.83s/it] 29%|██▉       | 44/150 [18:14<44:14, 25.04s/it] 30%|███       | 45/150 [18:39<43:30, 24.86s/it]                                                 30%|███       | 45/150 [18:39<43:30, 24.86s/it] 31%|███       | 46/150 [19:04<43:27, 25.07s/it] 31%|███▏      | 47/150 [19:29<42:42, 24.88s/it] 32%|███▏      | 48/150 [19:54<42:37, 25.07s/it]                                                 32%|███▏      | 48/150 [19:54<42:37, 25.07s/it] 33%|███▎      | 49/150 [20:19<41:52, 24.87s/it] 33%|███▎      | 50/150 [20:44<41:44, 25.05s/it] 34%|███▍      | 51/150 [21:09<41:02, 24.87s/it]                                                 34%|███▍      | 51/150 [21:09<41:02, 24.87s/it] 35%|███▍      | 52/150 [21:34<40:56, 25.06s/it] 35%|███▌      | 53/150 [21:59<40:12, 24.87s/it] 36%|███▌      | 54/150 [22:24<40:06, 25.07s/it]                                                 36%|███▌      | 54/150 [22:24<40:06, 25.07s/it] 37%|███▋      | 55/150 [22:49<39:22, 24.87s/it] 37%|███▋      | 56/150 [23:14<39:15, 25.06s/it] 38%|███▊      | 57/150 [23:38<38:33, 24.87s/it]                                                 38%|███▊      | 57/150 [23:38<38:33, 24.87s/it] 39%|███▊      | 58/150 [24:04<38:26, 25.07s/it] 39%|███▉      | 59/150 [24:28<37:44, 24.88s/it] 40%|████      | 60/150 [24:54<37:37, 25.08s/it]                                                 40%|████      | 60/150 [24:54<37:37, 25.08s/it] 41%|████      | 61/150 [25:18<36:55, 24.89s/it] 41%|████▏     | 62/150 [25:44<36:47, 25.08s/it] 42%|████▏     | 63/150 [26:08<36:05, 24.89s/it]                                                 42%|████▏     | 63/150 [26:08<36:05, 24.89s/it] 43%|████▎     | 64/150 [26:34<35:56, 25.07s/it] 43%|████▎     | 65/150 [26:58<35:14, 24.88s/it] 44%|████▍     | 66/150 [27:23<34:42, 24.79s/it]                                                 44%|████▍     | 66/150 [27:23<34:42, 24.79s/it] 45%|████▍     | 67/150 [27:47<34:08, 24.68s/it] 45%|████▌     | 68/150 [28:13<34:05, 24.94s/it] 46%|████▌     | 69/150 [28:37<33:28, 24.79s/it]                                                 46%|████▌     | 69/150 [28:37<33:28, 24.79s/it] 47%|████▋     | 70/150 [29:03<33:20, 25.01s/it] 47%|████▋     | 71/150 [29:27<32:42, 24.84s/it] 48%|████▊     | 72/150 [29:53<32:32, 25.04s/it]                                                 48%|████▊     | 72/150 [29:53<32:32, 25.04s/it] 49%|████▊     | 73/150 [30:17<31:54, 24.86s/it] 49%|████▉     | 74/150 [30:42<31:22, 24.76s/it] 50%|█████     | 75/150 [31:06<30:49, 24.66s/it]                                                 50%|█████     | 75/150 [31:06<30:49, 24.66s/it] 51%|█████     | 76/150 [31:31<30:23, 24.64s/it] 51%|█████▏    | 77/150 [31:55<29:54, 24.58s/it] 52%|█████▏    | 78/150 [32:21<29:51, 24.88s/it]                                                 52%|█████▏    | 78/150 [32:21<29:51, 24.88s/it] 53%|█████▎    | 79/150 [32:45<29:17, 24.75s/it] 53%|█████▎    | 80/150 [33:11<29:08, 24.98s/it] 54%|█████▍    | 81/150 [33:35<28:32, 24.82s/it]                                                 54%|█████▍    | 81/150 [33:35<28:32, 24.82s/it] 55%|█████▍    | 82/150 [34:01<28:22, 25.04s/it] 55%|█████▌    | 83/150 [34:25<27:45, 24.86s/it] 56%|█████▌    | 84/150 [34:51<27:34, 25.06s/it]                                                 56%|█████▌    | 84/150 [34:51<27:34, 25.06s/it] 57%|█████▋    | 85/150 [35:15<26:56, 24.87s/it] 57%|█████▋    | 86/150 [35:41<26:43, 25.06s/it] 58%|█████▊    | 87/150 [36:05<26:07, 24.87s/it]                                                 58%|█████▊    | 87/150 [36:05<26:07, 24.87s/it] 59%|█████▊    | 88/150 [36:31<25:54, 25.08s/it] 59%|█████▉    | 89/150 [36:55<25:18, 24.89s/it] 60%|██████    | 90/150 [37:21<25:04, 25.07s/it]                                                 60%|██████    | 90/150 [37:21<25:04, 25.07s/it] 61%|██████    | 91/150 [37:46<24:48, 25.23s/it] 61%|██████▏   | 92/150 [38:11<24:12, 25.04s/it] 62%|██████▏   | 93/150 [38:35<23:36, 24.85s/it]                                                 62%|██████▏   | 93/150 [38:35<23:36, 24.85s/it] 63%|██████▎   | 94/150 [39:01<23:23, 25.07s/it] 63%|██████▎   | 95/150 [39:25<22:48, 24.88s/it] 64%|██████▍   | 96/150 [39:51<22:33, 25.06s/it]                                                 64%|██████▍   | 96/150 [39:51<22:33, 25.06s/it] 65%|██████▍   | 97/150 [40:15<21:58, 24.87s/it] 65%|██████▌   | 98/150 [40:41<21:43, 25.06s/it] 66%|██████▌   | 99/150 [41:05<21:08, 24.88s/it]                                                 66%|██████▌   | 99/150 [41:05<21:08, 24.88s/it] 67%|██████▋   | 100/150 [41:31<20:53, 25.07s/it] 67%|██████▋   | 101/150 [41:55<20:19, 24.89s/it] 68%|██████▊   | 102/150 [42:21<20:03, 25.07s/it]                                                  68%|██████▊   | 102/150 [42:21<20:03, 25.07s/it] 69%|██████▊   | 103/150 [42:45<19:29, 24.88s/it] 69%|██████▉   | 104/150 [43:11<19:12, 25.06s/it] 70%|███████   | 105/150 [43:35<18:39, 24.88s/it]                                                  70%|███████   | 105/150 [43:35<18:39, 24.88s/it] 71%|███████   | 106/150 [44:00<18:22, 25.06s/it] 71%|███████▏  | 107/150 [44:25<17:49, 24.87s/it] 72%|███████▏  | 108/150 [44:50<17:32, 25.07s/it]                                                  72%|███████▏  | 108/150 [44:50<17:32, 25.07s/it] 73%|███████▎  | 109/150 [45:15<17:00, 24.88s/it] 73%|███████▎  | 110/150 [45:40<16:43, 25.08s/it] 74%|███████▍  | 111/150 [46:05<16:10, 24.90s/it]                                                  74%|███████▍  | 111/150 [46:05<16:10, 24.90s/it] 75%|███████▍  | 112/150 [46:30<15:53, 25.08s/it] 75%|███████▌  | 113/150 [46:55<15:20, 24.89s/it] 76%|███████▌  | 114/150 [47:20<15:02, 25.08s/it]                                                  76%|███████▌  | 114/150 [47:20<15:02, 25.08s/it] 77%|███████▋  | 115/150 [47:45<14:30, 24.88s/it] 77%|███████▋  | 116/150 [48:10<14:12, 25.07s/it] 78%|███████▊  | 117/150 [48:35<13:41, 24.88s/it]                                                  78%|███████▊  | 117/150 [48:35<13:41, 24.88s/it] 79%|███████▊  | 118/150 [49:00<13:22, 25.07s/it] 79%|███████▉  | 119/150 [49:25<12:51, 24.88s/it] 80%|████████  | 120/150 [49:49<12:23, 24.79s/it]                                                  80%|████████  | 120/150 [49:49<12:23, 24.79s/it] 81%|████████  | 121/150 [50:14<11:55, 24.69s/it] 81%|████████▏ | 122/150 [50:39<11:38, 24.94s/it] 82%|████████▏ | 123/150 [51:04<11:09, 24.79s/it]                                                  82%|████████▏ | 123/150 [51:04<11:09, 24.79s/it] 83%|████████▎ | 124/150 [51:29<10:50, 25.01s/it] 83%|████████▎ | 125/150 [51:54<10:20, 24.84s/it] 84%|████████▍ | 126/150 [52:19<10:01, 25.04s/it]                                                  84%|████████▍ | 126/150 [52:19<10:01, 25.04s/it] 85%|████████▍ | 127/150 [52:44<09:31, 24.85s/it] 85%|████████▌ | 128/150 [53:09<09:10, 25.04s/it] 86%|████████▌ | 129/150 [53:33<08:41, 24.85s/it]                                                  86%|████████▌ | 129/150 [53:33<08:41, 24.85s/it] 87%|████████▋ | 130/150 [53:59<08:21, 25.05s/it] 87%|████████▋ | 131/150 [54:23<07:52, 24.88s/it] 88%|████████▊ | 132/150 [54:48<07:26, 24.78s/it]                                                  88%|████████▊ | 132/150 [54:48<07:26, 24.78s/it] 89%|████████▊ | 133/150 [55:12<06:59, 24.68s/it] 89%|████████▉ | 134/150 [55:38<06:39, 24.94s/it] 90%|█████████ | 135/150 [56:02<06:11, 24.79s/it]                                                  90%|█████████ | 135/150 [56:02<06:11, 24.79s/it] 91%|█████████ | 136/150 [56:28<05:50, 25.00s/it] 91%|█████████▏| 137/150 [56:52<05:22, 24.84s/it] 92%|█████████▏| 138/150 [57:18<05:00, 25.06s/it]                                                  92%|█████████▏| 138/150 [57:18<05:00, 25.06s/it] 93%|█████████▎| 139/150 [57:42<04:33, 24.87s/it] 93%|█████████▎| 140/150 [58:08<04:10, 25.06s/it] 94%|█████████▍| 141/150 [58:32<03:43, 24.88s/it]                                                  94%|█████████▍| 141/150 [58:32<03:43, 24.88s/it] 95%|█████████▍| 142/150 [58:58<03:20, 25.08s/it] 95%|█████████▌| 143/150 [59:22<02:54, 24.88s/it] 96%|█████████▌| 144/150 [59:48<02:30, 25.07s/it]                                                  96%|█████████▌| 144/150 [59:48<02:30, 25.07s/it] 97%|█████████▋| 145/150 [1:00:12<02:04, 24.88s/it] 97%|█████████▋| 146/150 [1:00:38<01:40, 25.07s/it] 98%|█████████▊| 147/150 [1:01:03<01:15, 25.23s/it]                                                    98%|█████████▊| 147/150 [1:01:03<01:15, 25.23s/it] 99%|█████████▊| 148/150 [1:01:28<00:50, 25.04s/it] 99%|█████████▉| 149/150 [1:01:52<00:24, 24.86s/it]100%|██████████| 150/150 [1:02:18<00:00, 25.05s/it]                                                   100%|██████████| 150/150 [1:02:18<00:00, 25.05s/it]                                                   100%|██████████| 150/150 [1:02:18<00:00, 25.05s/it]100%|██████████| 150/150 [1:02:18<00:00, 24.92s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:     train/grad_norm ▂▂▂▃▃▄▃▂▂▂█▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▄▃▇▃█▅▄▅▅▄▄▄▄▃
wandb: train/learning_rate ▂▄▅▇███▇▇▇▅▅▄▄▃▂▂▂▁▁▁████▇▇▆▆▅▄▄▃▃▃▂▂▁▁▁
wandb:          train/loss ███▇▆▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 4.358501015956685e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.20127
wandb:      train/learning_rate 0
wandb:               train/loss 0.0765
wandb:               train_loss 0.47568
wandb:            train_runtime 3739.7049
wandb: train_samples_per_second 0.281
wandb:   train_steps_per_second 0.04
wandb: 
wandb: 🚀 View run fine_tune_json_fact_first_few_shot_unstructured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/io52wv1g
wandb: ⭐️ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_190355-io52wv1g/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.73s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.67s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.63s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.12s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.87s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.32s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.02s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.92s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.44s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.74s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.79s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.66s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
