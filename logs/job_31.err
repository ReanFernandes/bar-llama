Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.08s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<00:00, 1580.23 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_203221-qwj2tr02
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_fact_first_zero_shot_unstructured_seed_314_llama2
wandb: â­ï¸ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: ðŸš€ View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/qwj2tr02
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:25<1:02:22, 25.11s/it]  1%|â–         | 2/150 [00:38<45:26, 18.42s/it]    2%|â–         | 3/150 [01:02<51:05, 20.85s/it]                                                 2%|â–         | 3/150 [01:02<51:05, 20.85s/it]  3%|â–Ž         | 4/150 [01:15<42:58, 17.66s/it]  3%|â–Ž         | 5/150 [01:32<41:59, 17.38s/it]  4%|â–         | 6/150 [01:44<37:46, 15.74s/it]                                                 4%|â–         | 6/150 [01:44<37:46, 15.74s/it]  5%|â–         | 7/150 [02:06<42:13, 17.72s/it]  5%|â–Œ         | 8/150 [02:21<39:39, 16.76s/it]  6%|â–Œ         | 9/150 [02:40<40:56, 17.42s/it]                                                 6%|â–Œ         | 9/150 [02:40<40:56, 17.42s/it]  7%|â–‹         | 10/150 [02:53<37:46, 16.19s/it]  7%|â–‹         | 11/150 [03:18<43:49, 18.92s/it]  8%|â–Š         | 12/150 [03:34<41:16, 17.95s/it]                                                  8%|â–Š         | 12/150 [03:34<41:16, 17.95s/it]  9%|â–Š         | 13/150 [03:56<43:44, 19.16s/it]  9%|â–‰         | 14/150 [04:12<41:11, 18.17s/it] 10%|â–ˆ         | 15/150 [04:28<39:27, 17.54s/it]                                                 10%|â–ˆ         | 15/150 [04:28<39:27, 17.54s/it] 11%|â–ˆ         | 16/150 [04:39<35:12, 15.76s/it] 11%|â–ˆâ–        | 17/150 [04:58<36:32, 16.49s/it] 12%|â–ˆâ–        | 18/150 [05:10<33:49, 15.38s/it]                                                 12%|â–ˆâ–        | 18/150 [05:10<33:49, 15.38s/it] 13%|â–ˆâ–Ž        | 19/150 [05:32<37:31, 17.19s/it] 13%|â–ˆâ–Ž        | 20/150 [05:45<34:24, 15.88s/it] 14%|â–ˆâ–        | 21/150 [06:10<40:04, 18.64s/it]                                                 14%|â–ˆâ–        | 21/150 [06:10<40:04, 18.64s/it] 15%|â–ˆâ–        | 22/150 [06:23<36:26, 17.09s/it] 15%|â–ˆâ–Œ        | 23/150 [06:45<39:10, 18.51s/it] 16%|â–ˆâ–Œ        | 24/150 [06:58<35:08, 16.74s/it]                                                 16%|â–ˆâ–Œ        | 24/150 [06:58<35:08, 16.74s/it] 17%|â–ˆâ–‹        | 25/150 [07:19<38:03, 18.26s/it] 17%|â–ˆâ–‹        | 26/150 [07:34<35:32, 17.20s/it] 18%|â–ˆâ–Š        | 27/150 [07:58<39:21, 19.20s/it]                                                 18%|â–ˆâ–Š        | 27/150 [07:58<39:21, 19.20s/it] 19%|â–ˆâ–Š        | 28/150 [08:12<36:04, 17.74s/it] 19%|â–ˆâ–‰        | 29/150 [08:30<35:59, 17.85s/it] 20%|â–ˆâ–ˆ        | 30/150 [08:43<32:18, 16.15s/it]                                                 20%|â–ˆâ–ˆ        | 30/150 [08:43<32:18, 16.15s/it] 21%|â–ˆâ–ˆ        | 31/150 [09:08<37:17, 18.81s/it] 21%|â–ˆâ–ˆâ–       | 32/150 [09:21<33:27, 17.02s/it] 22%|â–ˆâ–ˆâ–       | 33/150 [09:37<33:08, 17.00s/it]                                                 22%|â–ˆâ–ˆâ–       | 33/150 [09:37<33:08, 17.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 34/150 [09:50<30:25, 15.73s/it] 23%|â–ˆâ–ˆâ–Ž       | 35/150 [10:12<33:26, 17.45s/it] 24%|â–ˆâ–ˆâ–       | 36/150 [10:29<33:01, 17.38s/it]                                                 24%|â–ˆâ–ˆâ–       | 36/150 [10:29<33:01, 17.38s/it] 25%|â–ˆâ–ˆâ–       | 37/150 [10:53<36:24, 19.33s/it] 25%|â–ˆâ–ˆâ–Œ       | 38/150 [11:05<32:06, 17.20s/it] 26%|â–ˆâ–ˆâ–Œ       | 39/150 [11:24<32:45, 17.71s/it]                                                 26%|â–ˆâ–ˆâ–Œ       | 39/150 [11:24<32:45, 17.71s/it] 27%|â–ˆâ–ˆâ–‹       | 40/150 [11:37<30:08, 16.44s/it] 27%|â–ˆâ–ˆâ–‹       | 41/150 [12:02<34:32, 19.01s/it] 28%|â–ˆâ–ˆâ–Š       | 42/150 [12:17<31:54, 17.73s/it]                                                 28%|â–ˆâ–ˆâ–Š       | 42/150 [12:17<31:54, 17.73s/it] 29%|â–ˆâ–ˆâ–Š       | 43/150 [12:39<33:48, 18.96s/it] 29%|â–ˆâ–ˆâ–‰       | 44/150 [12:52<30:12, 17.10s/it] 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [13:13<32:11, 18.39s/it]                                                 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [13:13<32:11, 18.39s/it] 31%|â–ˆâ–ˆâ–ˆ       | 46/150 [13:28<29:58, 17.30s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 47/150 [13:52<33:03, 19.25s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [14:05<29:46, 17.52s/it]                                                 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [14:05<29:46, 17.52s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 49/150 [14:23<29:47, 17.70s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 50/150 [14:36<27:03, 16.23s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [15:01<31:07, 18.86s/it]                                                 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [15:01<31:07, 18.86s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 52/150 [15:14<27:48, 17.03s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/150 [15:36<29:51, 18.47s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [15:50<27:33, 17.23s/it]                                                 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [15:50<27:33, 17.23s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 55/150 [16:14<30:25, 19.21s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/150 [16:27<27:04, 17.28s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [16:48<28:33, 18.43s/it]                                                 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [16:48<28:33, 18.43s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 58/150 [17:02<26:33, 17.32s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/150 [17:24<28:17, 18.65s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [17:39<26:04, 17.38s/it]                                                 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [17:39<26:04, 17.38s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/150 [18:04<29:11, 19.68s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 62/150 [18:19<26:46, 18.26s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [18:42<28:53, 19.92s/it]                                                 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [18:42<28:53, 19.92s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 64/150 [18:55<25:29, 17.78s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 65/150 [19:17<26:53, 18.98s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [19:29<23:43, 16.95s/it]                                                 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [19:29<23:43, 16.95s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/150 [19:48<24:14, 17.53s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/150 [20:02<22:27, 16.43s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [20:24<24:21, 18.05s/it]                                                 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [20:24<24:21, 18.05s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/150 [20:38<22:36, 16.96s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/150 [21:03<25:30, 19.37s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [21:16<22:36, 17.39s/it]                                                 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [21:16<22:36, 17.39s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/150 [21:35<22:52, 17.83s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/150 [21:47<20:32, 16.22s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [22:09<22:22, 17.90s/it]                                                 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [22:09<22:22, 17.90s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/150 [22:21<19:58, 16.20s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/150 [22:45<22:30, 18.50s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [23:00<20:44, 17.28s/it]                                                 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [23:00<20:44, 17.28s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 79/150 [23:21<22:02, 18.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 80/150 [23:36<20:22, 17.46s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [24:01<22:42, 19.74s/it]                                                 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [24:01<22:42, 19.74s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/150 [24:16<20:40, 18.24s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 83/150 [24:40<22:14, 19.92s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [24:53<19:46, 17.98s/it]                                                 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [24:53<19:46, 17.98s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/150 [25:12<19:45, 18.24s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/150 [25:25<17:38, 16.54s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [25:46<18:49, 17.92s/it]                                                 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [25:46<18:49, 17.92s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/150 [26:00<17:26, 16.88s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/150 [26:22<18:39, 18.35s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [26:35<16:40, 16.67s/it]                                                 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [26:35<16:40, 16.67s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/150 [27:00<18:50, 19.16s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/150 [27:12<16:30, 17.08s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [27:36<18:11, 19.14s/it]                                                 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [27:36<18:11, 19.14s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 94/150 [27:53<17:15, 18.48s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 95/150 [28:14<17:39, 19.27s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [28:27<15:32, 17.27s/it]                                                 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [28:27<15:32, 17.27s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/150 [28:46<15:50, 17.93s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 98/150 [28:59<14:12, 16.40s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [29:18<14:34, 17.15s/it]                                                 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [29:18<14:34, 17.15s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 100/150 [29:32<13:27, 16.15s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/150 [29:57<15:21, 18.81s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [30:09<13:35, 16.99s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [30:09<13:35, 16.99s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/150 [30:30<14:16, 18.23s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/150 [30:44<12:57, 16.91s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [31:03<13:07, 17.51s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [31:03<13:07, 17.51s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/150 [31:17<11:56, 16.29s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107/150 [31:39<12:51, 17.95s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [31:52<11:42, 16.73s/it]                                                  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [31:52<11:42, 16.73s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 109/150 [32:16<12:54, 18.89s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 110/150 [32:31<11:47, 17.69s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [32:56<12:55, 19.89s/it]                                                  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [32:56<12:55, 19.89s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/150 [33:11<11:33, 18.25s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 113/150 [33:32<11:49, 19.18s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [33:45<10:21, 17.27s/it]                                                  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [33:45<10:21, 17.27s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 115/150 [34:09<11:14, 19.26s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/150 [34:23<10:04, 17.77s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [34:41<09:50, 17.89s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [34:41<09:50, 17.89s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/150 [34:57<09:11, 17.23s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/150 [35:19<09:36, 18.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [35:31<08:20, 16.68s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [35:31<08:20, 16.68s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/150 [35:56<09:16, 19.18s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122/150 [36:09<08:03, 17.25s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [36:33<08:39, 19.24s/it]                                                  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [36:33<08:39, 19.24s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 124/150 [36:46<07:38, 17.63s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 125/150 [37:08<07:52, 18.89s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [37:23<07:00, 17.51s/it]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [37:23<07:00, 17.51s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/150 [37:44<07:09, 18.67s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 128/150 [37:57<06:12, 16.91s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [38:14<05:57, 17.01s/it]                                                  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [38:14<05:57, 17.01s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 130/150 [38:28<05:21, 16.05s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/150 [38:53<05:56, 18.74s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [39:05<05:03, 16.87s/it]                                                  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [39:05<05:03, 16.87s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/150 [39:27<05:12, 18.37s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 134/150 [39:40<04:27, 16.69s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [40:04<04:42, 18.84s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [40:04<04:42, 18.84s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/150 [40:19<04:10, 17.89s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137/150 [40:41<04:06, 18.95s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [40:57<03:36, 18.02s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [40:57<03:36, 18.02s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 139/150 [41:18<03:28, 18.93s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 140/150 [41:31<02:52, 17.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [41:56<02:56, 19.63s/it]                                                  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [41:56<02:56, 19.63s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/150 [42:13<02:29, 18.73s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 143/150 [42:35<02:17, 19.65s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [42:49<01:49, 18.17s/it]                                                  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [42:49<01:49, 18.17s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 145/150 [43:08<01:31, 18.38s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/150 [43:22<01:07, 16.90s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [43:43<00:54, 18.14s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [43:43<00:54, 18.14s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/150 [43:55<00:32, 16.37s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 149/150 [44:14<00:17, 17.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:28<00:00, 16.27s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:28<00:00, 16.27s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:28<00:00, 16.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [44:28<00:00, 17.79s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     train/grad_norm â–â–‚â–â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–„â–‡â–…â–…â–…â–ˆâ–†â–…â–…â–†â–…â–ƒâ–…â–„â–ƒâ–ƒ
wandb: train/learning_rate â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:          train/loss â–‡â–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–…â–…â–„â–„â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               total_flos 3.144435068161229e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.32836
wandb:      train/learning_rate 0
wandb:               train/loss 0.0864
wandb:               train_loss 0.73064
wandb:            train_runtime 2670.1675
wandb: train_samples_per_second 0.393
wandb:   train_steps_per_second 0.056
wandb: 
wandb: ðŸš€ View run fine_tune_json_fact_first_zero_shot_unstructured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/qwj2tr02
wandb: â­ï¸ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_203221-qwj2tr02/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.63s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.72s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:14<00:14, 14.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  8.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:19<00:00,  9.70s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.42s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.30s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.45s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.67s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.49s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  7.44s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.19s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.46s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.89s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
