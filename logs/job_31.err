Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 1580.23 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_203221-qwj2tr02
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_fact_first_zero_shot_unstructured_seed_314_llama2
wandb: ⭐️ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: 🚀 View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/qwj2tr02
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:25<1:02:22, 25.11s/it]  1%|▏         | 2/150 [00:38<45:26, 18.42s/it]    2%|▏         | 3/150 [01:02<51:05, 20.85s/it]                                                 2%|▏         | 3/150 [01:02<51:05, 20.85s/it]  3%|▎         | 4/150 [01:15<42:58, 17.66s/it]  3%|▎         | 5/150 [01:32<41:59, 17.38s/it]  4%|▍         | 6/150 [01:44<37:46, 15.74s/it]                                                 4%|▍         | 6/150 [01:44<37:46, 15.74s/it]  5%|▍         | 7/150 [02:06<42:13, 17.72s/it]  5%|▌         | 8/150 [02:21<39:39, 16.76s/it]  6%|▌         | 9/150 [02:40<40:56, 17.42s/it]                                                 6%|▌         | 9/150 [02:40<40:56, 17.42s/it]  7%|▋         | 10/150 [02:53<37:46, 16.19s/it]  7%|▋         | 11/150 [03:18<43:49, 18.92s/it]  8%|▊         | 12/150 [03:34<41:16, 17.95s/it]                                                  8%|▊         | 12/150 [03:34<41:16, 17.95s/it]  9%|▊         | 13/150 [03:56<43:44, 19.16s/it]  9%|▉         | 14/150 [04:12<41:11, 18.17s/it] 10%|█         | 15/150 [04:28<39:27, 17.54s/it]                                                 10%|█         | 15/150 [04:28<39:27, 17.54s/it] 11%|█         | 16/150 [04:39<35:12, 15.76s/it] 11%|█▏        | 17/150 [04:58<36:32, 16.49s/it] 12%|█▏        | 18/150 [05:10<33:49, 15.38s/it]                                                 12%|█▏        | 18/150 [05:10<33:49, 15.38s/it] 13%|█▎        | 19/150 [05:32<37:31, 17.19s/it] 13%|█▎        | 20/150 [05:45<34:24, 15.88s/it] 14%|█▍        | 21/150 [06:10<40:04, 18.64s/it]                                                 14%|█▍        | 21/150 [06:10<40:04, 18.64s/it] 15%|█▍        | 22/150 [06:23<36:26, 17.09s/it] 15%|█▌        | 23/150 [06:45<39:10, 18.51s/it] 16%|█▌        | 24/150 [06:58<35:08, 16.74s/it]                                                 16%|█▌        | 24/150 [06:58<35:08, 16.74s/it] 17%|█▋        | 25/150 [07:19<38:03, 18.26s/it] 17%|█▋        | 26/150 [07:34<35:32, 17.20s/it] 18%|█▊        | 27/150 [07:58<39:21, 19.20s/it]                                                 18%|█▊        | 27/150 [07:58<39:21, 19.20s/it] 19%|█▊        | 28/150 [08:12<36:04, 17.74s/it] 19%|█▉        | 29/150 [08:30<35:59, 17.85s/it] 20%|██        | 30/150 [08:43<32:18, 16.15s/it]                                                 20%|██        | 30/150 [08:43<32:18, 16.15s/it] 21%|██        | 31/150 [09:08<37:17, 18.81s/it] 21%|██▏       | 32/150 [09:21<33:27, 17.02s/it] 22%|██▏       | 33/150 [09:37<33:08, 17.00s/it]                                                 22%|██▏       | 33/150 [09:37<33:08, 17.00s/it] 23%|██▎       | 34/150 [09:50<30:25, 15.73s/it] 23%|██▎       | 35/150 [10:12<33:26, 17.45s/it] 24%|██▍       | 36/150 [10:29<33:01, 17.38s/it]                                                 24%|██▍       | 36/150 [10:29<33:01, 17.38s/it] 25%|██▍       | 37/150 [10:53<36:24, 19.33s/it] 25%|██▌       | 38/150 [11:05<32:06, 17.20s/it] 26%|██▌       | 39/150 [11:24<32:45, 17.71s/it]                                                 26%|██▌       | 39/150 [11:24<32:45, 17.71s/it] 27%|██▋       | 40/150 [11:37<30:08, 16.44s/it] 27%|██▋       | 41/150 [12:02<34:32, 19.01s/it] 28%|██▊       | 42/150 [12:17<31:54, 17.73s/it]                                                 28%|██▊       | 42/150 [12:17<31:54, 17.73s/it] 29%|██▊       | 43/150 [12:39<33:48, 18.96s/it] 29%|██▉       | 44/150 [12:52<30:12, 17.10s/it] 30%|███       | 45/150 [13:13<32:11, 18.39s/it]                                                 30%|███       | 45/150 [13:13<32:11, 18.39s/it] 31%|███       | 46/150 [13:28<29:58, 17.30s/it] 31%|███▏      | 47/150 [13:52<33:03, 19.25s/it] 32%|███▏      | 48/150 [14:05<29:46, 17.52s/it]                                                 32%|███▏      | 48/150 [14:05<29:46, 17.52s/it] 33%|███▎      | 49/150 [14:23<29:47, 17.70s/it] 33%|███▎      | 50/150 [14:36<27:03, 16.23s/it] 34%|███▍      | 51/150 [15:01<31:07, 18.86s/it]                                                 34%|███▍      | 51/150 [15:01<31:07, 18.86s/it] 35%|███▍      | 52/150 [15:14<27:48, 17.03s/it] 35%|███▌      | 53/150 [15:36<29:51, 18.47s/it] 36%|███▌      | 54/150 [15:50<27:33, 17.23s/it]                                                 36%|███▌      | 54/150 [15:50<27:33, 17.23s/it] 37%|███▋      | 55/150 [16:14<30:25, 19.21s/it] 37%|███▋      | 56/150 [16:27<27:04, 17.28s/it] 38%|███▊      | 57/150 [16:48<28:33, 18.43s/it]                                                 38%|███▊      | 57/150 [16:48<28:33, 18.43s/it] 39%|███▊      | 58/150 [17:02<26:33, 17.32s/it] 39%|███▉      | 59/150 [17:24<28:17, 18.65s/it] 40%|████      | 60/150 [17:39<26:04, 17.38s/it]                                                 40%|████      | 60/150 [17:39<26:04, 17.38s/it] 41%|████      | 61/150 [18:04<29:11, 19.68s/it] 41%|████▏     | 62/150 [18:19<26:46, 18.26s/it] 42%|████▏     | 63/150 [18:42<28:53, 19.92s/it]                                                 42%|████▏     | 63/150 [18:42<28:53, 19.92s/it] 43%|████▎     | 64/150 [18:55<25:29, 17.78s/it] 43%|████▎     | 65/150 [19:17<26:53, 18.98s/it] 44%|████▍     | 66/150 [19:29<23:43, 16.95s/it]                                                 44%|████▍     | 66/150 [19:29<23:43, 16.95s/it] 45%|████▍     | 67/150 [19:48<24:14, 17.53s/it] 45%|████▌     | 68/150 [20:02<22:27, 16.43s/it] 46%|████▌     | 69/150 [20:24<24:21, 18.05s/it]                                                 46%|████▌     | 69/150 [20:24<24:21, 18.05s/it] 47%|████▋     | 70/150 [20:38<22:36, 16.96s/it] 47%|████▋     | 71/150 [21:03<25:30, 19.37s/it] 48%|████▊     | 72/150 [21:16<22:36, 17.39s/it]                                                 48%|████▊     | 72/150 [21:16<22:36, 17.39s/it] 49%|████▊     | 73/150 [21:35<22:52, 17.83s/it] 49%|████▉     | 74/150 [21:47<20:32, 16.22s/it] 50%|█████     | 75/150 [22:09<22:22, 17.90s/it]                                                 50%|█████     | 75/150 [22:09<22:22, 17.90s/it] 51%|█████     | 76/150 [22:21<19:58, 16.20s/it] 51%|█████▏    | 77/150 [22:45<22:30, 18.50s/it] 52%|█████▏    | 78/150 [23:00<20:44, 17.28s/it]                                                 52%|█████▏    | 78/150 [23:00<20:44, 17.28s/it] 53%|█████▎    | 79/150 [23:21<22:02, 18.63s/it] 53%|█████▎    | 80/150 [23:36<20:22, 17.46s/it] 54%|█████▍    | 81/150 [24:01<22:42, 19.74s/it]                                                 54%|█████▍    | 81/150 [24:01<22:42, 19.74s/it] 55%|█████▍    | 82/150 [24:16<20:40, 18.24s/it] 55%|█████▌    | 83/150 [24:40<22:14, 19.92s/it] 56%|█████▌    | 84/150 [24:53<19:46, 17.98s/it]                                                 56%|█████▌    | 84/150 [24:53<19:46, 17.98s/it] 57%|█████▋    | 85/150 [25:12<19:45, 18.24s/it] 57%|█████▋    | 86/150 [25:25<17:38, 16.54s/it] 58%|█████▊    | 87/150 [25:46<18:49, 17.92s/it]                                                 58%|█████▊    | 87/150 [25:46<18:49, 17.92s/it] 59%|█████▊    | 88/150 [26:00<17:26, 16.88s/it] 59%|█████▉    | 89/150 [26:22<18:39, 18.35s/it] 60%|██████    | 90/150 [26:35<16:40, 16.67s/it]                                                 60%|██████    | 90/150 [26:35<16:40, 16.67s/it] 61%|██████    | 91/150 [27:00<18:50, 19.16s/it] 61%|██████▏   | 92/150 [27:12<16:30, 17.08s/it] 62%|██████▏   | 93/150 [27:36<18:11, 19.14s/it]                                                 62%|██████▏   | 93/150 [27:36<18:11, 19.14s/it] 63%|██████▎   | 94/150 [27:53<17:15, 18.48s/it] 63%|██████▎   | 95/150 [28:14<17:39, 19.27s/it] 64%|██████▍   | 96/150 [28:27<15:32, 17.27s/it]                                                 64%|██████▍   | 96/150 [28:27<15:32, 17.27s/it] 65%|██████▍   | 97/150 [28:46<15:50, 17.93s/it] 65%|██████▌   | 98/150 [28:59<14:12, 16.40s/it] 66%|██████▌   | 99/150 [29:18<14:34, 17.15s/it]                                                 66%|██████▌   | 99/150 [29:18<14:34, 17.15s/it] 67%|██████▋   | 100/150 [29:32<13:27, 16.15s/it] 67%|██████▋   | 101/150 [29:57<15:21, 18.81s/it] 68%|██████▊   | 102/150 [30:09<13:35, 16.99s/it]                                                  68%|██████▊   | 102/150 [30:09<13:35, 16.99s/it] 69%|██████▊   | 103/150 [30:30<14:16, 18.23s/it] 69%|██████▉   | 104/150 [30:44<12:57, 16.91s/it] 70%|███████   | 105/150 [31:03<13:07, 17.51s/it]                                                  70%|███████   | 105/150 [31:03<13:07, 17.51s/it] 71%|███████   | 106/150 [31:17<11:56, 16.29s/it] 71%|███████▏  | 107/150 [31:39<12:51, 17.95s/it] 72%|███████▏  | 108/150 [31:52<11:42, 16.73s/it]                                                  72%|███████▏  | 108/150 [31:52<11:42, 16.73s/it] 73%|███████▎  | 109/150 [32:16<12:54, 18.89s/it] 73%|███████▎  | 110/150 [32:31<11:47, 17.69s/it] 74%|███████▍  | 111/150 [32:56<12:55, 19.89s/it]                                                  74%|███████▍  | 111/150 [32:56<12:55, 19.89s/it] 75%|███████▍  | 112/150 [33:11<11:33, 18.25s/it] 75%|███████▌  | 113/150 [33:32<11:49, 19.18s/it] 76%|███████▌  | 114/150 [33:45<10:21, 17.27s/it]                                                  76%|███████▌  | 114/150 [33:45<10:21, 17.27s/it] 77%|███████▋  | 115/150 [34:09<11:14, 19.26s/it] 77%|███████▋  | 116/150 [34:23<10:04, 17.77s/it] 78%|███████▊  | 117/150 [34:41<09:50, 17.89s/it]                                                  78%|███████▊  | 117/150 [34:41<09:50, 17.89s/it] 79%|███████▊  | 118/150 [34:57<09:11, 17.23s/it] 79%|███████▉  | 119/150 [35:19<09:36, 18.59s/it] 80%|████████  | 120/150 [35:31<08:20, 16.68s/it]                                                  80%|████████  | 120/150 [35:31<08:20, 16.68s/it] 81%|████████  | 121/150 [35:56<09:16, 19.18s/it] 81%|████████▏ | 122/150 [36:09<08:03, 17.25s/it] 82%|████████▏ | 123/150 [36:33<08:39, 19.24s/it]                                                  82%|████████▏ | 123/150 [36:33<08:39, 19.24s/it] 83%|████████▎ | 124/150 [36:46<07:38, 17.63s/it] 83%|████████▎ | 125/150 [37:08<07:52, 18.89s/it] 84%|████████▍ | 126/150 [37:23<07:00, 17.51s/it]                                                  84%|████████▍ | 126/150 [37:23<07:00, 17.51s/it] 85%|████████▍ | 127/150 [37:44<07:09, 18.67s/it] 85%|████████▌ | 128/150 [37:57<06:12, 16.91s/it] 86%|████████▌ | 129/150 [38:14<05:57, 17.01s/it]                                                  86%|████████▌ | 129/150 [38:14<05:57, 17.01s/it] 87%|████████▋ | 130/150 [38:28<05:21, 16.05s/it] 87%|████████▋ | 131/150 [38:53<05:56, 18.74s/it] 88%|████████▊ | 132/150 [39:05<05:03, 16.87s/it]                                                  88%|████████▊ | 132/150 [39:05<05:03, 16.87s/it] 89%|████████▊ | 133/150 [39:27<05:12, 18.37s/it] 89%|████████▉ | 134/150 [39:40<04:27, 16.69s/it] 90%|█████████ | 135/150 [40:04<04:42, 18.84s/it]                                                  90%|█████████ | 135/150 [40:04<04:42, 18.84s/it] 91%|█████████ | 136/150 [40:19<04:10, 17.89s/it] 91%|█████████▏| 137/150 [40:41<04:06, 18.95s/it] 92%|█████████▏| 138/150 [40:57<03:36, 18.02s/it]                                                  92%|█████████▏| 138/150 [40:57<03:36, 18.02s/it] 93%|█████████▎| 139/150 [41:18<03:28, 18.93s/it] 93%|█████████▎| 140/150 [41:31<02:52, 17.28s/it] 94%|█████████▍| 141/150 [41:56<02:56, 19.63s/it]                                                  94%|█████████▍| 141/150 [41:56<02:56, 19.63s/it] 95%|█████████▍| 142/150 [42:13<02:29, 18.73s/it] 95%|█████████▌| 143/150 [42:35<02:17, 19.65s/it] 96%|█████████▌| 144/150 [42:49<01:49, 18.17s/it]                                                  96%|█████████▌| 144/150 [42:49<01:49, 18.17s/it] 97%|█████████▋| 145/150 [43:08<01:31, 18.38s/it] 97%|█████████▋| 146/150 [43:22<01:07, 16.90s/it] 98%|█████████▊| 147/150 [43:43<00:54, 18.14s/it]                                                  98%|█████████▊| 147/150 [43:43<00:54, 18.14s/it] 99%|█████████▊| 148/150 [43:55<00:32, 16.37s/it] 99%|█████████▉| 149/150 [44:14<00:17, 17.30s/it]100%|██████████| 150/150 [44:28<00:00, 16.27s/it]                                                 100%|██████████| 150/150 [44:28<00:00, 16.27s/it]                                                 100%|██████████| 150/150 [44:28<00:00, 16.27s/it]100%|██████████| 150/150 [44:28<00:00, 17.79s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:     train/grad_norm ▁▂▁▂▂▂▃▁▂▂▁▂▁▂▂▂▂▂▂▂▂▃▂▂▄▇▅▅▅█▆▅▅▆▅▃▅▄▃▃
wandb: train/learning_rate ▂▄▅▇███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁███▇▇▇▆▅▄▄▃▃▂▂▁▁▁
wandb:          train/loss ▇█▇▇▆▆▅▅▅▅▅▄▄▅▄▄▄▄▄▄▄▄▄▄▄▃▂▃▂▃▂▂▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 3.144435068161229e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.32836
wandb:      train/learning_rate 0
wandb:               train/loss 0.0864
wandb:               train_loss 0.73064
wandb:            train_runtime 2670.1675
wandb: train_samples_per_second 0.393
wandb:   train_steps_per_second 0.056
wandb: 
wandb: 🚀 View run fine_tune_json_fact_first_zero_shot_unstructured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/qwj2tr02
wandb: ⭐️ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_203221-qwj2tr02/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.63s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.72s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.70s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.42s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.30s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.45s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.67s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.49s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.19s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.46s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.89s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
