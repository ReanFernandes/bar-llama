Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 504.01 examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 479.36 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_185956-c83uj2qh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_answer_first_zero_shot_structured_seed_314_llama2
wandb: ⭐️ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: 🚀 View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/c83uj2qh
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:27<1:07:56, 27.36s/it]  1%|▏         | 2/150 [00:44<52:02, 21.10s/it]    2%|▏         | 3/150 [01:07<54:41, 22.32s/it]                                                 2%|▏         | 3/150 [01:08<54:41, 22.32s/it]  3%|▎         | 4/150 [01:26<50:25, 20.72s/it]  3%|▎         | 5/150 [01:49<52:25, 21.69s/it]  4%|▍         | 6/150 [02:06<48:19, 20.13s/it]                                                 4%|▍         | 6/150 [02:06<48:19, 20.13s/it]  5%|▍         | 7/150 [02:30<50:38, 21.25s/it]  5%|▌         | 8/150 [02:48<47:56, 20.26s/it]  6%|▌         | 9/150 [03:12<50:13, 21.37s/it]                                                 6%|▌         | 9/150 [03:12<50:13, 21.37s/it]  7%|▋         | 10/150 [03:28<46:31, 19.94s/it]  7%|▋         | 11/150 [03:53<49:50, 21.51s/it]  8%|▊         | 12/150 [04:11<46:28, 20.21s/it]                                                  8%|▊         | 12/150 [04:11<46:28, 20.21s/it]  9%|▊         | 13/150 [04:35<48:47, 21.37s/it]  9%|▉         | 14/150 [04:53<46:17, 20.42s/it] 10%|█         | 15/150 [05:17<48:09, 21.40s/it]                                                 10%|█         | 15/150 [05:17<48:09, 21.40s/it] 11%|█         | 16/150 [05:35<45:29, 20.37s/it] 11%|█▏        | 17/150 [05:57<46:47, 21.11s/it] 12%|█▏        | 18/150 [06:16<44:31, 20.24s/it]                                                 12%|█▏        | 18/150 [06:16<44:31, 20.24s/it] 13%|█▎        | 19/150 [06:36<44:11, 20.24s/it] 13%|█▎        | 20/150 [06:52<41:28, 19.14s/it] 14%|█▍        | 21/150 [07:17<44:56, 20.90s/it]                                                 14%|█▍        | 21/150 [07:17<44:56, 20.90s/it] 15%|█▍        | 22/150 [07:34<41:57, 19.66s/it] 15%|█▌        | 23/150 [07:57<43:37, 20.61s/it] 16%|█▌        | 24/150 [08:15<41:49, 19.92s/it]                                                 16%|█▌        | 24/150 [08:15<41:49, 19.92s/it] 17%|█▋        | 25/150 [08:39<43:56, 21.09s/it] 17%|█▋        | 26/150 [08:57<41:48, 20.23s/it] 18%|█▊        | 27/150 [09:21<43:48, 21.37s/it]                                                 18%|█▊        | 27/150 [09:21<43:48, 21.37s/it] 19%|█▊        | 28/150 [09:39<41:20, 20.33s/it] 19%|█▉        | 29/150 [10:01<41:39, 20.65s/it] 20%|██        | 30/150 [10:18<38:57, 19.48s/it]                                                 20%|██        | 30/150 [10:18<38:57, 19.48s/it] 21%|██        | 31/150 [10:43<41:56, 21.15s/it] 21%|██▏       | 32/150 [11:01<39:51, 20.27s/it] 22%|██▏       | 33/150 [11:20<38:48, 19.90s/it]                                                 22%|██▏       | 33/150 [11:20<38:48, 19.90s/it] 23%|██▎       | 34/150 [11:37<36:38, 18.95s/it] 23%|██▎       | 35/150 [12:00<39:07, 20.41s/it] 24%|██▍       | 36/150 [12:19<37:30, 19.74s/it]                                                 24%|██▍       | 36/150 [12:19<37:30, 19.74s/it] 25%|██▍       | 37/150 [12:43<39:35, 21.02s/it] 25%|██▌       | 38/150 [12:59<36:41, 19.66s/it] 26%|██▌       | 39/150 [13:23<38:39, 20.90s/it]                                                 26%|██▌       | 39/150 [13:23<38:39, 20.90s/it] 27%|██▋       | 40/150 [13:40<36:16, 19.79s/it] 27%|██▋       | 41/150 [14:05<38:47, 21.36s/it] 28%|██▊       | 42/150 [14:23<36:42, 20.39s/it]                                                 28%|██▊       | 42/150 [14:23<36:42, 20.39s/it] 29%|██▊       | 43/150 [14:47<38:14, 21.45s/it] 29%|██▉       | 44/150 [15:05<36:13, 20.50s/it] 30%|███       | 45/150 [15:26<36:02, 20.59s/it]                                                 30%|███       | 45/150 [15:26<36:02, 20.59s/it] 31%|███       | 46/150 [15:44<34:28, 19.89s/it] 31%|███▏      | 47/150 [16:08<36:15, 21.12s/it] 32%|███▏      | 48/150 [16:24<33:15, 19.57s/it]                                                 32%|███▏      | 48/150 [16:24<33:15, 19.57s/it] 33%|███▎      | 49/150 [16:46<33:44, 20.05s/it] 33%|███▎      | 50/150 [17:02<31:51, 19.11s/it] 34%|███▍      | 51/150 [17:27<34:26, 20.88s/it]                                                 34%|███▍      | 51/150 [17:27<34:26, 20.88s/it] 35%|███▍      | 52/150 [17:44<31:59, 19.59s/it] 35%|███▌      | 53/150 [18:08<33:38, 20.80s/it] 36%|███▌      | 54/150 [18:25<31:33, 19.72s/it]                                                 36%|███▌      | 54/150 [18:25<31:33, 19.72s/it] 37%|███▋      | 55/150 [18:49<33:11, 20.96s/it] 37%|███▋      | 56/150 [19:05<30:51, 19.70s/it] 38%|███▊      | 57/150 [19:26<30:47, 19.87s/it]                                                 38%|███▊      | 57/150 [19:26<30:47, 19.87s/it] 39%|███▊      | 58/150 [19:44<29:42, 19.38s/it] 39%|███▉      | 59/150 [20:08<31:29, 20.76s/it] 40%|████      | 60/150 [20:26<29:58, 19.98s/it]                                                 40%|████      | 60/150 [20:26<29:58, 19.98s/it] 41%|████      | 61/150 [20:51<31:53, 21.50s/it] 41%|████▏     | 62/150 [21:09<29:56, 20.42s/it] 42%|████▏     | 63/150 [21:33<31:10, 21.49s/it]                                                 42%|████▏     | 63/150 [21:33<31:10, 21.49s/it] 43%|████▎     | 64/150 [21:50<28:46, 20.08s/it] 43%|████▎     | 65/150 [22:13<29:35, 20.89s/it] 44%|████▍     | 66/150 [22:29<27:23, 19.57s/it]                                                 44%|████▍     | 66/150 [22:29<27:23, 19.57s/it] 45%|████▍     | 67/150 [22:50<27:45, 20.06s/it] 45%|████▌     | 68/150 [23:09<26:39, 19.51s/it] 46%|████▌     | 69/150 [23:32<28:05, 20.81s/it]                                                 46%|████▌     | 69/150 [23:32<28:05, 20.81s/it] 47%|████▋     | 70/150 [23:51<26:41, 20.02s/it] 47%|████▋     | 71/150 [24:16<28:20, 21.53s/it] 48%|████▊     | 72/150 [24:35<27:02, 20.80s/it]                                                 48%|████▊     | 72/150 [24:35<27:02, 20.80s/it] 49%|████▊     | 73/150 [24:58<27:27, 21.40s/it] 49%|████▉     | 74/150 [25:14<25:20, 20.01s/it] 50%|█████     | 75/150 [25:37<26:03, 20.84s/it]                                                 50%|█████     | 75/150 [25:37<26:03, 20.84s/it] 51%|█████     | 76/150 [25:55<24:44, 20.06s/it] 51%|█████▏    | 77/150 [26:19<25:50, 21.25s/it] 52%|█████▏    | 78/150 [26:38<24:23, 20.33s/it]                                                 52%|█████▏    | 78/150 [26:38<24:23, 20.33s/it] 53%|█████▎    | 79/150 [27:01<25:16, 21.37s/it] 53%|█████▎    | 80/150 [27:18<23:18, 19.97s/it] 54%|█████▍    | 81/150 [27:43<24:43, 21.50s/it]                                                 54%|█████▍    | 81/150 [27:43<24:43, 21.50s/it] 55%|█████▍    | 82/150 [28:02<23:27, 20.70s/it] 55%|█████▌    | 83/150 [28:26<24:13, 21.70s/it] 56%|█████▌    | 84/150 [28:45<22:57, 20.87s/it]                                                 56%|█████▌    | 84/150 [28:45<22:57, 20.87s/it] 57%|█████▋    | 85/150 [29:06<22:32, 20.81s/it] 57%|█████▋    | 86/150 [29:22<20:50, 19.54s/it] 58%|█████▊    | 87/150 [29:46<21:46, 20.73s/it]                                                 58%|█████▊    | 87/150 [29:46<21:46, 20.73s/it] 59%|█████▊    | 88/150 [30:04<20:36, 19.94s/it] 59%|█████▉    | 89/150 [30:27<21:08, 20.80s/it] 60%|██████    | 90/150 [30:43<19:35, 19.59s/it]                                                 60%|██████    | 90/150 [30:43<19:35, 19.59s/it] 61%|██████    | 91/150 [31:08<20:50, 21.20s/it] 61%|██████▏   | 92/150 [31:25<19:07, 19.78s/it] 62%|██████▏   | 93/150 [31:49<20:00, 21.06s/it]                                                 62%|██████▏   | 93/150 [31:49<20:00, 21.06s/it] 63%|██████▎   | 94/150 [32:08<19:07, 20.50s/it] 63%|██████▎   | 95/150 [32:32<19:41, 21.48s/it] 64%|██████▍   | 96/150 [32:51<18:37, 20.69s/it]                                                 64%|██████▍   | 96/150 [32:51<18:37, 20.69s/it] 65%|██████▍   | 97/150 [33:14<19:07, 21.64s/it] 65%|██████▌   | 98/150 [33:33<17:51, 20.61s/it] 66%|██████▌   | 99/150 [33:56<18:15, 21.48s/it]                                                 66%|██████▌   | 99/150 [33:56<18:15, 21.48s/it] 67%|██████▋   | 100/150 [34:13<16:42, 20.06s/it] 67%|██████▋   | 101/150 [34:38<17:35, 21.54s/it] 68%|██████▊   | 102/150 [34:55<16:05, 20.11s/it]                                                  68%|██████▊   | 102/150 [34:55<16:05, 20.11s/it] 69%|██████▊   | 103/150 [35:14<15:28, 19.76s/it] 69%|██████▉   | 104/150 [35:31<14:33, 18.99s/it] 70%|███████   | 105/150 [35:54<15:15, 20.35s/it]                                                  70%|███████   | 105/150 [35:54<15:15, 20.35s/it] 71%|███████   | 106/150 [36:11<14:10, 19.33s/it] 71%|███████▏  | 107/150 [36:35<14:49, 20.68s/it] 72%|███████▏  | 108/150 [36:54<14:12, 20.29s/it]                                                  72%|███████▏  | 108/150 [36:54<14:12, 20.29s/it] 73%|███████▎  | 109/150 [37:19<14:38, 21.42s/it] 73%|███████▎  | 110/150 [37:37<13:36, 20.41s/it] 74%|███████▍  | 111/150 [38:02<14:09, 21.79s/it]                                                  74%|███████▍  | 111/150 [38:02<14:09, 21.79s/it] 75%|███████▍  | 112/150 [38:20<13:06, 20.71s/it] 75%|███████▌  | 113/150 [38:41<12:47, 20.74s/it] 76%|███████▌  | 114/150 [38:57<11:43, 19.55s/it]                                                  76%|███████▌  | 114/150 [38:57<11:43, 19.55s/it] 77%|███████▋  | 115/150 [39:21<12:11, 20.90s/it] 77%|███████▋  | 116/150 [39:39<11:20, 20.02s/it] 78%|███████▊  | 117/150 [40:00<11:08, 20.26s/it]                                                  78%|███████▊  | 117/150 [40:00<11:08, 20.26s/it] 79%|███████▊  | 118/150 [40:17<10:18, 19.32s/it] 79%|███████▉  | 119/150 [40:41<10:41, 20.69s/it] 80%|████████  | 120/150 [40:58<09:49, 19.64s/it]                                                  80%|████████  | 120/150 [40:58<09:49, 19.64s/it] 81%|████████  | 121/150 [41:23<10:16, 21.25s/it] 81%|████████▏ | 122/150 [41:42<09:29, 20.34s/it] 82%|████████▏ | 123/150 [42:05<09:37, 21.39s/it]                                                  82%|████████▏ | 123/150 [42:05<09:37, 21.39s/it] 83%|████████▎ | 124/150 [42:23<08:43, 20.12s/it] 83%|████████▎ | 125/150 [42:47<08:52, 21.30s/it] 84%|████████▍ | 126/150 [43:05<08:09, 20.38s/it]                                                  84%|████████▍ | 126/150 [43:05<08:09, 20.38s/it] 85%|████████▍ | 127/150 [43:24<07:42, 20.11s/it] 85%|████████▌ | 128/150 [43:41<07:00, 19.10s/it] 86%|████████▌ | 129/150 [44:05<07:09, 20.43s/it]                                                  86%|████████▌ | 129/150 [44:05<07:09, 20.43s/it] 87%|████████▋ | 130/150 [44:23<06:38, 19.94s/it] 87%|████████▋ | 131/150 [44:48<06:47, 21.47s/it] 88%|████████▊ | 132/150 [45:07<06:08, 20.47s/it]                                                  88%|████████▊ | 132/150 [45:07<06:08, 20.47s/it] 89%|████████▊ | 133/150 [45:29<05:59, 21.17s/it] 89%|████████▉ | 134/150 [45:47<05:23, 20.20s/it] 90%|█████████ | 135/150 [46:11<05:19, 21.29s/it]                                                  90%|█████████ | 135/150 [46:11<05:19, 21.29s/it] 91%|█████████ | 136/150 [46:28<04:38, 19.93s/it] 91%|█████████▏| 137/150 [46:52<04:33, 21.02s/it] 92%|█████████▏| 138/150 [47:10<04:04, 20.40s/it]                                                  92%|█████████▏| 138/150 [47:10<04:04, 20.40s/it] 93%|█████████▎| 139/150 [47:34<03:54, 21.35s/it] 93%|█████████▎| 140/150 [47:51<03:19, 19.98s/it] 94%|█████████▍| 141/150 [48:16<03:13, 21.50s/it]                                                  94%|█████████▍| 141/150 [48:16<03:13, 21.50s/it] 95%|█████████▍| 142/150 [48:35<02:46, 20.79s/it] 95%|█████████▌| 143/150 [48:59<02:31, 21.68s/it] 96%|█████████▌| 144/150 [49:16<02:01, 20.32s/it]                                                  96%|█████████▌| 144/150 [49:16<02:01, 20.32s/it] 97%|█████████▋| 145/150 [49:39<01:46, 21.29s/it] 97%|█████████▋| 146/150 [49:56<01:19, 19.93s/it] 98%|█████████▊| 147/150 [50:20<01:03, 21.01s/it]                                                  98%|█████████▊| 147/150 [50:20<01:03, 21.01s/it] 99%|█████████▊| 148/150 [50:38<00:40, 20.08s/it] 99%|█████████▉| 149/150 [50:58<00:20, 20.14s/it]100%|██████████| 150/150 [51:16<00:00, 19.58s/it]                                                 100%|██████████| 150/150 [51:16<00:00, 19.58s/it]                                                 100%|██████████| 150/150 [51:16<00:00, 19.58s/it]100%|██████████| 150/150 [51:16<00:00, 20.51s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:     train/grad_norm ▁▂▂▃▂▃▂▂▁▂▁▁▁▂▁▂▂▂▂▂▃▂▄▅▆▆██▇▇█▆██▇▅▅▅▃▃
wandb: train/learning_rate ▂▄▇███▇▇▇▆▆▅▅▄▄▃▂▂▁▁▁████▇▇▆▆▅▄▄▃▃▃▂▂▁▁▁
wandb:          train/loss ██▇▇▆▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 3.608829859710566e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.2146
wandb:      train/learning_rate 0
wandb:               train/loss 0.0567
wandb:               train_loss 0.56266
wandb:            train_runtime 3080.739
wandb: train_samples_per_second 0.341
wandb:   train_steps_per_second 0.049
wandb: 
wandb: 🚀 View run fine_tune_json_answer_first_zero_shot_structured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/c83uj2qh
wandb: ⭐️ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_185956-c83uj2qh/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  5.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.56s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.56s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.67s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.51s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.01s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
