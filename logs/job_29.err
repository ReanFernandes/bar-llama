Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.42s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:00<00:00, 689.06 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_202600-sbey2eny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_fact_first_zero_shot_structured_seed_314_llama2
wandb: â­ï¸ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: ðŸš€ View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/sbey2eny
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:25<1:03:23, 25.53s/it]  1%|â–         | 2/150 [00:42<50:40, 20.54s/it]    2%|â–         | 3/150 [01:06<54:30, 22.25s/it]                                                 2%|â–         | 3/150 [01:06<54:30, 22.25s/it]  3%|â–Ž         | 4/150 [01:25<50:28, 20.74s/it]  3%|â–Ž         | 5/150 [01:49<52:49, 21.86s/it]  4%|â–         | 6/150 [02:06<48:52, 20.37s/it]                                                 4%|â–         | 6/150 [02:06<48:52, 20.37s/it]  5%|â–         | 7/150 [02:30<51:23, 21.56s/it]  5%|â–Œ         | 8/150 [02:49<48:43, 20.59s/it]  6%|â–Œ         | 9/150 [03:13<51:04, 21.73s/it]                                                 6%|â–Œ         | 9/150 [03:13<51:04, 21.73s/it]  7%|â–‹         | 10/150 [03:30<47:21, 20.30s/it]  7%|â–‹         | 11/150 [03:56<50:50, 21.94s/it]  8%|â–Š         | 12/150 [04:13<47:24, 20.61s/it]                                                  8%|â–Š         | 12/150 [04:13<47:24, 20.61s/it]  9%|â–Š         | 13/150 [04:38<49:44, 21.79s/it]  9%|â–‰         | 14/150 [04:56<47:11, 20.82s/it] 10%|â–ˆ         | 15/150 [05:20<49:01, 21.79s/it]                                                 10%|â–ˆ         | 15/150 [05:20<49:01, 21.79s/it] 11%|â–ˆ         | 16/150 [05:39<46:18, 20.73s/it] 11%|â–ˆâ–        | 17/150 [06:02<47:38, 21.49s/it] 12%|â–ˆâ–        | 18/150 [06:20<45:19, 20.60s/it]                                                 12%|â–ˆâ–        | 18/150 [06:20<45:19, 20.60s/it] 13%|â–ˆâ–Ž        | 19/150 [06:41<45:02, 20.63s/it] 13%|â–ˆâ–Ž        | 20/150 [06:58<42:16, 19.51s/it] 14%|â–ˆâ–        | 21/150 [07:24<45:52, 21.34s/it]                                                 14%|â–ˆâ–        | 21/150 [07:24<45:52, 21.34s/it] 15%|â–ˆâ–        | 22/150 [07:41<42:49, 20.08s/it] 15%|â–ˆâ–Œ        | 23/150 [08:04<44:33, 21.05s/it] 16%|â–ˆâ–Œ        | 24/150 [08:23<42:45, 20.36s/it]                                                 16%|â–ˆâ–Œ        | 24/150 [08:23<42:45, 20.36s/it] 17%|â–ˆâ–‹        | 25/150 [08:47<44:56, 21.57s/it] 17%|â–ˆâ–‹        | 26/150 [09:06<42:46, 20.70s/it] 18%|â–ˆâ–Š        | 27/150 [09:30<44:49, 21.86s/it]                                                 18%|â–ˆâ–Š        | 27/150 [09:30<44:49, 21.86s/it] 19%|â–ˆâ–Š        | 28/150 [09:49<42:17, 20.80s/it] 19%|â–ˆâ–‰        | 29/150 [10:11<42:36, 21.12s/it] 20%|â–ˆâ–ˆ        | 30/150 [10:28<39:51, 19.93s/it]                                                 20%|â–ˆâ–ˆ        | 30/150 [10:28<39:51, 19.93s/it] 21%|â–ˆâ–ˆ        | 31/150 [10:53<42:55, 21.65s/it] 21%|â–ˆâ–ˆâ–       | 32/150 [11:12<40:46, 20.74s/it] 22%|â–ˆâ–ˆâ–       | 33/150 [11:31<39:40, 20.35s/it]                                                 22%|â–ˆâ–ˆâ–       | 33/150 [11:31<39:40, 20.35s/it] 23%|â–ˆâ–ˆâ–Ž       | 34/150 [11:49<37:27, 19.38s/it] 23%|â–ˆâ–ˆâ–Ž       | 35/150 [12:13<39:59, 20.87s/it] 24%|â–ˆâ–ˆâ–       | 36/150 [12:31<38:19, 20.17s/it]                                                 24%|â–ˆâ–ˆâ–       | 36/150 [12:31<38:19, 20.17s/it] 25%|â–ˆâ–ˆâ–       | 37/150 [12:56<40:26, 21.47s/it] 25%|â–ˆâ–ˆâ–Œ       | 38/150 [13:13<37:30, 20.10s/it] 26%|â–ˆâ–ˆâ–Œ       | 39/150 [13:37<39:28, 21.34s/it]                                                 26%|â–ˆâ–ˆâ–Œ       | 39/150 [13:37<39:28, 21.34s/it] 27%|â–ˆâ–ˆâ–‹       | 40/150 [13:55<37:03, 20.21s/it] 27%|â–ˆâ–ˆâ–‹       | 41/150 [14:20<39:40, 21.84s/it] 28%|â–ˆâ–ˆâ–Š       | 42/150 [14:39<37:31, 20.85s/it]                                                 28%|â–ˆâ–ˆâ–Š       | 42/150 [14:39<37:31, 20.85s/it] 29%|â–ˆâ–ˆâ–Š       | 43/150 [15:03<39:04, 21.91s/it] 29%|â–ˆâ–ˆâ–‰       | 44/150 [15:22<37:02, 20.96s/it] 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [15:43<36:56, 21.11s/it]                                                 30%|â–ˆâ–ˆâ–ˆ       | 45/150 [15:43<36:56, 21.11s/it] 31%|â–ˆâ–ˆâ–ˆ       | 46/150 [16:02<35:18, 20.37s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 47/150 [16:27<37:04, 21.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [16:43<34:01, 20.01s/it]                                                 32%|â–ˆâ–ˆâ–ˆâ–      | 48/150 [16:43<34:01, 20.01s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 49/150 [17:05<34:33, 20.53s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 50/150 [17:22<32:35, 19.56s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [17:48<35:15, 21.37s/it]                                                 34%|â–ˆâ–ˆâ–ˆâ–      | 51/150 [17:48<35:15, 21.37s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 52/150 [18:04<32:43, 20.04s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 53/150 [18:29<34:21, 21.26s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [18:46<32:14, 20.15s/it]                                                 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/150 [18:46<32:14, 20.15s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 55/150 [19:10<33:54, 21.42s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/150 [19:28<31:32, 20.13s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [19:48<31:30, 20.33s/it]                                                 38%|â–ˆâ–ˆâ–ˆâ–Š      | 57/150 [19:48<31:30, 20.33s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 58/150 [20:07<30:23, 19.82s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 59/150 [20:32<32:11, 21.22s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [20:50<30:37, 20.42s/it]                                                 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 60/150 [20:50<30:37, 20.42s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 61/150 [21:16<32:36, 21.98s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 62/150 [21:34<30:37, 20.88s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [21:59<31:50, 21.96s/it]                                                 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 63/150 [21:59<31:50, 21.96s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 64/150 [22:16<29:24, 20.51s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 65/150 [22:39<30:14, 21.35s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [22:56<28:00, 20.01s/it]                                                 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/150 [22:56<28:00, 20.01s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/150 [23:18<28:23, 20.53s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 68/150 [23:36<27:16, 19.96s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [24:01<28:43, 21.27s/it]                                                 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/150 [24:01<28:43, 21.27s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 70/150 [24:19<27:18, 20.48s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/150 [24:45<28:59, 22.02s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [25:04<27:39, 21.28s/it]                                                 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 72/150 [25:04<27:39, 21.28s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/150 [25:28<28:05, 21.89s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 74/150 [25:45<25:55, 20.46s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [26:08<26:38, 21.31s/it]                                                 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 75/150 [26:08<26:38, 21.31s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 76/150 [26:27<25:17, 20.51s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 77/150 [26:51<26:25, 21.71s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [27:10<24:55, 20.77s/it]                                                 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78/150 [27:10<24:55, 20.77s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 79/150 [27:34<25:48, 21.82s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 80/150 [27:51<23:48, 20.40s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [28:17<25:17, 21.99s/it]                                                 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/150 [28:17<25:17, 21.99s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/150 [28:36<23:59, 21.17s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 83/150 [29:01<24:45, 22.18s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [29:20<23:27, 21.32s/it]                                                 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 84/150 [29:20<23:27, 21.32s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 85/150 [29:41<23:02, 21.28s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 86/150 [29:58<21:18, 19.98s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [30:22<22:14, 21.18s/it]                                                 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 87/150 [30:22<22:14, 21.18s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/150 [30:41<21:03, 20.38s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 89/150 [31:04<21:36, 21.25s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [31:21<20:00, 20.02s/it]                                                 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 90/150 [31:21<20:00, 20.02s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 91/150 [31:47<21:20, 21.70s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 92/150 [32:03<19:34, 20.25s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [32:28<20:27, 21.53s/it]                                                 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93/150 [32:28<20:27, 21.53s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 94/150 [32:48<19:33, 20.96s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 95/150 [33:12<20:06, 21.94s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [33:31<19:01, 21.14s/it]                                                 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/150 [33:31<19:01, 21.14s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/150 [33:55<19:31, 22.10s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 98/150 [34:14<18:15, 21.06s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [34:38<18:38, 21.94s/it]                                                 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 99/150 [34:38<18:38, 21.94s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 100/150 [34:55<17:04, 20.49s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 101/150 [35:21<17:59, 22.04s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [35:38<16:27, 20.56s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 102/150 [35:38<16:27, 20.56s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 103/150 [35:57<15:49, 20.21s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 104/150 [36:15<14:53, 19.42s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [36:39<15:35, 20.80s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 105/150 [36:39<15:35, 20.80s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 106/150 [36:56<14:29, 19.75s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107/150 [37:21<15:08, 21.12s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [37:40<14:30, 20.74s/it]                                                  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108/150 [37:40<14:30, 20.74s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 109/150 [38:05<14:57, 21.88s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 110/150 [38:23<13:54, 20.85s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [38:49<14:29, 22.29s/it]                                                  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/150 [38:49<14:29, 22.29s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/150 [39:08<13:24, 21.17s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 113/150 [39:29<13:04, 21.21s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [39:46<11:59, 19.99s/it]                                                  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/150 [39:46<11:59, 19.99s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 115/150 [40:11<12:27, 21.37s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 116/150 [40:29<11:35, 20.47s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [40:50<11:23, 20.73s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 117/150 [40:50<11:23, 20.73s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 118/150 [41:08<10:32, 19.77s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 119/150 [41:32<10:55, 21.15s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [41:50<10:02, 20.08s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 120/150 [41:50<10:02, 20.08s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 121/150 [42:15<10:30, 21.75s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122/150 [42:34<09:42, 20.81s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [42:58<09:50, 21.87s/it]                                                  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123/150 [42:58<09:50, 21.87s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 124/150 [43:16<08:54, 20.57s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 125/150 [43:40<09:03, 21.74s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [43:59<08:18, 20.78s/it]                                                  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/150 [43:59<08:18, 20.78s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/150 [44:19<07:51, 20.49s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 128/150 [44:36<07:08, 19.47s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [45:00<07:17, 20.82s/it]                                                  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 129/150 [45:00<07:17, 20.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 130/150 [45:19<06:46, 20.33s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 131/150 [45:45<06:56, 21.92s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [46:03<06:16, 20.89s/it]                                                  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/150 [46:03<06:16, 20.89s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 133/150 [46:26<06:07, 21.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 134/150 [46:45<05:30, 20.64s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [47:09<05:26, 21.74s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 135/150 [47:09<05:26, 21.74s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 136/150 [47:26<04:44, 20.36s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137/150 [47:50<04:39, 21.48s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [48:10<04:10, 20.84s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/150 [48:10<04:10, 20.84s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 139/150 [48:34<03:59, 21.82s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 140/150 [48:51<03:24, 20.42s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [49:17<03:17, 22.00s/it]                                                  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/150 [49:17<03:17, 22.00s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/150 [49:36<02:50, 21.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 143/150 [50:00<02:35, 22.17s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [50:18<02:04, 20.78s/it]                                                  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 144/150 [50:18<02:04, 20.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 145/150 [50:42<01:48, 21.78s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 146/150 [50:59<01:21, 20.38s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [51:23<01:04, 21.47s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 147/150 [51:23<01:04, 21.47s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 148/150 [51:42<00:41, 20.52s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 149/150 [52:02<00:20, 20.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [52:21<00:00, 20.05s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [52:21<00:00, 20.05s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [52:21<00:00, 20.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [52:21<00:00, 20.94s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     train/grad_norm â–â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–…â–…â–…â–†â–‡â–†â–ˆâ–‡â–†â–‡â–‡â–ƒâ–„â–„â–…â–ƒ
wandb: train/learning_rate â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:          train/loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               total_flos 3.608829859710566e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.22621
wandb:      train/learning_rate 0
wandb:               train/loss 0.06
wandb:               train_loss 0.56691
wandb:            train_runtime 3144.9801
wandb: train_samples_per_second 0.334
wandb:   train_steps_per_second 0.048
wandb: 
wandb: ðŸš€ View run fine_tune_json_fact_first_zero_shot_structured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/sbey2eny
wandb: â­ï¸ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_202600-sbey2eny/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:13<00:13, 13.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.29s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.73s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.82s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.83s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  7.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.62s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.73s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:09<00:09,  9.16s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.01s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  6.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.75s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:27<00:27, 27.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:36<00:00, 16.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:36<00:00, 18.22s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.91s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  7.38s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.21s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
