Unused kwargs: ['use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.42s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 689.06 examples/s]
wandb: Currently logged in as: rean-fernandes. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.3
wandb: Run data is saved locally in /pfs/work7/workspace/scratch/fr_rf1031-model_temp_storage/bar-llama/wandb/run-20250306_202600-sbey2eny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine_tune_json_fact_first_zero_shot_structured_seed_314_llama2
wandb: ⭐️ View project at https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: 🚀 View run at https://wandb.ai/rean-fernandes/Final_runs_paper/runs/sbey2eny
  0%|          | 0/150 [00:00<?, ?it/s]  1%|          | 1/150 [00:25<1:03:23, 25.53s/it]  1%|▏         | 2/150 [00:42<50:40, 20.54s/it]    2%|▏         | 3/150 [01:06<54:30, 22.25s/it]                                                 2%|▏         | 3/150 [01:06<54:30, 22.25s/it]  3%|▎         | 4/150 [01:25<50:28, 20.74s/it]  3%|▎         | 5/150 [01:49<52:49, 21.86s/it]  4%|▍         | 6/150 [02:06<48:52, 20.37s/it]                                                 4%|▍         | 6/150 [02:06<48:52, 20.37s/it]  5%|▍         | 7/150 [02:30<51:23, 21.56s/it]  5%|▌         | 8/150 [02:49<48:43, 20.59s/it]  6%|▌         | 9/150 [03:13<51:04, 21.73s/it]                                                 6%|▌         | 9/150 [03:13<51:04, 21.73s/it]  7%|▋         | 10/150 [03:30<47:21, 20.30s/it]  7%|▋         | 11/150 [03:56<50:50, 21.94s/it]  8%|▊         | 12/150 [04:13<47:24, 20.61s/it]                                                  8%|▊         | 12/150 [04:13<47:24, 20.61s/it]  9%|▊         | 13/150 [04:38<49:44, 21.79s/it]  9%|▉         | 14/150 [04:56<47:11, 20.82s/it] 10%|█         | 15/150 [05:20<49:01, 21.79s/it]                                                 10%|█         | 15/150 [05:20<49:01, 21.79s/it] 11%|█         | 16/150 [05:39<46:18, 20.73s/it] 11%|█▏        | 17/150 [06:02<47:38, 21.49s/it] 12%|█▏        | 18/150 [06:20<45:19, 20.60s/it]                                                 12%|█▏        | 18/150 [06:20<45:19, 20.60s/it] 13%|█▎        | 19/150 [06:41<45:02, 20.63s/it] 13%|█▎        | 20/150 [06:58<42:16, 19.51s/it] 14%|█▍        | 21/150 [07:24<45:52, 21.34s/it]                                                 14%|█▍        | 21/150 [07:24<45:52, 21.34s/it] 15%|█▍        | 22/150 [07:41<42:49, 20.08s/it] 15%|█▌        | 23/150 [08:04<44:33, 21.05s/it] 16%|█▌        | 24/150 [08:23<42:45, 20.36s/it]                                                 16%|█▌        | 24/150 [08:23<42:45, 20.36s/it] 17%|█▋        | 25/150 [08:47<44:56, 21.57s/it] 17%|█▋        | 26/150 [09:06<42:46, 20.70s/it] 18%|█▊        | 27/150 [09:30<44:49, 21.86s/it]                                                 18%|█▊        | 27/150 [09:30<44:49, 21.86s/it] 19%|█▊        | 28/150 [09:49<42:17, 20.80s/it] 19%|█▉        | 29/150 [10:11<42:36, 21.12s/it] 20%|██        | 30/150 [10:28<39:51, 19.93s/it]                                                 20%|██        | 30/150 [10:28<39:51, 19.93s/it] 21%|██        | 31/150 [10:53<42:55, 21.65s/it] 21%|██▏       | 32/150 [11:12<40:46, 20.74s/it] 22%|██▏       | 33/150 [11:31<39:40, 20.35s/it]                                                 22%|██▏       | 33/150 [11:31<39:40, 20.35s/it] 23%|██▎       | 34/150 [11:49<37:27, 19.38s/it] 23%|██▎       | 35/150 [12:13<39:59, 20.87s/it] 24%|██▍       | 36/150 [12:31<38:19, 20.17s/it]                                                 24%|██▍       | 36/150 [12:31<38:19, 20.17s/it] 25%|██▍       | 37/150 [12:56<40:26, 21.47s/it] 25%|██▌       | 38/150 [13:13<37:30, 20.10s/it] 26%|██▌       | 39/150 [13:37<39:28, 21.34s/it]                                                 26%|██▌       | 39/150 [13:37<39:28, 21.34s/it] 27%|██▋       | 40/150 [13:55<37:03, 20.21s/it] 27%|██▋       | 41/150 [14:20<39:40, 21.84s/it] 28%|██▊       | 42/150 [14:39<37:31, 20.85s/it]                                                 28%|██▊       | 42/150 [14:39<37:31, 20.85s/it] 29%|██▊       | 43/150 [15:03<39:04, 21.91s/it] 29%|██▉       | 44/150 [15:22<37:02, 20.96s/it] 30%|███       | 45/150 [15:43<36:56, 21.11s/it]                                                 30%|███       | 45/150 [15:43<36:56, 21.11s/it] 31%|███       | 46/150 [16:02<35:18, 20.37s/it] 31%|███▏      | 47/150 [16:27<37:04, 21.60s/it] 32%|███▏      | 48/150 [16:43<34:01, 20.01s/it]                                                 32%|███▏      | 48/150 [16:43<34:01, 20.01s/it] 33%|███▎      | 49/150 [17:05<34:33, 20.53s/it] 33%|███▎      | 50/150 [17:22<32:35, 19.56s/it] 34%|███▍      | 51/150 [17:48<35:15, 21.37s/it]                                                 34%|███▍      | 51/150 [17:48<35:15, 21.37s/it] 35%|███▍      | 52/150 [18:04<32:43, 20.04s/it] 35%|███▌      | 53/150 [18:29<34:21, 21.26s/it] 36%|███▌      | 54/150 [18:46<32:14, 20.15s/it]                                                 36%|███▌      | 54/150 [18:46<32:14, 20.15s/it] 37%|███▋      | 55/150 [19:10<33:54, 21.42s/it] 37%|███▋      | 56/150 [19:28<31:32, 20.13s/it] 38%|███▊      | 57/150 [19:48<31:30, 20.33s/it]                                                 38%|███▊      | 57/150 [19:48<31:30, 20.33s/it] 39%|███▊      | 58/150 [20:07<30:23, 19.82s/it] 39%|███▉      | 59/150 [20:32<32:11, 21.22s/it] 40%|████      | 60/150 [20:50<30:37, 20.42s/it]                                                 40%|████      | 60/150 [20:50<30:37, 20.42s/it] 41%|████      | 61/150 [21:16<32:36, 21.98s/it] 41%|████▏     | 62/150 [21:34<30:37, 20.88s/it] 42%|████▏     | 63/150 [21:59<31:50, 21.96s/it]                                                 42%|████▏     | 63/150 [21:59<31:50, 21.96s/it] 43%|████▎     | 64/150 [22:16<29:24, 20.51s/it] 43%|████▎     | 65/150 [22:39<30:14, 21.35s/it] 44%|████▍     | 66/150 [22:56<28:00, 20.01s/it]                                                 44%|████▍     | 66/150 [22:56<28:00, 20.01s/it] 45%|████▍     | 67/150 [23:18<28:23, 20.53s/it] 45%|████▌     | 68/150 [23:36<27:16, 19.96s/it] 46%|████▌     | 69/150 [24:01<28:43, 21.27s/it]                                                 46%|████▌     | 69/150 [24:01<28:43, 21.27s/it] 47%|████▋     | 70/150 [24:19<27:18, 20.48s/it] 47%|████▋     | 71/150 [24:45<28:59, 22.02s/it] 48%|████▊     | 72/150 [25:04<27:39, 21.28s/it]                                                 48%|████▊     | 72/150 [25:04<27:39, 21.28s/it] 49%|████▊     | 73/150 [25:28<28:05, 21.89s/it] 49%|████▉     | 74/150 [25:45<25:55, 20.46s/it] 50%|█████     | 75/150 [26:08<26:38, 21.31s/it]                                                 50%|█████     | 75/150 [26:08<26:38, 21.31s/it] 51%|█████     | 76/150 [26:27<25:17, 20.51s/it] 51%|█████▏    | 77/150 [26:51<26:25, 21.71s/it] 52%|█████▏    | 78/150 [27:10<24:55, 20.77s/it]                                                 52%|█████▏    | 78/150 [27:10<24:55, 20.77s/it] 53%|█████▎    | 79/150 [27:34<25:48, 21.82s/it] 53%|█████▎    | 80/150 [27:51<23:48, 20.40s/it] 54%|█████▍    | 81/150 [28:17<25:17, 21.99s/it]                                                 54%|█████▍    | 81/150 [28:17<25:17, 21.99s/it] 55%|█████▍    | 82/150 [28:36<23:59, 21.17s/it] 55%|█████▌    | 83/150 [29:01<24:45, 22.18s/it] 56%|█████▌    | 84/150 [29:20<23:27, 21.32s/it]                                                 56%|█████▌    | 84/150 [29:20<23:27, 21.32s/it] 57%|█████▋    | 85/150 [29:41<23:02, 21.28s/it] 57%|█████▋    | 86/150 [29:58<21:18, 19.98s/it] 58%|█████▊    | 87/150 [30:22<22:14, 21.18s/it]                                                 58%|█████▊    | 87/150 [30:22<22:14, 21.18s/it] 59%|█████▊    | 88/150 [30:41<21:03, 20.38s/it] 59%|█████▉    | 89/150 [31:04<21:36, 21.25s/it] 60%|██████    | 90/150 [31:21<20:00, 20.02s/it]                                                 60%|██████    | 90/150 [31:21<20:00, 20.02s/it] 61%|██████    | 91/150 [31:47<21:20, 21.70s/it] 61%|██████▏   | 92/150 [32:03<19:34, 20.25s/it] 62%|██████▏   | 93/150 [32:28<20:27, 21.53s/it]                                                 62%|██████▏   | 93/150 [32:28<20:27, 21.53s/it] 63%|██████▎   | 94/150 [32:48<19:33, 20.96s/it] 63%|██████▎   | 95/150 [33:12<20:06, 21.94s/it] 64%|██████▍   | 96/150 [33:31<19:01, 21.14s/it]                                                 64%|██████▍   | 96/150 [33:31<19:01, 21.14s/it] 65%|██████▍   | 97/150 [33:55<19:31, 22.10s/it] 65%|██████▌   | 98/150 [34:14<18:15, 21.06s/it] 66%|██████▌   | 99/150 [34:38<18:38, 21.94s/it]                                                 66%|██████▌   | 99/150 [34:38<18:38, 21.94s/it] 67%|██████▋   | 100/150 [34:55<17:04, 20.49s/it] 67%|██████▋   | 101/150 [35:21<17:59, 22.04s/it] 68%|██████▊   | 102/150 [35:38<16:27, 20.56s/it]                                                  68%|██████▊   | 102/150 [35:38<16:27, 20.56s/it] 69%|██████▊   | 103/150 [35:57<15:49, 20.21s/it] 69%|██████▉   | 104/150 [36:15<14:53, 19.42s/it] 70%|███████   | 105/150 [36:39<15:35, 20.80s/it]                                                  70%|███████   | 105/150 [36:39<15:35, 20.80s/it] 71%|███████   | 106/150 [36:56<14:29, 19.75s/it] 71%|███████▏  | 107/150 [37:21<15:08, 21.12s/it] 72%|███████▏  | 108/150 [37:40<14:30, 20.74s/it]                                                  72%|███████▏  | 108/150 [37:40<14:30, 20.74s/it] 73%|███████▎  | 109/150 [38:05<14:57, 21.88s/it] 73%|███████▎  | 110/150 [38:23<13:54, 20.85s/it] 74%|███████▍  | 111/150 [38:49<14:29, 22.29s/it]                                                  74%|███████▍  | 111/150 [38:49<14:29, 22.29s/it] 75%|███████▍  | 112/150 [39:08<13:24, 21.17s/it] 75%|███████▌  | 113/150 [39:29<13:04, 21.21s/it] 76%|███████▌  | 114/150 [39:46<11:59, 19.99s/it]                                                  76%|███████▌  | 114/150 [39:46<11:59, 19.99s/it] 77%|███████▋  | 115/150 [40:11<12:27, 21.37s/it] 77%|███████▋  | 116/150 [40:29<11:35, 20.47s/it] 78%|███████▊  | 117/150 [40:50<11:23, 20.73s/it]                                                  78%|███████▊  | 117/150 [40:50<11:23, 20.73s/it] 79%|███████▊  | 118/150 [41:08<10:32, 19.77s/it] 79%|███████▉  | 119/150 [41:32<10:55, 21.15s/it] 80%|████████  | 120/150 [41:50<10:02, 20.08s/it]                                                  80%|████████  | 120/150 [41:50<10:02, 20.08s/it] 81%|████████  | 121/150 [42:15<10:30, 21.75s/it] 81%|████████▏ | 122/150 [42:34<09:42, 20.81s/it] 82%|████████▏ | 123/150 [42:58<09:50, 21.87s/it]                                                  82%|████████▏ | 123/150 [42:58<09:50, 21.87s/it] 83%|████████▎ | 124/150 [43:16<08:54, 20.57s/it] 83%|████████▎ | 125/150 [43:40<09:03, 21.74s/it] 84%|████████▍ | 126/150 [43:59<08:18, 20.78s/it]                                                  84%|████████▍ | 126/150 [43:59<08:18, 20.78s/it] 85%|████████▍ | 127/150 [44:19<07:51, 20.49s/it] 85%|████████▌ | 128/150 [44:36<07:08, 19.47s/it] 86%|████████▌ | 129/150 [45:00<07:17, 20.82s/it]                                                  86%|████████▌ | 129/150 [45:00<07:17, 20.82s/it] 87%|████████▋ | 130/150 [45:19<06:46, 20.33s/it] 87%|████████▋ | 131/150 [45:45<06:56, 21.92s/it] 88%|████████▊ | 132/150 [46:03<06:16, 20.89s/it]                                                  88%|████████▊ | 132/150 [46:03<06:16, 20.89s/it] 89%|████████▊ | 133/150 [46:26<06:07, 21.61s/it] 89%|████████▉ | 134/150 [46:45<05:30, 20.64s/it] 90%|█████████ | 135/150 [47:09<05:26, 21.74s/it]                                                  90%|█████████ | 135/150 [47:09<05:26, 21.74s/it] 91%|█████████ | 136/150 [47:26<04:44, 20.36s/it] 91%|█████████▏| 137/150 [47:50<04:39, 21.48s/it] 92%|█████████▏| 138/150 [48:10<04:10, 20.84s/it]                                                  92%|█████████▏| 138/150 [48:10<04:10, 20.84s/it] 93%|█████████▎| 139/150 [48:34<03:59, 21.82s/it] 93%|█████████▎| 140/150 [48:51<03:24, 20.42s/it] 94%|█████████▍| 141/150 [49:17<03:17, 22.00s/it]                                                  94%|█████████▍| 141/150 [49:17<03:17, 22.00s/it] 95%|█████████▍| 142/150 [49:36<02:50, 21.27s/it] 95%|█████████▌| 143/150 [50:00<02:35, 22.17s/it] 96%|█████████▌| 144/150 [50:18<02:04, 20.78s/it]                                                  96%|█████████▌| 144/150 [50:18<02:04, 20.78s/it] 97%|█████████▋| 145/150 [50:42<01:48, 21.78s/it] 97%|█████████▋| 146/150 [50:59<01:21, 20.38s/it] 98%|█████████▊| 147/150 [51:23<01:04, 21.47s/it]                                                  98%|█████████▊| 147/150 [51:23<01:04, 21.47s/it] 99%|█████████▊| 148/150 [51:42<00:41, 20.52s/it] 99%|█████████▉| 149/150 [52:02<00:20, 20.58s/it]100%|██████████| 150/150 [52:21<00:00, 20.05s/it]                                                 100%|██████████| 150/150 [52:21<00:00, 20.05s/it]                                                 100%|██████████| 150/150 [52:21<00:00, 20.05s/it]100%|██████████| 150/150 [52:21<00:00, 20.94s/it]
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm ▁▁▂▂▃▂▂▂▂▂▁▁▂▁▂▂▂▂▂▂▂▂▂▄▅▅▅▆▇▆█▇▆▇▇▃▄▄▅▃
wandb: train/learning_rate ▂▄▅▇███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁███▇▇▆▆▅▄▃▃▃▂▂▁▁▁
wandb:          train/loss ███▇▇▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▂▂▂▁▂▂▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 3.608829859710566e+16
wandb:              train/epoch 15
wandb:        train/global_step 150
wandb:          train/grad_norm 0.22621
wandb:      train/learning_rate 0
wandb:               train/loss 0.06
wandb:               train_loss 0.56691
wandb:            train_runtime 3144.9801
wandb: train_samples_per_second 0.334
wandb:   train_steps_per_second 0.048
wandb: 
wandb: 🚀 View run fine_tune_json_fact_first_zero_shot_structured_seed_314_llama2 at: https://wandb.ai/rean-fernandes/Final_runs_paper/runs/sbey2eny
wandb: ⭐️ View project at: https://wandb.ai/rean-fernandes/Final_runs_paper
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250306_202600-sbey2eny/logs
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.29s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.73s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.40s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.82s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.83s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.62s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.73s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.01s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.64s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.75s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/pfs/data5/home/fr/fr_fr/fr_rf1031/llama-env/lib64/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 16.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.22s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.21s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
